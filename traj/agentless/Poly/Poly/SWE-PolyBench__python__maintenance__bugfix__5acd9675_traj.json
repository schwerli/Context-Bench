{
    "instance_id": "SWE-PolyBench__python__maintenance__bugfix__5acd9675",
    "original_id": "huggingface__transformers-16198",
    "1_model_selected_files": [
        "src/transformers/models/clip/configuration_clip.py",
        "src/transformers/models/clip/modeling_clip.py",
        "src/transformers/configuration_utils.py",
        "src/transformers/models/clip/__init__.py"
    ],
    "2_embedding_selected_files": [],
    "3_final_combined_files": [
        "src/transformers/models/clip/configuration_clip.py",
        "src/transformers/models/clip/modeling_clip.py",
        "src/transformers/configuration_utils.py"
    ],
    "4_related_elements": {
        "src/transformers/models/clip/configuration_clip.py": [
            "variable: CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP\nclass: CLIPVisionConfig\nclass: CLIPConfig"
        ],
        "src/transformers/models/clip/modeling_clip.py": [
            "variable: _CHECKPOINT_FOR_DOC\nvariable: CLIP_PRETRAINED_MODEL_ARCHIVE_LIST\nclass: CLIPVisionEmbeddings\nclass: CLIPVisionTransformer\nclass: CLIPVisionModel"
        ],
        "src/transformers/configuration_utils.py": [
            "function: PretrainedConfig.from_pretrained\nfunction: PretrainedConfig.from_dict\nfunction: PretrainedConfig.get_config_dict\nfunction: PretrainedConfig._get_config_dict"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "src/transformers/models/clip/configuration_clip.py": [
                    "function: CLIPVisionConfig.from_pretrained\nline: 25"
                ],
                "src/transformers/models/clip/modeling_clip.py": [
                    ""
                ],
                "src/transformers/configuration_utils.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/clip/configuration_clip.py b/src/transformers/models/clip/configuration_clip.py\nindex 0236ffd..544054e 100644\n--- a/src/transformers/models/clip/configuration_clip.py\n+++ b/src/transformers/models/clip/configuration_clip.py\n@@ -24,6 +24,7 @@ logger = logging.get_logger(__name__)\n \n CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n     \"openai/clip-vit-base-patch32\": \"https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\",\n+    \"openai/clip-vit-base-patch16\": \"https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/config.json\",\n     # See all CLIP models at https://huggingface.co/models?filter=clip\n }\n \n",
                "diff --git a/src/transformers/models/clip/configuration_clip.py b/src/transformers/models/clip/configuration_clip.py\nindex 0236ffd..544054e 100644\n--- a/src/transformers/models/clip/configuration_clip.py\n+++ b/src/transformers/models/clip/configuration_clip.py\n@@ -24,6 +24,7 @@ logger = logging.get_logger(__name__)\n \n CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n     \"openai/clip-vit-base-patch32\": \"https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\",\n+    \"openai/clip-vit-base-patch16\": \"https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/config.json\",\n     # See all CLIP models at https://huggingface.co/models?filter=clip\n }\n \ndiff --git a/src/transformers/models/clip/modeling_clip.py b/src/transformers/models/clip/modeling_clip.py\nindex 6142eb5..1c93a39 100644\n--- a/src/transformers/models/clip/modeling_clip.py\n+++ b/src/transformers/models/clip/modeling_clip.py\n@@ -37,10 +37,11 @@ from .configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"openai/clip-vit-base-patch32\"\n+_CHECKPOINT_FOR_DOC = \"openai/clip-vit-base-patch16\"\n \n CLIP_PRETRAINED_MODEL_ARCHIVE_LIST = [\n     \"openai/clip-vit-base-patch32\",\n+    \"openai/clip-vit-base-patch16\",\n     # See all CLIP models at https://huggingface.co/models?filter=clip\n ]\n \n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "src/transformers/models/clip/configuration_clip.py": [
                    "function: CLIPVisionConfig.from_pretrained\nfunction: CLIPTextConfig.from_pretrained\nline: 26"
                ],
                "src/transformers/models/clip/modeling_clip.py": [
                    "line: 42"
                ],
                "src/transformers/configuration_utils.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/clip/configuration_clip.py b/src/transformers/models/clip/configuration_clip.py\nindex 0236ffd..544054e 100644\n--- a/src/transformers/models/clip/configuration_clip.py\n+++ b/src/transformers/models/clip/configuration_clip.py\n@@ -24,6 +24,7 @@ logger = logging.get_logger(__name__)\n \n CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n     \"openai/clip-vit-base-patch32\": \"https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\",\n+    \"openai/clip-vit-base-patch16\": \"https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/config.json\",\n     # See all CLIP models at https://huggingface.co/models?filter=clip\n }\n \n",
                "diff --git a/src/transformers/models/clip/configuration_clip.py b/src/transformers/models/clip/configuration_clip.py\nindex 0236ffd..544054e 100644\n--- a/src/transformers/models/clip/configuration_clip.py\n+++ b/src/transformers/models/clip/configuration_clip.py\n@@ -24,6 +24,7 @@ logger = logging.get_logger(__name__)\n \n CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n     \"openai/clip-vit-base-patch32\": \"https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\",\n+    \"openai/clip-vit-base-patch16\": \"https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/config.json\",\n     # See all CLIP models at https://huggingface.co/models?filter=clip\n }\n \ndiff --git a/src/transformers/models/clip/modeling_clip.py b/src/transformers/models/clip/modeling_clip.py\nindex 6142eb5..a178571 100644\n--- a/src/transformers/models/clip/modeling_clip.py\n+++ b/src/transformers/models/clip/modeling_clip.py\n@@ -37,9 +37,10 @@ from .configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"openai/clip-vit-base-patch32\"\n+_CHECKPOINT_FOR_DOC = \"openai/clip-vit-base-patch16\"\n \n CLIP_PRETRAINED_MODEL_ARCHIVE_LIST = [\n+    \"openai/clip-vit-base-patch16\",\n     \"openai/clip-vit-base-patch32\",\n     # See all CLIP models at https://huggingface.co/models?filter=clip\n ]\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/src/transformers/models/clip/configuration_clip.py b/src/transformers/models/clip/configuration_clip.py\nindex 0236ffd..544054e 100644\n--- a/src/transformers/models/clip/configuration_clip.py\n+++ b/src/transformers/models/clip/configuration_clip.py\n@@ -24,6 +24,7 @@ logger = logging.get_logger(__name__)\n \n CLIP_PRETRAINED_CONFIG_ARCHIVE_MAP = {\n     \"openai/clip-vit-base-patch32\": \"https://huggingface.co/openai/clip-vit-base-patch32/resolve/main/config.json\",\n+    \"openai/clip-vit-base-patch16\": \"https://huggingface.co/openai/clip-vit-base-patch16/resolve/main/config.json\",\n     # See all CLIP models at https://huggingface.co/models?filter=clip\n }\n \n"
}