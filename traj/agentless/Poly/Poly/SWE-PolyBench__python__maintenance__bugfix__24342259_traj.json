{
    "instance_id": "SWE-PolyBench__python__maintenance__bugfix__24342259",
    "original_id": "keras-team__keras-19484",
    "1_model_selected_files": [
        "keras/optimizers/adamw.py",
        "keras/optimizers/optimizer.py",
        "keras/backend/tensorflow/optimizer.py",
        "keras/optimizers/schedules/learning_rate_schedule.py",
        "keras/optimizers/base_optimizer.py"
    ],
    "2_embedding_selected_files": [
        "keras/optimizers/adamw_test.py",
        "keras/optimizers/adamw.py",
        "keras/optimizers/adamax_test.py",
        "keras/optimizers/adam_test.py",
        "keras/optimizers/nadam_test.py",
        "keras/optimizers/adagrad_test.py",
        "keras/optimizers/adafactor_test.py",
        "keras/optimizers/optimizer_test.py",
        "keras/optimizers/adadelta_test.py",
        "keras/backend/torch/optimizers/torch_adam.py",
        "keras/optimizers/optimizer_sparse_test.py",
        "keras/backend/torch/optimizers/torch_adamw.py",
        "keras/backend/tensorflow/optimizer_distribute_test.py",
        "keras/optimizers/adam.py",
        "keras/optimizers/schedules/learning_rate_schedule_test.py",
        "keras/optimizers/sgd_test.py",
        "keras/optimizers/loss_scale_optimizer_test.py",
        "keras/callbacks/swap_ema_weights_test.py",
        "keras/optimizers/adamax.py",
        "keras/optimizers/nadam.py",
        "keras/backend/torch/optimizers/torch_nadam.py",
        "keras/optimizers/lion_test.py",
        "keras/callbacks/learning_rate_scheduler_test.py",
        "keras/optimizers/rmsprop_test.py",
        "keras/losses/losses_test.py",
        "keras/optimizers/base_optimizer.py",
        "keras/backend/torch/optimizers/torch_optimizer.py",
        "keras/losses/loss_test.py",
        "keras/optimizers/adadelta.py",
        "keras/optimizers/adafactor.py",
        "keras/utils/torch_utils_test.py",
        "keras/backend/torch/optimizers/torch_adamax.py",
        "keras/callbacks/reduce_lr_on_plateau_test.py",
        "keras/backend/tensorflow/trainer.py",
        "keras/trainers/trainer_test.py",
        "keras/callbacks/early_stopping_test.py",
        "keras/optimizers/adagrad.py",
        "keras/backend/torch/optimizers/torch_adadelta.py",
        "keras/ops/numpy_test.py",
        "keras/legacy/saving/legacy_h5_format_test.py",
        "keras/callbacks/tensorboard_test.py",
        "keras/trainers/data_adapters/tf_dataset_adapter_test.py",
        "keras/saving/saving_lib_test.py",
        "keras/callbacks/swap_ema_weights.py",
        "keras/layers/regularization/alpha_dropout_test.py",
        "keras/trainers/data_adapters/array_data_adapter_test.py",
        "keras/optimizers/ftrl_test.py",
        "keras/applications/applications_test.py",
        "keras/trainers/compile_utils_test.py",
        "keras/ops/nn_test.py"
    ],
    "3_final_combined_files": [
        "keras/optimizers/adamw.py",
        "keras/optimizers/optimizer.py",
        "keras/backend/tensorflow/optimizer.py",
        "keras/optimizers/adamw_test.py",
        "keras/optimizers/adamax_test.py"
    ],
    "4_related_elements": {
        "keras/optimizers/adamw.py": [
            "class: AdamW"
        ],
        "keras/optimizers/optimizer.py": [
            "class: Optimizer\nvariable: base_optimizer_keyword_args"
        ],
        "keras/backend/tensorflow/optimizer.py": [
            "function: TFOptimizer._apply_weight_decay\nfunction: TFOptimizer._backend_update_step\nfunction: TFOptimizer.stateless_apply\nfunction: TFOptimizer._distributed_tf_update_step\nfunction: filter_empty_gradients"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "keras/backend/tensorflow/optimizer.py": [
                    "function: BackendOptimizer._apply_weight_decay\nline: 97\nline: 98\nline: 99"
                ],
                "keras/optimizers/adamw.py": [
                    ""
                ],
                "keras/optimizers/optimizer.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/keras/backend/tensorflow/optimizer.py b/keras/backend/tensorflow/optimizer.py\nindex b3a1ac7..56f2fe9 100644\n--- a/keras/backend/tensorflow/optimizer.py\n+++ b/keras/backend/tensorflow/optimizer.py\n@@ -94,8 +94,15 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n         def distributed_apply_weight_decay(distribution, variables, **kwargs):\n             def weight_decay_fn(variable):\n                 if self._use_weight_decay(variable):\n-                    lr = tf.cast(self.learning_rate, variable.dtype)\n-                    wd = tf.cast(self.weight_decay, variable.dtype)\n+                    # Resolve potential callables or backend.Variables before casting.\n+                    lr_value = self.learning_rate() if callable(self.learning_rate) else self.learning_rate\n+                    wd_value = self.weight_decay() if callable(self.weight_decay) else self.weight_decay\n+                    if isinstance(lr_value, backend.Variable):\n+                        lr_value = lr_value.value  # Convert to tf.Variable\n+                    if isinstance(wd_value, backend.Variable):\n+                        wd_value = wd_value.value  # Convert to tf.Variable\n+                    lr = tf.cast(lr_value, variable.dtype)\n+                    wd = tf.cast(wd_value, variable.dtype)\n                     variable.assign_sub(variable * wd * lr)\n \n             for variable in variables:\n",
                "diff --git a/keras/backend/tensorflow/optimizer.py b/keras/backend/tensorflow/optimizer.py\nindex b3a1ac7..01144e8 100644\n--- a/keras/backend/tensorflow/optimizer.py\n+++ b/keras/backend/tensorflow/optimizer.py\n@@ -1,3 +1,4 @@\n+\n import warnings\n \n import tensorflow as tf\n@@ -92,10 +93,27 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n             return\n \n         def distributed_apply_weight_decay(distribution, variables, **kwargs):\n+            def _resolve_hyper_value(value):\n+                # Resolve hyperparameter values that may be callables or variables.\n+                if callable(value):\n+                    try:\n+                        # Try calling without arguments (e.g., lambda: 0.01).\n+                        return value()\n+                    except TypeError:\n+                        # If an argument is expected (e.g., schedule(step)),\n+                        # try passing the optimizer iterations if available.\n+                        if hasattr(self, \"iterations\"):\n+                            return value(self.iterations)\n+                        # Fallback: attempt calling without args again.\n+                        return value()\n+                return value\n+\n             def weight_decay_fn(variable):\n                 if self._use_weight_decay(variable):\n-                    lr = tf.cast(self.learning_rate, variable.dtype)\n-                    wd = tf.cast(self.weight_decay, variable.dtype)\n+                    lr_val = _resolve_hyper_value(self.learning_rate)\n+                    wd_val = _resolve_hyper_value(self.weight_decay)\n+                    lr = tf.cast(lr_val, variable.dtype)\n+                    wd = tf.cast(wd_val, variable.dtype)\n                     variable.assign_sub(variable * wd * lr)\n \n             for variable in variables:\n@@ -248,4 +266,4 @@ def filter_empty_gradients(grads_and_vars):\n             \"provide a `loss` argument?\",\n             ([v.name for v in vars_with_empty_grads]),\n         )\n-    return filtered\n+    return filtered\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "keras/backend/tensorflow/optimizer.py": [
                    "function: _apply_weight_decay\nline: 97"
                ],
                "keras/optimizers/adamw.py": [
                    ""
                ],
                "keras/optimizers/optimizer.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/keras/backend/tensorflow/optimizer.py b/keras/backend/tensorflow/optimizer.py\nindex b3a1ac7..23d2061 100644\n--- a/keras/backend/tensorflow/optimizer.py\n+++ b/keras/backend/tensorflow/optimizer.py\n@@ -94,8 +94,10 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n         def distributed_apply_weight_decay(distribution, variables, **kwargs):\n             def weight_decay_fn(variable):\n                 if self._use_weight_decay(variable):\n-                    lr = tf.cast(self.learning_rate, variable.dtype)\n-                    wd = tf.cast(self.weight_decay, variable.dtype)\n+                    lr_value = self.learning_rate() if callable(self.learning_rate) else self.learning_rate\n+                    wd_value = self.weight_decay() if callable(self.weight_decay) else self.weight_decay\n+                    lr = tf.cast(lr_value, variable.dtype)\n+                    wd = tf.cast(wd_value, variable.dtype)\n                     variable.assign_sub(variable * wd * lr)\n \n             for variable in variables:\n",
                "diff --git a/keras/backend/tensorflow/optimizer.py b/keras/backend/tensorflow/optimizer.py\nindex b3a1ac7..a870844 100644\n--- a/keras/backend/tensorflow/optimizer.py\n+++ b/keras/backend/tensorflow/optimizer.py\n@@ -1,3 +1,4 @@\n+\n import warnings\n \n import tensorflow as tf\n@@ -94,8 +95,22 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n         def distributed_apply_weight_decay(distribution, variables, **kwargs):\n             def weight_decay_fn(variable):\n                 if self._use_weight_decay(variable):\n-                    lr = tf.cast(self.learning_rate, variable.dtype)\n-                    wd = tf.cast(self.weight_decay, variable.dtype)\n+                    # Support callable or schedule learning_rate/weight_decay values\n+                    lr_value = self.learning_rate\n+                    if callable(lr_value):\n+                        try:\n+                            lr_value = lr_value()\n+                        except TypeError:\n+                            # Schedules generally expect a step argument\n+                            lr_value = lr_value(self.iterations)\n+                    wd_value = self.weight_decay\n+                    if callable(wd_value):\n+                        try:\n+                            wd_value = wd_value()\n+                        except TypeError:\n+                            wd_value = wd_value(self.iterations)\n+                    lr = tf.cast(lr_value, variable.dtype)\n+                    wd = tf.cast(wd_value, variable.dtype)\n                     variable.assign_sub(variable * wd * lr)\n \n             for variable in variables:\n@@ -248,4 +263,4 @@ def filter_empty_gradients(grads_and_vars):\n             \"provide a `loss` argument?\",\n             ([v.name for v in vars_with_empty_grads]),\n         )\n-    return filtered\n+    return filtered\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/keras/backend/tensorflow/optimizer.py b/keras/backend/tensorflow/optimizer.py\nindex b3a1ac7..56f2fe9 100644\n--- a/keras/backend/tensorflow/optimizer.py\n+++ b/keras/backend/tensorflow/optimizer.py\n@@ -94,8 +94,15 @@ class TFOptimizer(KerasAutoTrackable, base_optimizer.BaseOptimizer):\n         def distributed_apply_weight_decay(distribution, variables, **kwargs):\n             def weight_decay_fn(variable):\n                 if self._use_weight_decay(variable):\n-                    lr = tf.cast(self.learning_rate, variable.dtype)\n-                    wd = tf.cast(self.weight_decay, variable.dtype)\n+                    # Resolve potential callables or backend.Variables before casting.\n+                    lr_value = self.learning_rate() if callable(self.learning_rate) else self.learning_rate\n+                    wd_value = self.weight_decay() if callable(self.weight_decay) else self.weight_decay\n+                    if isinstance(lr_value, backend.Variable):\n+                        lr_value = lr_value.value  # Convert to tf.Variable\n+                    if isinstance(wd_value, backend.Variable):\n+                        wd_value = wd_value.value  # Convert to tf.Variable\n+                    lr = tf.cast(lr_value, variable.dtype)\n+                    wd = tf.cast(wd_value, variable.dtype)\n                     variable.assign_sub(variable * wd * lr)\n \n             for variable in variables:\n"
}