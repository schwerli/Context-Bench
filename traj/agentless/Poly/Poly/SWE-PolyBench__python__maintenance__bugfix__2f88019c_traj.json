{
    "instance_id": "SWE-PolyBench__python__maintenance__bugfix__2f88019c",
    "original_id": "huggingface__transformers-8624",
    "1_model_selected_files": [
        "src/transformers/trainer.py"
    ],
    "2_embedding_selected_files": [],
    "3_final_combined_files": [
        "src/transformers/trainer.py"
    ],
    "4_related_elements": {
        "src/transformers/trainer.py": [
            "function: Trainer.train"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "src/transformers/trainer.py": [
                    "function: Trainer.train\nline: 677\nline: 678"
                ]
            },
            "patches": [
                "diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py\nindex 00b9374..889410d 100644\n--- a/src/transformers/trainer.py\n+++ b/src/transformers/trainer.py\n@@ -674,8 +674,12 @@ class Trainer:\n         # Check if continuing training from a checkpoint\n         if model_path and os.path.isfile(os.path.join(model_path, \"trainer_state.json\")):\n             self.state = TrainerState.load_from_json(os.path.join(model_path, \"trainer_state.json\"))\n-            epochs_trained = self.state.global_step // num_update_steps_per_epoch\n-            steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n+            if train_dataset_is_sized:\n+                epochs_trained = (self.state.global_step * self.args.gradient_accumulation_steps) // len(train_dataloader)\n+                steps_trained_in_current_epoch = (self.state.global_step * self.args.gradient_accumulation_steps) % len(train_dataloader)\n+            else:\n+                epochs_trained = self.state.global_step // num_update_steps_per_epoch\n+                steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n \n             logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n             logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
                "diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py\nindex 00b9374..21ae926 100644\n--- a/src/transformers/trainer.py\n+++ b/src/transformers/trainer.py\n@@ -674,8 +674,15 @@ class Trainer:\n         # Check if continuing training from a checkpoint\n         if model_path and os.path.isfile(os.path.join(model_path, \"trainer_state.json\")):\n             self.state = TrainerState.load_from_json(os.path.join(model_path, \"trainer_state.json\"))\n-            epochs_trained = self.state.global_step // num_update_steps_per_epoch\n-            steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n+            if train_dataset_is_sized:\n+                # When resuming, global_step counts optimization steps, but we need to skip dataloader steps.\n+                # Account for gradient accumulation to convert optimization steps to dataloader steps.\n+                epochs_trained = (self.state.global_step * self.args.gradient_accumulation_steps) // len(train_dataloader)\n+                steps_trained_in_current_epoch = (self.state.global_step * self.args.gradient_accumulation_steps) % len(train_dataloader)\n+            else:\n+                # Fallback for unsized datasets where len(train_dataloader) is not available.\n+                epochs_trained = self.state.global_step // num_update_steps_per_epoch\n+                steps_trained_in_current_epoch = self.state.global_step % num_update_steps_per_epoch\n \n             logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n             logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "src/transformers/trainer.py": [
                    "function: Trainer.train\nline: 677\nline: 678"
                ]
            },
            "patches": [
                "diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py\nindex 00b9374..62c56e8 100644\n--- a/src/transformers/trainer.py\n+++ b/src/transformers/trainer.py\n@@ -674,8 +674,9 @@ class Trainer:\n         # Check if continuing training from a checkpoint\n         if model_path and os.path.isfile(os.path.join(model_path, \"trainer_state.json\")):\n             self.state = TrainerState.load_from_json(os.path.join(model_path, \"trainer_state.json\"))\n-            epochs_trained = self.state.global_step // num_update_steps_per_epoch\n-            steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n+            # global_step counts optimizer (update) steps; convert to dataloader steps (micro-batches)\n+            epochs_trained = (self.state.global_step * self.args.gradient_accumulation_steps) // len(train_dataloader)\n+            steps_trained_in_current_epoch = (self.state.global_step * self.args.gradient_accumulation_steps) % len(train_dataloader)\n \n             logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n             logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n",
                "diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py\nindex 00b9374..0ca8f99 100644\n--- a/src/transformers/trainer.py\n+++ b/src/transformers/trainer.py\n@@ -674,8 +674,12 @@ class Trainer:\n         # Check if continuing training from a checkpoint\n         if model_path and os.path.isfile(os.path.join(model_path, \"trainer_state.json\")):\n             self.state = TrainerState.load_from_json(os.path.join(model_path, \"trainer_state.json\"))\n-            epochs_trained = self.state.global_step // num_update_steps_per_epoch\n-            steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n+            if train_dataset_is_sized:\n+                epochs_trained = (self.state.global_step * self.args.gradient_accumulation_steps) // len(train_dataloader)\n+                steps_trained_in_current_epoch = (self.state.global_step * self.args.gradient_accumulation_steps) % len(train_dataloader)\n+            else:\n+                epochs_trained = self.state.global_step // num_update_steps_per_epoch\n+                steps_trained_in_current_epoch = self.state.global_step % num_update_steps_per_epoch\n \n             logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n             logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py\nindex 00b9374..889410d 100644\n--- a/src/transformers/trainer.py\n+++ b/src/transformers/trainer.py\n@@ -674,8 +674,12 @@ class Trainer:\n         # Check if continuing training from a checkpoint\n         if model_path and os.path.isfile(os.path.join(model_path, \"trainer_state.json\")):\n             self.state = TrainerState.load_from_json(os.path.join(model_path, \"trainer_state.json\"))\n-            epochs_trained = self.state.global_step // num_update_steps_per_epoch\n-            steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n+            if train_dataset_is_sized:\n+                epochs_trained = (self.state.global_step * self.args.gradient_accumulation_steps) // len(train_dataloader)\n+                steps_trained_in_current_epoch = (self.state.global_step * self.args.gradient_accumulation_steps) % len(train_dataloader)\n+            else:\n+                epochs_trained = self.state.global_step // num_update_steps_per_epoch\n+                steps_trained_in_current_epoch = self.state.global_step % (num_update_steps_per_epoch)\n \n             logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n             logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n"
}