{
    "instance_id": "SWE-PolyBench__python__maintenance__bugfix__dbd7cde5",
    "original_id": "huggingface__transformers-12981",
    "1_model_selected_files": [
        "src/transformers/trainer.py"
    ],
    "2_embedding_selected_files": [],
    "3_final_combined_files": [
        "src/transformers/trainer.py"
    ],
    "4_related_elements": {
        "src/transformers/trainer.py": [
            "function: Trainer.train\nfunction: Trainer._sorted_checkpoints\nfunction: Trainer._load_optimizer_and_scheduler\nfunction: Trainer._load_rng_state\nfunction: Trainer.hyperparameter_search"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "src/transformers/trainer.py": [
                    "function: Trainer.train"
                ]
            },
            "patches": [
                "diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py\nindex 372a863..8da014e 100644\n--- a/src/transformers/trainer.py\n+++ b/src/transformers/trainer.py\n@@ -1027,6 +1027,9 @@ class Trainer:\n             )\n         if len(kwargs) > 0:\n             raise TypeError(f\"train() received got unexpected keyword arguments: {', '.join(list(kwargs.keys()))}.\")\n+        # Normalize resume_from_checkpoint: treat False as no checkpoint\n+        if isinstance(resume_from_checkpoint, bool) and not resume_from_checkpoint:\n+            resume_from_checkpoint = None\n         # This might change the seed so needs to run first.\n         self._hp_search_setup(trial)\n \n",
                "diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py\nindex 372a863..ed0542d 100644\n--- a/src/transformers/trainer.py\n+++ b/src/transformers/trainer.py\n@@ -1030,6 +1030,11 @@ class Trainer:\n         # This might change the seed so needs to run first.\n         self._hp_search_setup(trial)\n \n+        # Normalize resume_from_checkpoint in case it's the boolean False.\n+        # Downstream code expects a string path or None.\n+        if isinstance(resume_from_checkpoint, bool) and not resume_from_checkpoint:\n+            resume_from_checkpoint = None\n+\n         # Model re-init\n         model_reloaded = False\n         if self.model_init is not None:\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "src/transformers/trainer.py": [
                    "function: Trainer.train\nline: 1043"
                ]
            },
            "patches": [
                "diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py\nindex 372a863..caab456 100644\n--- a/src/transformers/trainer.py\n+++ b/src/transformers/trainer.py\n@@ -1027,6 +1027,9 @@ class Trainer:\n             )\n         if len(kwargs) > 0:\n             raise TypeError(f\"train() received got unexpected keyword arguments: {', '.join(list(kwargs.keys()))}.\")\n+        # Normalize resume_from_checkpoint: accept False as \"do not resume\"\n+        if isinstance(resume_from_checkpoint, bool) and not resume_from_checkpoint:\n+            resume_from_checkpoint = None\n         # This might change the seed so needs to run first.\n         self._hp_search_setup(trial)\n \n",
                "diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py\nindex 372a863..bd7a4c3 100644\n--- a/src/transformers/trainer.py\n+++ b/src/transformers/trainer.py\n@@ -1027,6 +1027,9 @@ class Trainer:\n             )\n         if len(kwargs) > 0:\n             raise TypeError(f\"train() received got unexpected keyword arguments: {', '.join(list(kwargs.keys()))}.\")\n+        # Normalize False to None for resume_from_checkpoint to avoid path operations on a boolean.\n+        if isinstance(resume_from_checkpoint, bool) and not resume_from_checkpoint:\n+            resume_from_checkpoint = None\n         # This might change the seed so needs to run first.\n         self._hp_search_setup(trial)\n \n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/src/transformers/trainer.py b/src/transformers/trainer.py\nindex 372a863..8da014e 100644\n--- a/src/transformers/trainer.py\n+++ b/src/transformers/trainer.py\n@@ -1027,6 +1027,9 @@ class Trainer:\n             )\n         if len(kwargs) > 0:\n             raise TypeError(f\"train() received got unexpected keyword arguments: {', '.join(list(kwargs.keys()))}.\")\n+        # Normalize resume_from_checkpoint: treat False as no checkpoint\n+        if isinstance(resume_from_checkpoint, bool) and not resume_from_checkpoint:\n+            resume_from_checkpoint = None\n         # This might change the seed so needs to run first.\n         self._hp_search_setup(trial)\n \n"
}