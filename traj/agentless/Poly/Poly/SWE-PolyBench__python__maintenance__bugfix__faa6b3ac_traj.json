{
    "instance_id": "SWE-PolyBench__python__maintenance__bugfix__faa6b3ac",
    "original_id": "huggingface__transformers-3716",
    "1_model_selected_files": [
        "src/transformers/modeling_transfo_xl.py"
    ],
    "2_embedding_selected_files": [
        "src/transformers/modeling_transfo_xl.py",
        "src/transformers/modeling_xlnet.py",
        "src/transformers/modeling_tf_transfo_xl.py",
        "src/transformers/modeling_ctrl.py",
        "src/transformers/modeling_gpt2.py",
        "src/transformers/modeling_openai.py",
        "src/transformers/modeling_xlm.py",
        "src/transformers/modeling_bert.py",
        "src/transformers/modeling_tf_xlnet.py",
        "src/transformers/modeling_albert.py",
        "src/transformers/modeling_tf_xlm.py",
        "templates/adding_a_new_model/modeling_xxx.py",
        "templates/adding_a_new_model/modeling_tf_xxx.py",
        "src/transformers/modeling_t5.py",
        "src/transformers/modeling_tf_gpt2.py",
        "examples/contrib/run_transfo_xl.py",
        "src/transformers/modeling_tf_ctrl.py",
        "src/transformers/modeling_tf_utils.py",
        "src/transformers/modeling_distilbert.py",
        "src/transformers/modeling_tf_openai.py",
        "src/transformers/modeling_roberta.py",
        "src/transformers/convert_pytorch_checkpoint_to_tf2.py",
        "src/transformers/modeling_electra.py",
        "src/transformers/modeling_tf_flaubert.py",
        "src/transformers/modeling_utils.py",
        "src/transformers/modeling_flaubert.py"
    ],
    "3_final_combined_files": [
        "src/transformers/modeling_transfo_xl.py",
        "src/transformers/modeling_xlnet.py",
        "src/transformers/modeling_tf_transfo_xl.py"
    ],
    "4_related_elements": {
        "src/transformers/modeling_transfo_xl.py": [
            "function: TransfoXLLMHeadModel.forward"
        ],
        "src/transformers/modeling_tf_transfo_xl.py": [
            "function: TFTransfoXLLMHeadModel.call"
        ],
        "src/transformers/modeling_xlnet.py": [
            ""
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "src/transformers/modeling_transfo_xl.py": [
                    "function: TransfoXLLMHeadModel.forward\nline: 904\nline: 907\nline: 909\nline: 912\nline: 913"
                ],
                "src/transformers/modeling_tf_transfo_xl.py": [
                    "function: TFTransfoXLLMHeadModel.call\nline: 847\nline: 850"
                ],
                "src/transformers/modeling_xlnet.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/modeling_tf_transfo_xl.py b/src/transformers/modeling_tf_transfo_xl.py\nindex 46171d3..80c5cd1 100644\n--- a/src/transformers/modeling_tf_transfo_xl.py\n+++ b/src/transformers/modeling_tf_transfo_xl.py\n@@ -847,6 +847,10 @@ class TFTransfoXLLMHeadModel(TFTransfoXLPreTrainedModel):\n         pred_hid = last_hidden[:, -tgt_len:]\n         outputs = transformer_outputs[1:]\n \n+        if labels is not None:\n+            # Shift so that tokens < n predict n\n+            pred_hid = pred_hid[:, :-1, :]\n+            labels = labels[:, 1:]\n         softmax_output = self.crit([pred_hid, labels], training=training)\n         outputs = [softmax_output] + outputs\n \n@@ -859,4 +863,4 @@ class TFTransfoXLLMHeadModel(TFTransfoXLPreTrainedModel):\n         if past:\n             inputs[\"mems\"] = past\n \n-        return inputs\n+        return inputs\ndiff --git a/src/transformers/modeling_transfo_xl.py b/src/transformers/modeling_transfo_xl.py\nindex 910a0b5..ba24779 100644\n--- a/src/transformers/modeling_transfo_xl.py\n+++ b/src/transformers/modeling_transfo_xl.py\n@@ -904,12 +904,16 @@ class TransfoXLLMHeadModel(TransfoXLPreTrainedModel):\n         pred_hid = last_hidden[:, -tgt_len:]\n         outputs = transformer_outputs[1:]\n \n-        softmax_output = self.crit(pred_hid.view(-1, pred_hid.size(-1)), labels)\n         if labels is None:\n+            softmax_output = self.crit(pred_hid.view(-1, pred_hid.size(-1)), labels)\n             softmax_output = softmax_output.view(bsz, tgt_len, -1)\n             outputs = [softmax_output] + outputs\n         else:\n-            softmax_output = softmax_output.view(bsz, tgt_len)\n+            # Shift so that tokens < n predict n\n+            shift_pred_hid = pred_hid[:, :-1, :]\n+            shift_labels = labels[:, 1:].contiguous()\n+            softmax_output = self.crit(shift_pred_hid.contiguous().view(-1, shift_pred_hid.size(-1)), shift_labels)\n+            softmax_output = softmax_output.view(bsz, tgt_len - 1)\n             outputs = [softmax_output, None] + outputs\n \n         return outputs  # (loss), logits or None if labels is not None (speed up adaptive softmax), new_mems, (all hidden states), (all attentions)\n",
                "diff --git a/src/transformers/modeling_tf_transfo_xl.py b/src/transformers/modeling_tf_transfo_xl.py\nindex 46171d3..a5d988c 100644\n--- a/src/transformers/modeling_tf_transfo_xl.py\n+++ b/src/transformers/modeling_tf_transfo_xl.py\n@@ -847,6 +847,19 @@ class TFTransfoXLLMHeadModel(TFTransfoXLPreTrainedModel):\n         pred_hid = last_hidden[:, -tgt_len:]\n         outputs = transformer_outputs[1:]\n \n+        # If labels are provided, shift them so that tokens < n predict token n\n+        if labels is not None:\n+            # Shift predictions and labels by one for next-token prediction\n+            if tgt_len > 1:\n+                pred_hid = pred_hid[:, :-1]\n+                labels = labels[:, 1:]\n+            else:\n+                # No valid next-token labels; return NaNs to indicate undefined loss\n+                dims = tf.stack([bsz, tgt_len])\n+                nan_loss = tf.fill(dims, tf.cast(float(\"nan\"), dtype=last_hidden.dtype))\n+                outputs = [nan_loss] + outputs\n+                return outputs\n+\n         softmax_output = self.crit([pred_hid, labels], training=training)\n         outputs = [softmax_output] + outputs\n \n@@ -859,4 +872,4 @@ class TFTransfoXLLMHeadModel(TFTransfoXLPreTrainedModel):\n         if past:\n             inputs[\"mems\"] = past\n \n-        return inputs\n+        return inputs\ndiff --git a/src/transformers/modeling_transfo_xl.py b/src/transformers/modeling_transfo_xl.py\nindex 910a0b5..1d6bf63 100644\n--- a/src/transformers/modeling_transfo_xl.py\n+++ b/src/transformers/modeling_transfo_xl.py\n@@ -904,12 +904,25 @@ class TransfoXLLMHeadModel(TransfoXLPreTrainedModel):\n         pred_hid = last_hidden[:, -tgt_len:]\n         outputs = transformer_outputs[1:]\n \n-        softmax_output = self.crit(pred_hid.view(-1, pred_hid.size(-1)), labels)\n+        # If labels are provided, shift them so that tokens < n predict token n\n+        if labels is not None:\n+            # Shift predictions and labels by one for next-token prediction\n+            if tgt_len > 1:\n+                pred_hid = pred_hid[:, :-1].contiguous()\n+                labels = labels[:, 1:].contiguous()\n+            else:\n+                # No valid next-token labels; return NaNs to indicate undefined loss\n+                nan_loss = pred_hid.new_full((bsz, tgt_len), float(\"nan\"))\n+                outputs = [nan_loss, None] + outputs\n+                return outputs\n+\n+        softmax_output = self.crit(pred_hid.contiguous().view(-1, pred_hid.size(-1)), labels)\n         if labels is None:\n             softmax_output = softmax_output.view(bsz, tgt_len, -1)\n             outputs = [softmax_output] + outputs\n         else:\n-            softmax_output = softmax_output.view(bsz, tgt_len)\n+            # time dimension reduced by 1 due to shifting\n+            softmax_output = softmax_output.view(bsz, tgt_len - 1)\n             outputs = [softmax_output, None] + outputs\n \n         return outputs  # (loss), logits or None if labels is not None (speed up adaptive softmax), new_mems, (all hidden states), (all attentions)\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "src/transformers/modeling_transfo_xl.py": [
                    "function: TransfoXLLMHeadModel.forward\nline: 907\nline: 912\nline: 913"
                ],
                "src/transformers/modeling_tf_transfo_xl.py": [
                    "function: TFTransfoXLLMHeadModel.call\nline: 850\nline: 851"
                ],
                "src/transformers/modeling_xlnet.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/modeling_tf_transfo_xl.py b/src/transformers/modeling_tf_transfo_xl.py\nindex 46171d3..44ff2c6 100644\n--- a/src/transformers/modeling_tf_transfo_xl.py\n+++ b/src/transformers/modeling_tf_transfo_xl.py\n@@ -847,8 +847,15 @@ class TFTransfoXLLMHeadModel(TFTransfoXLPreTrainedModel):\n         pred_hid = last_hidden[:, -tgt_len:]\n         outputs = transformer_outputs[1:]\n \n-        softmax_output = self.crit([pred_hid, labels], training=training)\n-        outputs = [softmax_output] + outputs\n+        if labels is None:\n+            softmax_output = self.crit([pred_hid, None], training=training)\n+            outputs = [softmax_output] + outputs\n+        else:\n+            # shift labels for next-token prediction: ignore first label, drop last hidden state\n+            shifted_labels = labels[:, 1:]\n+            pred_hid = pred_hid[:, :-1]\n+            softmax_output = self.crit([pred_hid, shifted_labels], training=training)\n+            outputs = [softmax_output] + outputs\n \n         return outputs  # logits, new_mems, (all hidden states), (all attentions)\n \n@@ -859,4 +866,4 @@ class TFTransfoXLLMHeadModel(TFTransfoXLPreTrainedModel):\n         if past:\n             inputs[\"mems\"] = past\n \n-        return inputs\n+        return inputs\ndiff --git a/src/transformers/modeling_transfo_xl.py b/src/transformers/modeling_transfo_xl.py\nindex 910a0b5..7e7f4bc 100644\n--- a/src/transformers/modeling_transfo_xl.py\n+++ b/src/transformers/modeling_transfo_xl.py\n@@ -904,12 +904,16 @@ class TransfoXLLMHeadModel(TransfoXLPreTrainedModel):\n         pred_hid = last_hidden[:, -tgt_len:]\n         outputs = transformer_outputs[1:]\n \n-        softmax_output = self.crit(pred_hid.view(-1, pred_hid.size(-1)), labels)\n         if labels is None:\n+            softmax_output = self.crit(pred_hid.view(-1, pred_hid.size(-1)), None)\n             softmax_output = softmax_output.view(bsz, tgt_len, -1)\n             outputs = [softmax_output] + outputs\n         else:\n-            softmax_output = softmax_output.view(bsz, tgt_len)\n+            # shift labels for next-token prediction: ignore first label, drop last hidden state\n+            shifted_labels = labels[:, 1:].contiguous()\n+            pred_hid = pred_hid[:, :-1, :].contiguous()\n+            softmax_output = self.crit(pred_hid.view(-1, pred_hid.size(-1)), shifted_labels)\n+            softmax_output = softmax_output.view(bsz, tgt_len - 1)\n             outputs = [softmax_output, None] + outputs\n \n         return outputs  # (loss), logits or None if labels is not None (speed up adaptive softmax), new_mems, (all hidden states), (all attentions)\n",
                "diff --git a/src/transformers/modeling_tf_transfo_xl.py b/src/transformers/modeling_tf_transfo_xl.py\nindex 46171d3..ce7bf5b 100644\n--- a/src/transformers/modeling_tf_transfo_xl.py\n+++ b/src/transformers/modeling_tf_transfo_xl.py\n@@ -847,7 +847,13 @@ class TFTransfoXLLMHeadModel(TFTransfoXLPreTrainedModel):\n         pred_hid = last_hidden[:, -tgt_len:]\n         outputs = transformer_outputs[1:]\n \n-        softmax_output = self.crit([pred_hid, labels], training=training)\n+        # Shift labels for next-token prediction when computing loss\n+        if labels is None:\n+            softmax_output = self.crit([pred_hid, None], training=training)\n+        else:\n+            shifted_pred_hid = pred_hid[:, :-1, :]\n+            shifted_labels = labels[:, 1:]\n+            softmax_output = self.crit([shifted_pred_hid, shifted_labels], training=training)\n         outputs = [softmax_output] + outputs\n \n         return outputs  # logits, new_mems, (all hidden states), (all attentions)\n@@ -859,4 +865,4 @@ class TFTransfoXLLMHeadModel(TFTransfoXLPreTrainedModel):\n         if past:\n             inputs[\"mems\"] = past\n \n-        return inputs\n+        return inputs\ndiff --git a/src/transformers/modeling_transfo_xl.py b/src/transformers/modeling_transfo_xl.py\nindex 910a0b5..0f43402 100644\n--- a/src/transformers/modeling_transfo_xl.py\n+++ b/src/transformers/modeling_transfo_xl.py\n@@ -904,12 +904,16 @@ class TransfoXLLMHeadModel(TransfoXLPreTrainedModel):\n         pred_hid = last_hidden[:, -tgt_len:]\n         outputs = transformer_outputs[1:]\n \n-        softmax_output = self.crit(pred_hid.view(-1, pred_hid.size(-1)), labels)\n         if labels is None:\n+            softmax_output = self.crit(pred_hid.view(-1, pred_hid.size(-1)), None)\n             softmax_output = softmax_output.view(bsz, tgt_len, -1)\n             outputs = [softmax_output] + outputs\n         else:\n-            softmax_output = softmax_output.view(bsz, tgt_len)\n+            # Shift logits and labels for next-token prediction\n+            shifted_pred_hid = pred_hid[:, :-1, :].contiguous()\n+            shifted_labels = labels[:, 1:].contiguous()\n+            softmax_output = self.crit(shifted_pred_hid.view(-1, shifted_pred_hid.size(-1)), shifted_labels.view(-1))\n+            softmax_output = softmax_output.view(bsz, tgt_len - 1)\n             outputs = [softmax_output, None] + outputs\n \n         return outputs  # (loss), logits or None if labels is not None (speed up adaptive softmax), new_mems, (all hidden states), (all attentions)\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/src/transformers/modeling_tf_transfo_xl.py b/src/transformers/modeling_tf_transfo_xl.py\nindex 46171d3..80c5cd1 100644\n--- a/src/transformers/modeling_tf_transfo_xl.py\n+++ b/src/transformers/modeling_tf_transfo_xl.py\n@@ -847,6 +847,10 @@ class TFTransfoXLLMHeadModel(TFTransfoXLPreTrainedModel):\n         pred_hid = last_hidden[:, -tgt_len:]\n         outputs = transformer_outputs[1:]\n \n+        if labels is not None:\n+            # Shift so that tokens < n predict n\n+            pred_hid = pred_hid[:, :-1, :]\n+            labels = labels[:, 1:]\n         softmax_output = self.crit([pred_hid, labels], training=training)\n         outputs = [softmax_output] + outputs\n \n@@ -859,4 +863,4 @@ class TFTransfoXLLMHeadModel(TFTransfoXLPreTrainedModel):\n         if past:\n             inputs[\"mems\"] = past\n \n-        return inputs\n+        return inputs\ndiff --git a/src/transformers/modeling_transfo_xl.py b/src/transformers/modeling_transfo_xl.py\nindex 910a0b5..ba24779 100644\n--- a/src/transformers/modeling_transfo_xl.py\n+++ b/src/transformers/modeling_transfo_xl.py\n@@ -904,12 +904,16 @@ class TransfoXLLMHeadModel(TransfoXLPreTrainedModel):\n         pred_hid = last_hidden[:, -tgt_len:]\n         outputs = transformer_outputs[1:]\n \n-        softmax_output = self.crit(pred_hid.view(-1, pred_hid.size(-1)), labels)\n         if labels is None:\n+            softmax_output = self.crit(pred_hid.view(-1, pred_hid.size(-1)), labels)\n             softmax_output = softmax_output.view(bsz, tgt_len, -1)\n             outputs = [softmax_output] + outputs\n         else:\n-            softmax_output = softmax_output.view(bsz, tgt_len)\n+            # Shift so that tokens < n predict n\n+            shift_pred_hid = pred_hid[:, :-1, :]\n+            shift_labels = labels[:, 1:].contiguous()\n+            softmax_output = self.crit(shift_pred_hid.contiguous().view(-1, shift_pred_hid.size(-1)), shift_labels)\n+            softmax_output = softmax_output.view(bsz, tgt_len - 1)\n             outputs = [softmax_output, None] + outputs\n \n         return outputs  # (loss), logits or None if labels is not None (speed up adaptive softmax), new_mems, (all hidden states), (all attentions)\n"
}