{
    "instance_id": "SWE-PolyBench__python__maintenance__bugfix__ac5893a5",
    "original_id": "huggingface__transformers-22458",
    "1_model_selected_files": [
        "src/transformers/image_transforms.py",
        "src/transformers/image_processing_utils.py",
        "src/transformers/models/vit/image_processing_vit.py",
        "src/transformers/image_utils.py"
    ],
    "2_embedding_selected_files": [],
    "3_final_combined_files": [
        "src/transformers/image_transforms.py",
        "src/transformers/image_processing_utils.py",
        "src/transformers/models/vit/image_processing_vit.py"
    ],
    "4_related_elements": {
        "src/transformers/models/vit/image_processing_vit.py": [
            "function: ViTImageProcessor.__init__\nfunction: ViTImageProcessor.preprocess\nfunction: ViTImageProcessor.resize\nfunction: ViTImageProcessor.rescale\nfunction: ViTImageProcessor.normalize"
        ],
        "src/transformers/image_transforms.py": [
            "function: to_pil_image\nfunction: rescale\nfunction: normalize\nfunction: resize\nfunction: to_channel_dimension_format"
        ],
        "src/transformers/image_processing_utils.py": [
            "class: BaseImageProcessor\nclass: ImageProcessingMixin"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "src/transformers/models/vit/image_processing_vit.py": [
                    "function: ViTImageProcessor.preprocess\nline: 258\nline: 264"
                ],
                "src/transformers/image_transforms.py": [
                    ""
                ],
                "src/transformers/image_processing_utils.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/vit/image_processing_vit.py b/src/transformers/models/vit/image_processing_vit.py\nindex f2bd442..b2eb555 100644\n--- a/src/transformers/models/vit/image_processing_vit.py\n+++ b/src/transformers/models/vit/image_processing_vit.py\n@@ -262,7 +262,21 @@ class ViTImageProcessor(BaseImageProcessor):\n             images = [self.resize(image=image, size=size_dict, resample=resample) for image in images]\n \n         if do_rescale:\n-            images = [self.rescale(image=image, scale=rescale_factor) for image in images]\n+            # Only rescale when images are in [0, 255] range. If inputs are already floats in [0, 1],\n+            # skip rescaling to keep behavior consistent across PIL/torch inputs.\n+            processed_images = []\n+            for image in images:\n+                # numpy floating types have dtype.kind == 'f'\n+                if getattr(image, \"dtype\", None) is not None and image.dtype.kind == \"f\":\n+                    # If max value <= 1, we assume the image is already scaled to [0, 1].\n+                    # Otherwise, apply the configured rescale factor.\n+                    if image.max() <= 1.0:\n+                        processed_images.append(image)\n+                    else:\n+                        processed_images.append(self.rescale(image=image, scale=rescale_factor))\n+                else:\n+                    processed_images.append(self.rescale(image=image, scale=rescale_factor))\n+            images = processed_images\n \n         if do_normalize:\n             images = [self.normalize(image=image, mean=image_mean, std=image_std) for image in images]\n@@ -270,4 +284,4 @@ class ViTImageProcessor(BaseImageProcessor):\n         images = [to_channel_dimension_format(image, data_format) for image in images]\n \n         data = {\"pixel_values\": images}\n-        return BatchFeature(data=data, tensor_type=return_tensors)\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n",
                "diff --git a/src/transformers/models/vit/image_processing_vit.py b/src/transformers/models/vit/image_processing_vit.py\nindex f2bd442..9a69dae 100644\n--- a/src/transformers/models/vit/image_processing_vit.py\n+++ b/src/transformers/models/vit/image_processing_vit.py\n@@ -262,7 +262,19 @@ class ViTImageProcessor(BaseImageProcessor):\n             images = [self.resize(image=image, size=size_dict, resample=resample) for image in images]\n \n         if do_rescale:\n-            images = [self.rescale(image=image, scale=rescale_factor) for image in images]\n+            # If the input image is already in [0, 1] range (e.g. torch.Tensor from torchvision.transforms.ToTensor),\n+            # skip rescaling to keep behavior consistent with PIL inputs that are typically in [0, 255].\n+            new_images = []\n+            for image in images:\n+                # Check for float dtype and [0, 1] range\n+                if image.dtype.kind == \"f\":\n+                    im_min = image.min()\n+                    im_max = image.max()\n+                    if im_min >= 0.0 and im_max <= 1.0:\n+                        new_images.append(image)\n+                        continue\n+                new_images.append(self.rescale(image=image, scale=rescale_factor))\n+            images = new_images\n \n         if do_normalize:\n             images = [self.normalize(image=image, mean=image_mean, std=image_std) for image in images]\n@@ -270,4 +282,4 @@ class ViTImageProcessor(BaseImageProcessor):\n         images = [to_channel_dimension_format(image, data_format) for image in images]\n \n         data = {\"pixel_values\": images}\n-        return BatchFeature(data=data, tensor_type=return_tensors)\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "src/transformers/models/vit/image_processing_vit.py": [
                    "function: ViTImageProcessor.preprocess\nline: 264\nline: 265\nline: 266"
                ],
                "src/transformers/image_transforms.py": [
                    ""
                ],
                "src/transformers/image_processing_utils.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/vit/image_processing_vit.py b/src/transformers/models/vit/image_processing_vit.py\nindex f2bd442..4cb0752 100644\n--- a/src/transformers/models/vit/image_processing_vit.py\n+++ b/src/transformers/models/vit/image_processing_vit.py\n@@ -262,7 +262,27 @@ class ViTImageProcessor(BaseImageProcessor):\n             images = [self.resize(image=image, size=size_dict, resample=resample) for image in images]\n \n         if do_rescale:\n-            images = [self.rescale(image=image, scale=rescale_factor) for image in images]\n+            import numpy as np\n+\n+            def _needs_rescale(img: np.ndarray) -> bool:\n+                # Only rescale if the image appears to be in [0, 255] range.\n+                # - Integer types are assumed to be [0, 255].\n+                # - Floating types are rescaled only if values exceed 1.0.\n+                if np.issubdtype(img.dtype, np.integer):\n+                    return True\n+                if np.issubdtype(img.dtype, np.floating):\n+                    try:\n+                        return float(img.max()) > 1.0\n+                    except Exception:\n+                        # If we can't compute max, be safe and rescale.\n+                        return True\n+                # Fallback: rescale\n+                return True\n+\n+            images = [\n+                self.rescale(image=image, scale=rescale_factor) if _needs_rescale(image) else image\n+                for image in images\n+            ]\n \n         if do_normalize:\n             images = [self.normalize(image=image, mean=image_mean, std=image_std) for image in images]\n@@ -270,4 +290,4 @@ class ViTImageProcessor(BaseImageProcessor):\n         images = [to_channel_dimension_format(image, data_format) for image in images]\n \n         data = {\"pixel_values\": images}\n-        return BatchFeature(data=data, tensor_type=return_tensors)\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n",
                "diff --git a/src/transformers/models/vit/image_processing_vit.py b/src/transformers/models/vit/image_processing_vit.py\nindex f2bd442..2696585 100644\n--- a/src/transformers/models/vit/image_processing_vit.py\n+++ b/src/transformers/models/vit/image_processing_vit.py\n@@ -262,7 +262,19 @@ class ViTImageProcessor(BaseImageProcessor):\n             images = [self.resize(image=image, size=size_dict, resample=resample) for image in images]\n \n         if do_rescale:\n-            images = [self.rescale(image=image, scale=rescale_factor) for image in images]\n+            # Only rescale when images appear to be in 0-255 range (e.g. uint8 or float with values > 1.0).\n+            # If inputs are already floats in [0, 1], skip rescaling to maintain consistent behavior\n+            # across different input types when do_resize is False.\n+            processed_images = []\n+            for image in images:\n+                dtype = getattr(image, \"dtype\", None)\n+                is_uint8 = dtype is not None and dtype.kind == \"u\" and dtype.itemsize == 1\n+                is_float = dtype is not None and dtype.kind == \"f\"\n+                if is_uint8 or (is_float and image.max() > 1.0):\n+                    processed_images.append(self.rescale(image=image, scale=rescale_factor))\n+                else:\n+                    processed_images.append(image)\n+            images = processed_images\n \n         if do_normalize:\n             images = [self.normalize(image=image, mean=image_mean, std=image_std) for image in images]\n@@ -270,4 +282,4 @@ class ViTImageProcessor(BaseImageProcessor):\n         images = [to_channel_dimension_format(image, data_format) for image in images]\n \n         data = {\"pixel_values\": images}\n-        return BatchFeature(data=data, tensor_type=return_tensors)\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/src/transformers/models/vit/image_processing_vit.py b/src/transformers/models/vit/image_processing_vit.py\nindex f2bd442..b2eb555 100644\n--- a/src/transformers/models/vit/image_processing_vit.py\n+++ b/src/transformers/models/vit/image_processing_vit.py\n@@ -262,7 +262,21 @@ class ViTImageProcessor(BaseImageProcessor):\n             images = [self.resize(image=image, size=size_dict, resample=resample) for image in images]\n \n         if do_rescale:\n-            images = [self.rescale(image=image, scale=rescale_factor) for image in images]\n+            # Only rescale when images are in [0, 255] range. If inputs are already floats in [0, 1],\n+            # skip rescaling to keep behavior consistent across PIL/torch inputs.\n+            processed_images = []\n+            for image in images:\n+                # numpy floating types have dtype.kind == 'f'\n+                if getattr(image, \"dtype\", None) is not None and image.dtype.kind == \"f\":\n+                    # If max value <= 1, we assume the image is already scaled to [0, 1].\n+                    # Otherwise, apply the configured rescale factor.\n+                    if image.max() <= 1.0:\n+                        processed_images.append(image)\n+                    else:\n+                        processed_images.append(self.rescale(image=image, scale=rescale_factor))\n+                else:\n+                    processed_images.append(self.rescale(image=image, scale=rescale_factor))\n+            images = processed_images\n \n         if do_normalize:\n             images = [self.normalize(image=image, mean=image_mean, std=image_std) for image in images]\n@@ -270,4 +284,4 @@ class ViTImageProcessor(BaseImageProcessor):\n         images = [to_channel_dimension_format(image, data_format) for image in images]\n \n         data = {\"pixel_values\": images}\n-        return BatchFeature(data=data, tensor_type=return_tensors)\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n"
}