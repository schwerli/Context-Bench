{
    "instance_id": "SWE-PolyBench__python__maintenance__bugfix__40f09c26",
    "original_id": "huggingface__transformers-28535",
    "1_model_selected_files": [
        "src/transformers/models/esm/tokenization_esm.py",
        "src/transformers/tokenization_utils_base.py",
        "src/transformers/tokenization_utils.py",
        "src/transformers/models/esm/configuration_esm.py"
    ],
    "2_embedding_selected_files": [],
    "3_final_combined_files": [
        "src/transformers/models/esm/tokenization_esm.py",
        "src/transformers/tokenization_utils_base.py",
        "src/transformers/tokenization_utils.py"
    ],
    "4_related_elements": {
        "src/transformers/models/esm/tokenization_esm.py": [
            "function: EsmTokenizer._add_tokens\nfunction: EsmTokenizer.get_vocab\nfunction: EsmTokenizer.get_vocab_size\nfunction: EsmTokenizer.vocab_size\nfunction: EsmTokenizer.__init__"
        ],
        "src/transformers/tokenization_utils_base.py": [
            "function: SpecialTokensMixin.add_tokens\nfunction: SpecialTokensMixin.add_special_tokens\nfunction: SpecialTokensMixin.sanitize_special_tokens\nfunction: PreTrainedTokenizerBase.__len__\nfunction: PreTrainedTokenizerBase.get_vocab"
        ],
        "src/transformers/tokenization_utils.py": [
            "function: PreTrainedTokenizer._add_tokens\nfunction: PreTrainedTokenizer.__len__\nfunction: PreTrainedTokenizer.get_added_vocab\nfunction: PreTrainedTokenizer._update_trie"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "src/transformers/models/esm/tokenization_esm.py": [
                    "function: EsmTokenizer._add_tokens\nline: 161\nline: 162\nfunction: EsmTokenizer.get_vocab_size\nline: 94\nline: 95\nfunction: EsmTokenizer.get_vocab\nline: 97\nline: 98\nline: 99"
                ],
                "src/transformers/tokenization_utils_base.py": [
                    ""
                ],
                "src/transformers/tokenization_utils.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/esm/tokenization_esm.py b/src/transformers/models/esm/tokenization_esm.py\nindex 87f2cca..8ba23c3 100644\n--- a/src/transformers/models/esm/tokenization_esm.py\n+++ b/src/transformers/models/esm/tokenization_esm.py\n@@ -91,11 +91,19 @@ class EsmTokenizer(PreTrainedTokenizer):\n     def _tokenize(self, text, **kwargs):\n         return text.split()\n \n-    def get_vocab_size(self, with_added_tokens=False):\n-        return len(self._id_to_token)\n+    def get_vocab_size(self, with_added_tokens: bool = False):\n+        base_size = len(self._id_to_token)\n+        if with_added_tokens:\n+            # Include the number of non-special added tokens in the reported size\n+            return base_size + len(self.get_added_vocab())\n+        return base_size\n \n     def get_vocab(self):\n-        return {token: i for i, token in enumerate(self.all_tokens)}\n+        # Base ESM vocabulary\n+        vocab = {token: i for i, token in enumerate(self.all_tokens)}\n+        # Merge in non-special added tokens with their assigned ids\n+        vocab.update(self.get_added_vocab())\n+        return vocab\n \n     def token_to_id(self, token: str) -> int:\n         return self._token_to_id.get(token, self._token_to_id.get(self.unk_token))\n@@ -159,4 +167,5 @@ class EsmTokenizer(PreTrainedTokenizer):\n         return self.get_vocab_size(with_added_tokens=False)\n \n     def _add_tokens(self, new_tokens: Union[List[str], List[AddedToken]], special_tokens: bool = False) -> int:\n-        return super()._add_tokens(new_tokens, special_tokens=True)\n+        # Respect the caller's intent; do not force added tokens to be special\n+        return super()._add_tokens(new_tokens, special_tokens=special_tokens)\n",
                "diff --git a/src/transformers/models/esm/tokenization_esm.py b/src/transformers/models/esm/tokenization_esm.py\nindex 87f2cca..9ec6d27 100644\n--- a/src/transformers/models/esm/tokenization_esm.py\n+++ b/src/transformers/models/esm/tokenization_esm.py\n@@ -91,8 +91,16 @@ class EsmTokenizer(PreTrainedTokenizer):\n     def _tokenize(self, text, **kwargs):\n         return text.split()\n \n-    def get_vocab_size(self, with_added_tokens=False):\n-        return len(self._id_to_token)\n+    def get_vocab_size(self, with_added_tokens: bool = False):\n+        base_size = len(self._id_to_token)\n+        if with_added_tokens:\n+            # Count only tokens that were added via the tokenizer API\n+            try:\n+                added_size = len(self.added_tokens_encoder)\n+            except AttributeError:\n+                added_size = 0\n+            return base_size + added_size\n+        return base_size\n \n     def get_vocab(self):\n         return {token: i for i, token in enumerate(self.all_tokens)}\n@@ -156,7 +164,9 @@ class EsmTokenizer(PreTrainedTokenizer):\n \n     @property\n     def vocab_size(self) -> int:\n-        return self.get_vocab_size(with_added_tokens=False)\n+        # Include added tokens so that len(tokenizer) reflects the extended vocabulary.\n+        return self.get_vocab_size(with_added_tokens=True)\n \n     def _add_tokens(self, new_tokens: Union[List[str], List[AddedToken]], special_tokens: bool = False) -> int:\n-        return super()._add_tokens(new_tokens, special_tokens=True)\n+        # Respect the passed-in special_tokens flag instead of forcing True\n+        return super()._add_tokens(new_tokens, special_tokens=special_tokens)\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "src/transformers/models/esm/tokenization_esm.py": [
                    "function: EsmTokenizer._add_tokens\nfunction: EsmTokenizer.get_vocab\nfunction: EsmTokenizer.get_vocab_size"
                ],
                "src/transformers/tokenization_utils_base.py": [
                    ""
                ],
                "src/transformers/tokenization_utils.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/esm/tokenization_esm.py b/src/transformers/models/esm/tokenization_esm.py\nindex 87f2cca..0868ee6 100644\n--- a/src/transformers/models/esm/tokenization_esm.py\n+++ b/src/transformers/models/esm/tokenization_esm.py\n@@ -92,7 +92,10 @@ class EsmTokenizer(PreTrainedTokenizer):\n         return text.split()\n \n     def get_vocab_size(self, with_added_tokens=False):\n-        return len(self._id_to_token)\n+        base_size = len(self._id_to_token)\n+        if with_added_tokens:\n+            return base_size + len(self.get_added_vocab())\n+        return base_size\n \n     def get_vocab(self):\n         return {token: i for i, token in enumerate(self.all_tokens)}\n@@ -159,4 +162,4 @@ class EsmTokenizer(PreTrainedTokenizer):\n         return self.get_vocab_size(with_added_tokens=False)\n \n     def _add_tokens(self, new_tokens: Union[List[str], List[AddedToken]], special_tokens: bool = False) -> int:\n-        return super()._add_tokens(new_tokens, special_tokens=True)\n+        return super()._add_tokens(new_tokens, special_tokens=special_tokens)\n",
                "diff --git a/src/transformers/models/esm/tokenization_esm.py b/src/transformers/models/esm/tokenization_esm.py\nindex 87f2cca..5b0309f 100644\n--- a/src/transformers/models/esm/tokenization_esm.py\n+++ b/src/transformers/models/esm/tokenization_esm.py\n@@ -92,7 +92,11 @@ class EsmTokenizer(PreTrainedTokenizer):\n         return text.split()\n \n     def get_vocab_size(self, with_added_tokens=False):\n-        return len(self._id_to_token)\n+        base_size = len(self._id_to_token)\n+        if with_added_tokens:\n+            # Include tokens added at runtime (non-special tokens tracked in added_tokens_encoder).\n+            return base_size + len(getattr(self, \"added_tokens_encoder\", {}))\n+        return base_size\n \n     def get_vocab(self):\n         return {token: i for i, token in enumerate(self.all_tokens)}\n@@ -159,4 +163,5 @@ class EsmTokenizer(PreTrainedTokenizer):\n         return self.get_vocab_size(with_added_tokens=False)\n \n     def _add_tokens(self, new_tokens: Union[List[str], List[AddedToken]], special_tokens: bool = False) -> int:\n-        return super()._add_tokens(new_tokens, special_tokens=True)\n+        # Respect the caller's intent: only mark as special when explicitly requested.\n+        return super()._add_tokens(new_tokens, special_tokens=special_tokens)\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/src/transformers/models/esm/tokenization_esm.py b/src/transformers/models/esm/tokenization_esm.py\nindex 87f2cca..8ba23c3 100644\n--- a/src/transformers/models/esm/tokenization_esm.py\n+++ b/src/transformers/models/esm/tokenization_esm.py\n@@ -91,11 +91,19 @@ class EsmTokenizer(PreTrainedTokenizer):\n     def _tokenize(self, text, **kwargs):\n         return text.split()\n \n-    def get_vocab_size(self, with_added_tokens=False):\n-        return len(self._id_to_token)\n+    def get_vocab_size(self, with_added_tokens: bool = False):\n+        base_size = len(self._id_to_token)\n+        if with_added_tokens:\n+            # Include the number of non-special added tokens in the reported size\n+            return base_size + len(self.get_added_vocab())\n+        return base_size\n \n     def get_vocab(self):\n-        return {token: i for i, token in enumerate(self.all_tokens)}\n+        # Base ESM vocabulary\n+        vocab = {token: i for i, token in enumerate(self.all_tokens)}\n+        # Merge in non-special added tokens with their assigned ids\n+        vocab.update(self.get_added_vocab())\n+        return vocab\n \n     def token_to_id(self, token: str) -> int:\n         return self._token_to_id.get(token, self._token_to_id.get(self.unk_token))\n@@ -159,4 +167,5 @@ class EsmTokenizer(PreTrainedTokenizer):\n         return self.get_vocab_size(with_added_tokens=False)\n \n     def _add_tokens(self, new_tokens: Union[List[str], List[AddedToken]], special_tokens: bool = False) -> int:\n-        return super()._add_tokens(new_tokens, special_tokens=True)\n+        # Respect the caller's intent; do not force added tokens to be special\n+        return super()._add_tokens(new_tokens, special_tokens=special_tokens)\n"
}