{
    "instance_id": "SWE-PolyBench__python__maintenance__bugfix__158909b8",
    "original_id": "huggingface__transformers-28071",
    "1_model_selected_files": [
        "src/transformers/models/speecht5/modeling_speecht5.py"
    ],
    "2_embedding_selected_files": [],
    "3_final_combined_files": [
        "src/transformers/models/speecht5/modeling_speecht5.py"
    ],
    "4_related_elements": {
        "src/transformers/models/speecht5/modeling_speecht5.py": [
            "function: shift_spectrograms_right\nfunction: SpeechT5Model.forward\nfunction: SpeechT5ForTextToSpeech.forward\nfunction: SpeechT5ForSpeechToSpeech.forward\nfunction: SpeechT5DecoderWithSpeechPrenet.forward\nfunction: SpeechT5Decoder.forward\nfunction: SpeechT5DecoderWithoutPrenet.forward\nfunction: SpeechT5DecoderLayer.forward"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "src/transformers/models/speecht5/modeling_speecht5.py": [
                    "function: SpeechT5ForTextToSpeech.forward\nfunction: SpeechT5ForSpeechToSpeech.forward"
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/speecht5/modeling_speecht5.py b/src/transformers/models/speecht5/modeling_speecht5.py\nindex 3f753b2..b96d424 100644\n--- a/src/transformers/models/speecht5/modeling_speecht5.py\n+++ b/src/transformers/models/speecht5/modeling_speecht5.py\n@@ -2698,8 +2698,21 @@ class SpeechT5ForTextToSpeech(SpeechT5PreTrainedModel):\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if labels is not None:\n+            shifted_inputs = False\n             if decoder_input_values is None:\n                 decoder_input_values = shift_spectrograms_right(labels, self.config.reduction_factor)\n+                shifted_inputs = True\n+            # Ensure decoder_attention_mask matches any shift/interleave applied to decoder inputs\n+            if shifted_inputs:\n+                if decoder_attention_mask is not None:\n+                    dec_mask = decoder_attention_mask.unsqueeze(-1).to(decoder_input_values.dtype)\n+                    dec_mask = shift_spectrograms_right(dec_mask, self.config.reduction_factor)\n+                    decoder_attention_mask = (dec_mask.squeeze(-1) > 0).long()\n+                else:\n+                    # Derive mask from labels: 1 where timestep is not masked (-100)\n+                    label_mask = (labels != -100.0).any(dim=-1).to(decoder_input_values.dtype).unsqueeze(-1)\n+                    label_mask = shift_spectrograms_right(label_mask, self.config.reduction_factor)\n+                    decoder_attention_mask = (label_mask.squeeze(-1) > 0).long()\n             if self.config.use_guided_attention_loss:\n                 output_attentions = True\n \n@@ -3043,8 +3056,21 @@ class SpeechT5ForSpeechToSpeech(SpeechT5PreTrainedModel):\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if labels is not None:\n+            shifted_inputs = False\n             if decoder_input_values is None:\n                 decoder_input_values = shift_spectrograms_right(labels, self.config.reduction_factor)\n+                shifted_inputs = True\n+            # Ensure decoder_attention_mask matches any shift/interleave applied to decoder inputs\n+            if shifted_inputs:\n+                if decoder_attention_mask is not None:\n+                    dec_mask = decoder_attention_mask.unsqueeze(-1).to(decoder_input_values.dtype)\n+                    dec_mask = shift_spectrograms_right(dec_mask, self.config.reduction_factor)\n+                    decoder_attention_mask = (dec_mask.squeeze(-1) > 0).long()\n+                else:\n+                    # Derive mask from labels: 1 where timestep is not masked (-100)\n+                    label_mask = (labels != -100.0).any(dim=-1).to(decoder_input_values.dtype).unsqueeze(-1)\n+                    label_mask = shift_spectrograms_right(label_mask, self.config.reduction_factor)\n+                    decoder_attention_mask = (label_mask.squeeze(-1) > 0).long()\n \n         outputs = self.speecht5(\n             input_values=input_values,\n",
                "diff --git a/src/transformers/models/speecht5/modeling_speecht5.py b/src/transformers/models/speecht5/modeling_speecht5.py\nindex 3f753b2..69d3806 100644\n--- a/src/transformers/models/speecht5/modeling_speecht5.py\n+++ b/src/transformers/models/speecht5/modeling_speecht5.py\n@@ -2700,6 +2700,18 @@ class SpeechT5ForTextToSpeech(SpeechT5PreTrainedModel):\n         if labels is not None:\n             if decoder_input_values is None:\n                 decoder_input_values = shift_spectrograms_right(labels, self.config.reduction_factor)\n+            # Adjust decoder attention mask to match shifted and reduction-factor grouped decoder inputs\n+            if decoder_attention_mask is not None:\n+                r = self.config.reduction_factor\n+                decoder_lengths = decoder_attention_mask.sum(dim=1)\n+                dec_steps = torch.div(decoder_lengths + r - 1, r, rounding_mode=\"floor\")\n+                max_steps = decoder_input_values.shape[1]\n+                new_mask = decoder_attention_mask.new_zeros((decoder_attention_mask.size(0), max_steps))\n+                for i in range(new_mask.size(0)):\n+                    valid = int(dec_steps[i].item())\n+                    if valid > 0:\n+                        new_mask[i, :valid] = 1\n+                decoder_attention_mask = new_mask\n             if self.config.use_guided_attention_loss:\n                 output_attentions = True\n \n@@ -3045,6 +3057,18 @@ class SpeechT5ForSpeechToSpeech(SpeechT5PreTrainedModel):\n         if labels is not None:\n             if decoder_input_values is None:\n                 decoder_input_values = shift_spectrograms_right(labels, self.config.reduction_factor)\n+            # Adjust decoder attention mask to match shifted and reduction-factor grouped decoder inputs\n+            if decoder_attention_mask is not None:\n+                r = self.config.reduction_factor\n+                decoder_lengths = decoder_attention_mask.sum(dim=1)\n+                dec_steps = torch.div(decoder_lengths + r - 1, r, rounding_mode=\"floor\")\n+                max_steps = decoder_input_values.shape[1]\n+                new_mask = decoder_attention_mask.new_zeros((decoder_attention_mask.size(0), max_steps))\n+                for i in range(new_mask.size(0)):\n+                    valid = int(dec_steps[i].item())\n+                    if valid > 0:\n+                        new_mask[i, :valid] = 1\n+                decoder_attention_mask = new_mask\n \n         outputs = self.speecht5(\n             input_values=input_values,\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "src/transformers/models/speecht5/modeling_speecht5.py": [
                    "function: SpeechT5ForTextToSpeech.forward\nfunction: SpeechT5ForSpeechToSpeech.forward\nfunction: SpeechT5DecoderWithSpeechPrenet.forward"
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/speecht5/modeling_speecht5.py b/src/transformers/models/speecht5/modeling_speecht5.py\nindex 3f753b2..9ec28b8 100644\n--- a/src/transformers/models/speecht5/modeling_speecht5.py\n+++ b/src/transformers/models/speecht5/modeling_speecht5.py\n@@ -2700,6 +2700,23 @@ class SpeechT5ForTextToSpeech(SpeechT5PreTrainedModel):\n         if labels is not None:\n             if decoder_input_values is None:\n                 decoder_input_values = shift_spectrograms_right(labels, self.config.reduction_factor)\n+            # Ensure the decoder attention mask matches the transformed decoder inputs\n+            if decoder_input_values is not None:\n+                if decoder_attention_mask is not None:\n+                    _dec_mask = shift_spectrograms_right(\n+                        decoder_attention_mask.unsqueeze(-1).to(decoder_input_values.dtype),\n+                        self.config.reduction_factor,\n+                    ).squeeze(-1)\n+                    _dec_mask = _dec_mask.to(decoder_attention_mask.dtype)\n+                    if _dec_mask.size(1) > 0:\n+                        _dec_mask[:, 0] = 1\n+                    decoder_attention_mask = _dec_mask\n+                else:\n+                    decoder_attention_mask = torch.ones(\n+                        (decoder_input_values.size(0), decoder_input_values.size(1)),\n+                        dtype=torch.long,\n+                        device=decoder_input_values.device,\n+                    )\n             if self.config.use_guided_attention_loss:\n                 output_attentions = True\n \n@@ -3045,6 +3062,23 @@ class SpeechT5ForSpeechToSpeech(SpeechT5PreTrainedModel):\n         if labels is not None:\n             if decoder_input_values is None:\n                 decoder_input_values = shift_spectrograms_right(labels, self.config.reduction_factor)\n+            # Ensure the decoder attention mask matches the transformed decoder inputs\n+            if decoder_input_values is not None:\n+                if decoder_attention_mask is not None:\n+                    _dec_mask = shift_spectrograms_right(\n+                        decoder_attention_mask.unsqueeze(-1).to(decoder_input_values.dtype),\n+                        self.config.reduction_factor,\n+                    ).squeeze(-1)\n+                    _dec_mask = _dec_mask.to(decoder_attention_mask.dtype)\n+                    if _dec_mask.size(1) > 0:\n+                        _dec_mask[:, 0] = 1\n+                    decoder_attention_mask = _dec_mask\n+                else:\n+                    decoder_attention_mask = torch.ones(\n+                        (decoder_input_values.size(0), decoder_input_values.size(1)),\n+                        dtype=torch.long,\n+                        device=decoder_input_values.device,\n+                    )\n \n         outputs = self.speecht5(\n             input_values=input_values,\n",
                "diff --git a/src/transformers/models/speecht5/modeling_speecht5.py b/src/transformers/models/speecht5/modeling_speecht5.py\nindex 3f753b2..1a4fe52 100644\n--- a/src/transformers/models/speecht5/modeling_speecht5.py\n+++ b/src/transformers/models/speecht5/modeling_speecht5.py\n@@ -2700,6 +2700,56 @@ class SpeechT5ForTextToSpeech(SpeechT5PreTrainedModel):\n         if labels is not None:\n             if decoder_input_values is None:\n                 decoder_input_values = shift_spectrograms_right(labels, self.config.reduction_factor)\n+\n+            # Adjust decoder attention mask to match the shifted/interleaved decoder inputs\n+            if decoder_input_values is not None:\n+                bsz = decoder_input_values.size(0)\n+                tgt_len = decoder_input_values.size(1)\n+                r = max(1, int(self.config.reduction_factor))\n+\n+                if decoder_attention_mask is not None:\n+                    src_len = decoder_attention_mask.size(1)\n+                    if tgt_len != src_len:\n+                        if tgt_len == src_len * r + 1:\n+                            start = decoder_attention_mask.new_ones(bsz, 1)\n+                            rest = decoder_attention_mask.repeat_interleave(r, dim=1)\n+                            decoder_attention_mask = torch.cat([start, rest], dim=1)\n+                        elif tgt_len == src_len * r:\n+                            decoder_attention_mask = decoder_attention_mask.repeat_interleave(r, dim=1)\n+                        elif tgt_len == src_len + 1:\n+                            start = decoder_attention_mask.new_ones(bsz, 1)\n+                            decoder_attention_mask = torch.cat([start, decoder_attention_mask], dim=1)\n+                        else:\n+                            # Fallback: pad with ones or truncate as needed\n+                            if tgt_len > src_len:\n+                                pad = decoder_attention_mask.new_ones(bsz, tgt_len - src_len)\n+                                decoder_attention_mask = torch.cat([decoder_attention_mask, pad], dim=1)\n+                            else:\n+                                decoder_attention_mask = decoder_attention_mask[:, :tgt_len]\n+                else:\n+                    # Build a decoder attention mask derived from labels if available, else default to ones\n+                    if labels is not None and labels.dim() == 3:\n+                        # Determine base mask assuming -100.0 is used as ignore (padding) for timesteps\n+                        base = (labels[:, :, 0] != -100.0).long()\n+                        label_len = base.size(1)\n+\n+                        if tgt_len == label_len * r + 1:\n+                            rest = base.repeat_interleave(r, dim=1)\n+                            decoder_attention_mask = torch.cat(\n+                                [torch.ones(bsz, 1, dtype=base.dtype, device=base.device), rest], dim=1\n+                            )\n+                        elif tgt_len == label_len * r:\n+                            decoder_attention_mask = base.repeat_interleave(r, dim=1)\n+                        elif tgt_len == label_len + 1:\n+                            decoder_attention_mask = torch.cat(\n+                                [torch.ones(bsz, 1, dtype=base.dtype, device=base.device), base], dim=1\n+                            )\n+                        else:\n+                            decoder_attention_mask = torch.ones(bsz, tgt_len, dtype=torch.long, device=labels.device)\n+                    else:\n+                        decoder_attention_mask = torch.ones(\n+                            bsz, tgt_len, dtype=torch.long, device=decoder_input_values.device\n+                        )\n             if self.config.use_guided_attention_loss:\n                 output_attentions = True\n \n@@ -3046,6 +3096,56 @@ class SpeechT5ForSpeechToSpeech(SpeechT5PreTrainedModel):\n             if decoder_input_values is None:\n                 decoder_input_values = shift_spectrograms_right(labels, self.config.reduction_factor)\n \n+            # Adjust decoder attention mask to match the shifted/interleaved decoder inputs\n+            if decoder_input_values is not None:\n+                bsz = decoder_input_values.size(0)\n+                tgt_len = decoder_input_values.size(1)\n+                r = max(1, int(self.config.reduction_factor))\n+\n+                if decoder_attention_mask is not None:\n+                    src_len = decoder_attention_mask.size(1)\n+                    if tgt_len != src_len:\n+                        if tgt_len == src_len * r + 1:\n+                            start = decoder_attention_mask.new_ones(bsz, 1)\n+                            rest = decoder_attention_mask.repeat_interleave(r, dim=1)\n+                            decoder_attention_mask = torch.cat([start, rest], dim=1)\n+                        elif tgt_len == src_len * r:\n+                            decoder_attention_mask = decoder_attention_mask.repeat_interleave(r, dim=1)\n+                        elif tgt_len == src_len + 1:\n+                            start = decoder_attention_mask.new_ones(bsz, 1)\n+                            decoder_attention_mask = torch.cat([start, decoder_attention_mask], dim=1)\n+                        else:\n+                            # Fallback: pad with ones or truncate as needed\n+                            if tgt_len > src_len:\n+                                pad = decoder_attention_mask.new_ones(bsz, tgt_len - src_len)\n+                                decoder_attention_mask = torch.cat([decoder_attention_mask, pad], dim=1)\n+                            else:\n+                                decoder_attention_mask = decoder_attention_mask[:, :tgt_len]\n+                else:\n+                    # Build a decoder attention mask derived from labels if available, else default to ones\n+                    if labels is not None and labels.dim() == 3:\n+                        # Determine base mask assuming -100.0 is used as ignore (padding) for timesteps\n+                        base = (labels[:, :, 0] != -100.0).long()\n+                        label_len = base.size(1)\n+\n+                        if tgt_len == label_len * r + 1:\n+                            rest = base.repeat_interleave(r, dim=1)\n+                            decoder_attention_mask = torch.cat(\n+                                [torch.ones(bsz, 1, dtype=base.dtype, device=base.device), rest], dim=1\n+                            )\n+                        elif tgt_len == label_len * r:\n+                            decoder_attention_mask = base.repeat_interleave(r, dim=1)\n+                        elif tgt_len == label_len + 1:\n+                            decoder_attention_mask = torch.cat(\n+                                [torch.ones(bsz, 1, dtype=base.dtype, device=base.device), base], dim=1\n+                            )\n+                        else:\n+                            decoder_attention_mask = torch.ones(bsz, tgt_len, dtype=torch.long, device=labels.device)\n+                    else:\n+                        decoder_attention_mask = torch.ones(\n+                            bsz, tgt_len, dtype=torch.long, device=decoder_input_values.device\n+                        )\n+\n         outputs = self.speecht5(\n             input_values=input_values,\n             attention_mask=attention_mask,\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/src/transformers/models/speecht5/modeling_speecht5.py b/src/transformers/models/speecht5/modeling_speecht5.py\nindex 3f753b2..b96d424 100644\n--- a/src/transformers/models/speecht5/modeling_speecht5.py\n+++ b/src/transformers/models/speecht5/modeling_speecht5.py\n@@ -2698,8 +2698,21 @@ class SpeechT5ForTextToSpeech(SpeechT5PreTrainedModel):\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if labels is not None:\n+            shifted_inputs = False\n             if decoder_input_values is None:\n                 decoder_input_values = shift_spectrograms_right(labels, self.config.reduction_factor)\n+                shifted_inputs = True\n+            # Ensure decoder_attention_mask matches any shift/interleave applied to decoder inputs\n+            if shifted_inputs:\n+                if decoder_attention_mask is not None:\n+                    dec_mask = decoder_attention_mask.unsqueeze(-1).to(decoder_input_values.dtype)\n+                    dec_mask = shift_spectrograms_right(dec_mask, self.config.reduction_factor)\n+                    decoder_attention_mask = (dec_mask.squeeze(-1) > 0).long()\n+                else:\n+                    # Derive mask from labels: 1 where timestep is not masked (-100)\n+                    label_mask = (labels != -100.0).any(dim=-1).to(decoder_input_values.dtype).unsqueeze(-1)\n+                    label_mask = shift_spectrograms_right(label_mask, self.config.reduction_factor)\n+                    decoder_attention_mask = (label_mask.squeeze(-1) > 0).long()\n             if self.config.use_guided_attention_loss:\n                 output_attentions = True\n \n@@ -3043,8 +3056,21 @@ class SpeechT5ForSpeechToSpeech(SpeechT5PreTrainedModel):\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if labels is not None:\n+            shifted_inputs = False\n             if decoder_input_values is None:\n                 decoder_input_values = shift_spectrograms_right(labels, self.config.reduction_factor)\n+                shifted_inputs = True\n+            # Ensure decoder_attention_mask matches any shift/interleave applied to decoder inputs\n+            if shifted_inputs:\n+                if decoder_attention_mask is not None:\n+                    dec_mask = decoder_attention_mask.unsqueeze(-1).to(decoder_input_values.dtype)\n+                    dec_mask = shift_spectrograms_right(dec_mask, self.config.reduction_factor)\n+                    decoder_attention_mask = (dec_mask.squeeze(-1) > 0).long()\n+                else:\n+                    # Derive mask from labels: 1 where timestep is not masked (-100)\n+                    label_mask = (labels != -100.0).any(dim=-1).to(decoder_input_values.dtype).unsqueeze(-1)\n+                    label_mask = shift_spectrograms_right(label_mask, self.config.reduction_factor)\n+                    decoder_attention_mask = (label_mask.squeeze(-1) > 0).long()\n \n         outputs = self.speecht5(\n             input_values=input_values,\n"
}