{
    "instance_id": "SWE-PolyBench__python__maintenance__bugfix__5188c6bc",
    "original_id": "huggingface__transformers-6098",
    "1_model_selected_files": [
        "src/transformers/tokenization_bart.py",
        "src/transformers/tokenization_utils.py",
        "src/transformers/configuration_bart.py"
    ],
    "2_embedding_selected_files": [
        "src/transformers/tokenization_bart.py",
        "src/transformers/modeling_bart.py",
        "src/transformers/tokenization_bert.py",
        "examples/contrib/run_camembert.py",
        "src/transformers/modeling_mobilebert.py",
        "src/transformers/tokenization_xlm_roberta.py",
        "src/transformers/generation_utils.py",
        "src/transformers/tokenization_camembert.py",
        "src/transformers/file_utils.py",
        "examples/movement-pruning/emmental/modeling_bert_masked.py",
        "src/transformers/tokenization_auto.py",
        "src/transformers/generation_tf_utils.py",
        "src/transformers/modeling_xlm.py",
        "src/transformers/__init__.py",
        "src/transformers/modeling_bert.py",
        "src/transformers/tokenization_utils_base.py",
        "src/transformers/tokenization_xlm.py",
        "src/transformers/configuration_bert.py",
        "src/transformers/modeling_retribert.py",
        "src/transformers/configuration_bart.py",
        "src/transformers/tokenization_bert_japanese.py",
        "src/transformers/modeling_tf_xlnet.py",
        "src/transformers/tokenization_albert.py",
        "src/transformers/modeling_auto.py",
        "src/transformers/modeling_longformer.py",
        "src/transformers/data/data_collator.py",
        "src/transformers/tokenization_roberta.py",
        "src/transformers/modeling_tf_auto.py",
        "src/transformers/modeling_tf_albert.py",
        "examples/distillation/scripts/token_counts.py",
        "src/transformers/modeling_distilbert.py",
        "templates/adding_a_new_model/tokenization_xxx.py",
        "src/transformers/modeling_encoder_decoder.py",
        "src/transformers/modeling_reformer.py",
        "src/transformers/tokenization_marian.py",
        "src/transformers/tokenization_longformer.py",
        "src/transformers/convert_pytorch_checkpoint_to_tf2.py",
        "examples/movement-pruning/masked_run_glue.py",
        "src/transformers/tokenization_xlnet.py",
        "src/transformers/convert_bart_original_pytorch_checkpoint_to_pytorch.py",
        "examples/token-classification/scripts/preprocess.py",
        "src/transformers/modeling_marian.py",
        "src/transformers/tokenization_distilbert.py",
        "src/transformers/tokenization_gpt2.py"
    ],
    "3_final_combined_files": [
        "src/transformers/tokenization_bart.py",
        "src/transformers/tokenization_utils.py",
        "src/transformers/configuration_bart.py",
        "src/transformers/modeling_bart.py",
        "src/transformers/tokenization_bert.py"
    ],
    "4_related_elements": {
        "src/transformers/tokenization_bart.py": [
            "variable: FAIRSEQ_LANGUAGE_CODES\nvariable: SPM_URL\nvariable: _all_mbart_models\nclass: MBartTokenizer"
        ],
        "src/transformers/tokenization_utils.py": [
            "function: PreTrainedTokenizer._add_tokens\nfunction: PreTrainedTokenizer.convert_tokens_to_ids\nfunction: PreTrainedTokenizer._convert_token_to_id_with_added_voc\nfunction: PreTrainedTokenizer.get_added_vocab\nfunction: PreTrainedTokenizer._convert_token_to_id"
        ],
        "src/transformers/configuration_bart.py": [
            "function: BartConfig.is_valid_mbart\nclass: MBartConfig"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "src/transformers/tokenization_bart.py": [
                    "function: MBartTokenizer.__init__\nline: 119\nline: 126\nline: 127"
                ],
                "src/transformers/tokenization_utils.py": [
                    ""
                ],
                "src/transformers/configuration_bart.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/tokenization_bart.py b/src/transformers/tokenization_bart.py\nindex 5366039..0f6b1ef 100644\n--- a/src/transformers/tokenization_bart.py\n+++ b/src/transformers/tokenization_bart.py\n@@ -124,6 +124,12 @@ class MBartTokenizer(XLMRobertaTokenizer):\n         self.cur_lang_code = self.lang_code_to_id[\"en_XX\"]\n \n         self.fairseq_tokens_to_ids.update(self.lang_code_to_id)\n+        # Ensure <mask> token id comes after all language codes, matching fairseq.\n+        # In fairseq, language codes are added first, then <mask> is added as the last token.\n+        self.fairseq_tokens_to_ids[\"<mask>\"] = (\n+            self.sp_model_size + self.fairseq_offset + len(FAIRSEQ_LANGUAGE_CODES)\n+        )\n+        # Rebuild reverse mapping to reflect updated ids\n         self.fairseq_ids_to_tokens = {v: k for k, v in self.fairseq_tokens_to_ids.items()}\n         self._additional_special_tokens = list(self.lang_code_to_id.keys())\n         self.set_src_lang_special_tokens(kwargs.get(\"src_lang\", \"en_XX\"))\n",
                "diff --git a/src/transformers/tokenization_bart.py b/src/transformers/tokenization_bart.py\nindex 5366039..5df0461 100644\n--- a/src/transformers/tokenization_bart.py\n+++ b/src/transformers/tokenization_bart.py\n@@ -124,6 +124,9 @@ class MBartTokenizer(XLMRobertaTokenizer):\n         self.cur_lang_code = self.lang_code_to_id[\"en_XX\"]\n \n         self.fairseq_tokens_to_ids.update(self.lang_code_to_id)\n+        # Ensure <mask> token id comes after all language codes, as in fairseq mBART.\n+        mask_id = self.sp_model_size + self.fairseq_offset + len(FAIRSEQ_LANGUAGE_CODES)\n+        self.fairseq_tokens_to_ids[\"<mask>\"] = mask_id\n         self.fairseq_ids_to_tokens = {v: k for k, v in self.fairseq_tokens_to_ids.items()}\n         self._additional_special_tokens = list(self.lang_code_to_id.keys())\n         self.set_src_lang_special_tokens(kwargs.get(\"src_lang\", \"en_XX\"))\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "src/transformers/tokenization_bart.py": [
                    "function: MBartTokenizer.__init__\nline: 126\nline: 127"
                ],
                "src/transformers/tokenization_utils.py": [
                    ""
                ],
                "src/transformers/configuration_bart.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/tokenization_bart.py b/src/transformers/tokenization_bart.py\nindex 5366039..d7a8f0d 100644\n--- a/src/transformers/tokenization_bart.py\n+++ b/src/transformers/tokenization_bart.py\n@@ -124,6 +124,8 @@ class MBartTokenizer(XLMRobertaTokenizer):\n         self.cur_lang_code = self.lang_code_to_id[\"en_XX\"]\n \n         self.fairseq_tokens_to_ids.update(self.lang_code_to_id)\n+        # Ensure <mask> token id is placed after all language codes, matching fairseq.\n+        self.fairseq_tokens_to_ids[\"<mask>\"] = self.sp_model_size + self.fairseq_offset + len(FAIRSEQ_LANGUAGE_CODES)\n         self.fairseq_ids_to_tokens = {v: k for k, v in self.fairseq_tokens_to_ids.items()}\n         self._additional_special_tokens = list(self.lang_code_to_id.keys())\n         self.set_src_lang_special_tokens(kwargs.get(\"src_lang\", \"en_XX\"))\n",
                "diff --git a/src/transformers/tokenization_bart.py b/src/transformers/tokenization_bart.py\nindex 5366039..50b8a9d 100644\n--- a/src/transformers/tokenization_bart.py\n+++ b/src/transformers/tokenization_bart.py\n@@ -123,7 +123,9 @@ class MBartTokenizer(XLMRobertaTokenizer):\n         self.id_to_lang_code = {v: k for k, v in self.lang_code_to_id.items()}\n         self.cur_lang_code = self.lang_code_to_id[\"en_XX\"]\n \n+        # Add language codes first, then set <mask> to come after all language codes (as in fairseq)\n         self.fairseq_tokens_to_ids.update(self.lang_code_to_id)\n+        self.fairseq_tokens_to_ids[\"<mask>\"] = self.sp_model_size + self.fairseq_offset + len(FAIRSEQ_LANGUAGE_CODES)\n         self.fairseq_ids_to_tokens = {v: k for k, v in self.fairseq_tokens_to_ids.items()}\n         self._additional_special_tokens = list(self.lang_code_to_id.keys())\n         self.set_src_lang_special_tokens(kwargs.get(\"src_lang\", \"en_XX\"))\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/src/transformers/tokenization_bart.py b/src/transformers/tokenization_bart.py\nindex 5366039..0f6b1ef 100644\n--- a/src/transformers/tokenization_bart.py\n+++ b/src/transformers/tokenization_bart.py\n@@ -124,6 +124,12 @@ class MBartTokenizer(XLMRobertaTokenizer):\n         self.cur_lang_code = self.lang_code_to_id[\"en_XX\"]\n \n         self.fairseq_tokens_to_ids.update(self.lang_code_to_id)\n+        # Ensure <mask> token id comes after all language codes, matching fairseq.\n+        # In fairseq, language codes are added first, then <mask> is added as the last token.\n+        self.fairseq_tokens_to_ids[\"<mask>\"] = (\n+            self.sp_model_size + self.fairseq_offset + len(FAIRSEQ_LANGUAGE_CODES)\n+        )\n+        # Rebuild reverse mapping to reflect updated ids\n         self.fairseq_ids_to_tokens = {v: k for k, v in self.fairseq_tokens_to_ids.items()}\n         self._additional_special_tokens = list(self.lang_code_to_id.keys())\n         self.set_src_lang_special_tokens(kwargs.get(\"src_lang\", \"en_XX\"))\n"
}