{
    "instance_id": "SWE-PolyBench__python__maintenance__bugfix__14ce2fa9",
    "original_id": "huggingface__transformers-23223",
    "1_model_selected_files": [
        "src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py",
        "src/transformers/feature_extraction_sequence_utils.py",
        "src/transformers/feature_extraction_utils.py",
        "src/transformers/models/wav2vec2/processing_wav2vec2.py"
    ],
    "2_embedding_selected_files": [],
    "3_final_combined_files": [
        "src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py",
        "src/transformers/feature_extraction_sequence_utils.py",
        "src/transformers/feature_extraction_utils.py"
    ],
    "4_related_elements": {
        "src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py": [
            "function: Wav2Vec2FeatureExtractor.__call__\nfunction: Wav2Vec2FeatureExtractor.zero_mean_unit_var_norm"
        ],
        "src/transformers/feature_extraction_sequence_utils.py": [
            "function: SequenceFeatureExtractor.pad\nfunction: SequenceFeatureExtractor._pad\nfunction: SequenceFeatureExtractor._truncate"
        ],
        "src/transformers/feature_extraction_utils.py": [
            "function: BatchFeature.convert_to_tensors"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py": [
                    "function: Wav2Vec2FeatureExtractor.__call__\nline: 104\nline: 184\nline: 185\nline: 186\nline: 187\nline: 190\nline: 191\nline: 192"
                ],
                "src/transformers/feature_extraction_sequence_utils.py": [
                    ""
                ],
                "src/transformers/feature_extraction_utils.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py b/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py\nindex 3453146..408ebe9 100644\n--- a/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py\n+++ b/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py\n@@ -181,13 +181,28 @@ class Wav2Vec2FeatureExtractor(SequenceFeatureExtractor):\n                 \"Failing to do so can result in silent errors that might be hard to debug.\"\n             )\n \n-        is_batched = bool(\n-            isinstance(raw_speech, (list, tuple))\n-            and (isinstance(raw_speech[0], np.ndarray) or isinstance(raw_speech[0], (tuple, list)))\n-        )\n+        # Determine whether the input is batched.\n+        # Accept lists/tuples of arrays or lists, multi-dimensional numpy arrays,\n+        # and multi-dimensional torch tensors as batched inputs.\n+        try:\n+            import torch  # optional dependency\n+        except Exception:\n+            torch = None\n+\n+        if isinstance(raw_speech, (list, tuple)):\n+            is_batched = isinstance(raw_speech[0], (np.ndarray, list, tuple))\n+        elif isinstance(raw_speech, np.ndarray):\n+            is_batched = raw_speech.ndim > 1\n+        elif torch is not None and isinstance(raw_speech, torch.Tensor):\n+            is_batched = raw_speech.ndim > 1\n+        else:\n+            is_batched = False\n \n-        # always return batch\n-        if not is_batched:\n+        # always return batch as a list of sequences\n+        if is_batched:\n+            if not isinstance(raw_speech, (list, tuple)):\n+                raw_speech = list(raw_speech)\n+        else:\n             raw_speech = [raw_speech]\n \n         # convert into correct format for padding\n@@ -234,4 +249,4 @@ class Wav2Vec2FeatureExtractor(SequenceFeatureExtractor):\n         if return_tensors is not None:\n             padded_inputs = padded_inputs.convert_to_tensors(return_tensors)\n \n-        return padded_inputs\n+        return padded_inputs\n",
                "diff --git a/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py b/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py\nindex 3453146..30d5a45 100644\n--- a/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py\n+++ b/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py\n@@ -181,13 +181,24 @@ class Wav2Vec2FeatureExtractor(SequenceFeatureExtractor):\n                 \"Failing to do so can result in silent errors that might be hard to debug.\"\n             )\n \n+        # Determine whether the input is batched.\n+        # Accept list/tuple batches as before, but also accept array-like inputs (e.g., numpy/torch tensors)\n+        # with a leading batch dimension (ndim > 1). In that case, we will split along the first dimension\n+        # to create a list of examples so downstream padding logic can operate uniformly on lists.\n+        is_list_tuple = isinstance(raw_speech, (list, tuple))\n+        is_array_like_batched = (not is_list_tuple) and hasattr(raw_speech, \"ndim\") and getattr(raw_speech, \"ndim\") is not None and raw_speech.ndim > 1\n+\n         is_batched = bool(\n-            isinstance(raw_speech, (list, tuple))\n-            and (isinstance(raw_speech[0], np.ndarray) or isinstance(raw_speech[0], (tuple, list)))\n+            (is_list_tuple and (isinstance(raw_speech[0], np.ndarray) or hasattr(raw_speech[0], \"ndim\") or isinstance(raw_speech[0], (tuple, list))))\n+            or is_array_like_batched\n         )\n \n         # always return batch\n-        if not is_batched:\n+        if is_array_like_batched:\n+            # Convert array-like batched input (e.g., np.ndarray/torch.Tensor with shape [batch, ...])\n+            # into a list of individual examples.\n+            raw_speech = list(raw_speech)\n+        elif not is_batched:\n             raw_speech = [raw_speech]\n \n         # convert into correct format for padding\n@@ -234,4 +245,4 @@ class Wav2Vec2FeatureExtractor(SequenceFeatureExtractor):\n         if return_tensors is not None:\n             padded_inputs = padded_inputs.convert_to_tensors(return_tensors)\n \n-        return padded_inputs\n+        return padded_inputs\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py": [
                    "function: Wav2Vec2FeatureExtractor.__call__\nline: 104\nline: 184\nline: 185\nline: 186\nline: 187\nline: 189\nline: 190\nline: 191\nline: 192"
                ],
                "src/transformers/feature_extraction_sequence_utils.py": [
                    "function: FeatureExtractionMixin.pad\nline: 143"
                ],
                "src/transformers/feature_extraction_utils.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py b/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py\nindex 3453146..02d0e0b 100644\n--- a/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py\n+++ b/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py\n@@ -181,10 +181,25 @@ class Wav2Vec2FeatureExtractor(SequenceFeatureExtractor):\n                 \"Failing to do so can result in silent errors that might be hard to debug.\"\n             )\n \n-        is_batched = bool(\n-            isinstance(raw_speech, (list, tuple))\n-            and (isinstance(raw_speech[0], np.ndarray) or isinstance(raw_speech[0], (tuple, list)))\n-        )\n+        # Determine whether the input is batched.\n+        # - List/Tuple of arrays or list/tuples -> batched\n+        # - Numpy array with ndim > 1 -> batched\n+        # - Torch tensor with ndim > 1 -> batched\n+        is_batched = False\n+        if isinstance(raw_speech, (list, tuple)):\n+            is_batched = len(raw_speech) > 0 and (\n+                isinstance(raw_speech[0], np.ndarray) or isinstance(raw_speech[0], (tuple, list))\n+            )\n+        elif isinstance(raw_speech, np.ndarray):\n+            is_batched = raw_speech.ndim > 1\n+        else:\n+            try:\n+                import torch  # noqa: F401\n+                if isinstance(raw_speech, torch.Tensor):\n+                    is_batched = raw_speech.ndim > 1\n+            except Exception:\n+                # torch is not available or raw_speech is not a torch.Tensor\n+                pass\n \n         # always return batch\n         if not is_batched:\n@@ -234,4 +249,4 @@ class Wav2Vec2FeatureExtractor(SequenceFeatureExtractor):\n         if return_tensors is not None:\n             padded_inputs = padded_inputs.convert_to_tensors(return_tensors)\n \n-        return padded_inputs\n+        return padded_inputs\n",
                "diff --git a/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py b/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py\nindex 3453146..3adc889 100644\n--- a/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py\n+++ b/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py\n@@ -181,14 +181,19 @@ class Wav2Vec2FeatureExtractor(SequenceFeatureExtractor):\n                 \"Failing to do so can result in silent errors that might be hard to debug.\"\n             )\n \n-        is_batched = bool(\n-            isinstance(raw_speech, (list, tuple))\n-            and (isinstance(raw_speech[0], np.ndarray) or isinstance(raw_speech[0], (tuple, list)))\n+        # determine whether the input is a batch\n+        is_list_batch = isinstance(raw_speech, (list, tuple)) and (\n+            isinstance(raw_speech[0], np.ndarray) or isinstance(raw_speech[0], (tuple, list))\n         )\n+        has_ndim_batch = hasattr(raw_speech, \"ndim\") and getattr(raw_speech, \"ndim\") > 1\n+        is_batched = bool(is_list_batch or has_ndim_batch)\n \n         # always return batch\n         if not is_batched:\n             raw_speech = [raw_speech]\n+        elif has_ndim_batch and not is_list_batch:\n+            # If a batched numpy/torch tensor is passed, split it into a list of single examples\n+            raw_speech = [raw_speech[i] for i in range(len(raw_speech))]\n \n         # convert into correct format for padding\n         encoded_inputs = BatchFeature({\"input_values\": raw_speech})\n@@ -234,4 +239,4 @@ class Wav2Vec2FeatureExtractor(SequenceFeatureExtractor):\n         if return_tensors is not None:\n             padded_inputs = padded_inputs.convert_to_tensors(return_tensors)\n \n-        return padded_inputs\n+        return padded_inputs\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py b/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py\nindex 3453146..408ebe9 100644\n--- a/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py\n+++ b/src/transformers/models/wav2vec2/feature_extraction_wav2vec2.py\n@@ -181,13 +181,28 @@ class Wav2Vec2FeatureExtractor(SequenceFeatureExtractor):\n                 \"Failing to do so can result in silent errors that might be hard to debug.\"\n             )\n \n-        is_batched = bool(\n-            isinstance(raw_speech, (list, tuple))\n-            and (isinstance(raw_speech[0], np.ndarray) or isinstance(raw_speech[0], (tuple, list)))\n-        )\n+        # Determine whether the input is batched.\n+        # Accept lists/tuples of arrays or lists, multi-dimensional numpy arrays,\n+        # and multi-dimensional torch tensors as batched inputs.\n+        try:\n+            import torch  # optional dependency\n+        except Exception:\n+            torch = None\n+\n+        if isinstance(raw_speech, (list, tuple)):\n+            is_batched = isinstance(raw_speech[0], (np.ndarray, list, tuple))\n+        elif isinstance(raw_speech, np.ndarray):\n+            is_batched = raw_speech.ndim > 1\n+        elif torch is not None and isinstance(raw_speech, torch.Tensor):\n+            is_batched = raw_speech.ndim > 1\n+        else:\n+            is_batched = False\n \n-        # always return batch\n-        if not is_batched:\n+        # always return batch as a list of sequences\n+        if is_batched:\n+            if not isinstance(raw_speech, (list, tuple)):\n+                raw_speech = list(raw_speech)\n+        else:\n             raw_speech = [raw_speech]\n \n         # convert into correct format for padding\n@@ -234,4 +249,4 @@ class Wav2Vec2FeatureExtractor(SequenceFeatureExtractor):\n         if return_tensors is not None:\n             padded_inputs = padded_inputs.convert_to_tensors(return_tensors)\n \n-        return padded_inputs\n+        return padded_inputs\n"
}