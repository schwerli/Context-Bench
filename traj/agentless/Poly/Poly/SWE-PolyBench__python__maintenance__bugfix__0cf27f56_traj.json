{
    "instance_id": "SWE-PolyBench__python__maintenance__bugfix__0cf27f56",
    "original_id": "keras-team__keras-19973",
    "1_model_selected_files": [
        "keras/src/layers/attention/multi_head_attention.py",
        "keras/src/layers/attention/multi_head_attention_test.py",
        "keras/src/layers/input_spec.py",
        "keras/src/utils/argument_validation.py"
    ],
    "2_embedding_selected_files": [
        "keras/src/layers/attention/multi_head_attention_test.py",
        "keras/src/layers/attention/grouped_query_attention_test.py",
        "keras/src/layers/attention/multi_head_attention.py",
        "keras/src/layers/attention/attention_test.py",
        "keras/src/layers/attention/grouped_query_attention.py",
        "keras/src/layers/attention/attention.py",
        "keras/src/layers/attention/additive_attention_test.py",
        "keras/src/ops/nn_test.py",
        "keras/src/ops/image_test.py",
        "keras/src/ops/numpy_test.py",
        "keras/src/layers/preprocessing/hashed_crossing_test.py",
        "keras/src/layers/preprocessing/hashing_test.py",
        "keras/src/layers/reshaping/reshape_test.py",
        "keras/src/layers/preprocessing/resizing_test.py",
        "keras/src/layers/reshaping/up_sampling1d_test.py",
        "keras/src/layers/convolutional/conv_transpose_test.py",
        "keras/src/ops/core_test.py",
        "keras/src/models/sequential_test.py",
        "benchmarks/layer_benchmark/attention_benchmark.py",
        "keras/src/layers/layer_test.py",
        "keras/src/layers/preprocessing/random_brightness_test.py",
        "keras/src/layers/preprocessing/index_lookup_test.py",
        "keras/src/layers/convolutional/conv_test.py",
        "keras/src/applications/imagenet_utils_test.py",
        "keras/src/layers/reshaping/up_sampling3d_test.py",
        "keras/src/layers/attention/additive_attention.py",
        "keras/src/layers/rnn/rnn_test.py",
        "keras/src/layers/normalization/batch_normalization_test.py",
        "keras/src/ops/math_test.py",
        "keras/src/ops/function_test.py",
        "keras/src/layers/core/input_layer_test.py",
        "keras/src/layers/merging/merging_test.py",
        "keras/src/layers/reshaping/flatten_test.py",
        "keras/src/backend/common/keras_tensor_test.py",
        "keras/src/ops/operation_utils_test.py",
        "keras/src/ops/linalg_test.py",
        "keras/src/layers/core/embedding_test.py"
    ],
    "3_final_combined_files": [
        "keras/src/layers/attention/multi_head_attention.py",
        "keras/src/layers/attention/multi_head_attention_test.py",
        "keras/src/layers/input_spec.py",
        "keras/src/layers/attention/grouped_query_attention_test.py"
    ],
    "4_related_elements": {
        "keras/src/layers/attention/multi_head_attention.py": [
            "function: MultiHeadAttention.build\nfunction: MultiHeadAttention.call\nfunction: MultiHeadAttention.compute_output_spec\nfunction: MultiHeadAttention.compute_output_shape\nfunction: _get_output_shape"
        ],
        "keras/src/layers/input_spec.py": [
            "class: InputSpec\nfunction: assert_input_compatibility"
        ],
        "keras/src/layers/attention/multi_head_attention_test.py": [
            "function: MultiHeadAttentionTest.test_shape_mismatch_error\nfunction: MultiHeadAttentionTest.test_basics\nfunction: MultiHeadAttentionTest.test_compute_output_shape"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "keras/src/layers/attention/multi_head_attention.py": [
                    "function: MultiHeadAttention.compute_output_shape\nline: 592\nline: 593\nline: 594\nline: 595\nline: 596\nline: 597\nline: 598"
                ],
                "keras/src/layers/attention/multi_head_attention_test.py": [
                    "function: MultiHeadAttentionTest.test_shape_mismatch_error\nline: 132\nline: 133\nline: 134\nline: 135\nline: 136\nline: 137\nline: 138\nline: 139\nline: 140\nline: 141\nline: 142\nline: 143\nline: 144\nline: 145\nline: 146\nline: 147\nline: 148\nline: 149\nline: 150\nline: 151"
                ],
                "keras/src/layers/input_spec.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/keras/src/layers/attention/multi_head_attention.py b/keras/src/layers/attention/multi_head_attention.py\nindex c2e2f69..468e3c3 100644\n--- a/keras/src/layers/attention/multi_head_attention.py\n+++ b/keras/src/layers/attention/multi_head_attention.py\n@@ -589,13 +589,7 @@ class MultiHeadAttention(Layer):\n         if key_shape is None:\n             key_shape = value_shape\n \n-        if query_shape[-1] != value_shape[-1]:\n-            raise ValueError(\n-                \"The last dimension of `query_shape` and `value_shape` \"\n-                f\"must be equal, but are {query_shape[-1]}, {value_shape[-1]}. \"\n-                \"Received: query_shape={query_shape}, value_shape={value_shape}\"\n-            )\n-\n+        # `value` and `key` must match on all dimensions except the last one.\n         if value_shape[1:-1] != key_shape[1:-1]:\n             raise ValueError(\n                 \"All dimensions of `value` and `key`, except the last one, \"\n@@ -603,6 +597,8 @@ class MultiHeadAttention(Layer):\n                 f\"key_shape={key_shape}\"\n             )\n \n+        # The output shape by default matches the query shape (unless an explicit\n+        # `output_shape` is provided).\n         if self._output_shape:\n             return query_shape[:-1] + self._output_shape\n \n",
                "diff --git a/keras/src/layers/attention/multi_head_attention.py b/keras/src/layers/attention/multi_head_attention.py\nindex c2e2f69..a4e6cf1 100644\n--- a/keras/src/layers/attention/multi_head_attention.py\n+++ b/keras/src/layers/attention/multi_head_attention.py\n@@ -589,12 +589,9 @@ class MultiHeadAttention(Layer):\n         if key_shape is None:\n             key_shape = value_shape\n \n-        if query_shape[-1] != value_shape[-1]:\n-            raise ValueError(\n-                \"The last dimension of `query_shape` and `value_shape` \"\n-                f\"must be equal, but are {query_shape[-1]}, {value_shape[-1]}. \"\n-                \"Received: query_shape={query_shape}, value_shape={value_shape}\"\n-            )\n+        # Note: Do not assert equality of the last dimension of `query_shape` and\n+        # `value_shape`. They can differ (e.g., value_dim != query_dim), and the\n+        # layer applies internal projections to reconcile dimensions.\n \n         if value_shape[1:-1] != key_shape[1:-1]:\n             raise ValueError(\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "keras/src/layers/attention/multi_head_attention.py": [
                    "function: MultiHeadAttention.compute_output_shape\nline: 592\nline: 593\nline: 594\nline: 595\nline: 596\nline: 597"
                ],
                "keras/src/layers/attention/multi_head_attention_test.py": [
                    "function: MultiHeadAttentionTest.test_shape_mismatch_error\nline: 132\nline: 133\nline: 134\nline: 135\nline: 136\nline: 137\nline: 138\nline: 139\nline: 140\nline: 141\nline: 142\nline: 143\nline: 144\nline: 145\nline: 146\nline: 147\nline: 148\nline: 149\nline: 150\nline: 151"
                ],
                "keras/src/layers/input_spec.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/keras/src/layers/attention/multi_head_attention.py b/keras/src/layers/attention/multi_head_attention.py\nindex c2e2f69..03fa8f1 100644\n--- a/keras/src/layers/attention/multi_head_attention.py\n+++ b/keras/src/layers/attention/multi_head_attention.py\n@@ -593,7 +593,7 @@ class MultiHeadAttention(Layer):\n             raise ValueError(\n                 \"The last dimension of `query_shape` and `value_shape` \"\n                 f\"must be equal, but are {query_shape[-1]}, {value_shape[-1]}. \"\n-                \"Received: query_shape={query_shape}, value_shape={value_shape}\"\n+                f\"Received: query_shape={query_shape}, value_shape={value_shape}\"\n             )\n \n         if value_shape[1:-1] != key_shape[1:-1]:\n",
                "diff --git a/keras/src/layers/attention/multi_head_attention.py b/keras/src/layers/attention/multi_head_attention.py\nindex c2e2f69..2f7f691 100644\n--- a/keras/src/layers/attention/multi_head_attention.py\n+++ b/keras/src/layers/attention/multi_head_attention.py\n@@ -589,13 +589,10 @@ class MultiHeadAttention(Layer):\n         if key_shape is None:\n             key_shape = value_shape\n \n-        if query_shape[-1] != value_shape[-1]:\n-            raise ValueError(\n-                \"The last dimension of `query_shape` and `value_shape` \"\n-                f\"must be equal, but are {query_shape[-1]}, {value_shape[-1]}. \"\n-                \"Received: query_shape={query_shape}, value_shape={value_shape}\"\n-            )\n-\n+        # MultiHeadAttention allows the last dimensions of `query` and `value`\n+        # to differ, since inputs are projected internally to head dimensions.\n+        # We only require `value` and `key` to have matching dimensions except\n+        # for the last feature dimension.\n         if value_shape[1:-1] != key_shape[1:-1]:\n             raise ValueError(\n                 \"All dimensions of `value` and `key`, except the last one, \"\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/keras/src/layers/attention/multi_head_attention.py b/keras/src/layers/attention/multi_head_attention.py\nindex c2e2f69..468e3c3 100644\n--- a/keras/src/layers/attention/multi_head_attention.py\n+++ b/keras/src/layers/attention/multi_head_attention.py\n@@ -589,13 +589,7 @@ class MultiHeadAttention(Layer):\n         if key_shape is None:\n             key_shape = value_shape\n \n-        if query_shape[-1] != value_shape[-1]:\n-            raise ValueError(\n-                \"The last dimension of `query_shape` and `value_shape` \"\n-                f\"must be equal, but are {query_shape[-1]}, {value_shape[-1]}. \"\n-                \"Received: query_shape={query_shape}, value_shape={value_shape}\"\n-            )\n-\n+        # `value` and `key` must match on all dimensions except the last one.\n         if value_shape[1:-1] != key_shape[1:-1]:\n             raise ValueError(\n                 \"All dimensions of `value` and `key`, except the last one, \"\n@@ -603,6 +597,8 @@ class MultiHeadAttention(Layer):\n                 f\"key_shape={key_shape}\"\n             )\n \n+        # The output shape by default matches the query shape (unless an explicit\n+        # `output_shape` is provided).\n         if self._output_shape:\n             return query_shape[:-1] + self._output_shape\n \n"
}