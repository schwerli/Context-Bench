{
    "instance_id": "SWE-PolyBench__python__evolution__feature__7fe6d907",
    "original_id": "huggingface__transformers-27463",
    "1_model_selected_files": [
        "src/transformers/models/sam/image_processing_sam.py",
        "src/transformers/models/sam/processing_sam.py",
        "src/transformers/image_transforms.py",
        "src/transformers/image_processing_utils.py",
        "src/transformers/models/sam/__init__.py"
    ],
    "2_embedding_selected_files": [],
    "3_final_combined_files": [
        "src/transformers/models/sam/image_processing_sam.py",
        "src/transformers/models/sam/processing_sam.py",
        "src/transformers/image_transforms.py"
    ],
    "4_related_elements": {
        "src/transformers/models/sam/image_processing_sam.py": [
            "function: SamImageProcessor.__init__\nfunction: SamImageProcessor.preprocess\nfunction: SamImageProcessor.resize\nfunction: SamImageProcessor.pad_image\nfunction: SamImageProcessor._get_preprocess_shape\nfunction: SamImageProcessor.post_process_masks\nvariable: SamImageProcessor.model_input_names"
        ],
        "src/transformers/models/sam/processing_sam.py": [
            "function: SamProcessor.__call__\nfunction: SamProcessor._normalize_and_convert\nvariable: SamProcessor.model_input_names"
        ],
        "src/transformers/image_transforms.py": [
            "function: resize\nfunction: pad"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "src/transformers/models/sam/image_processing_sam.py": [
                    "function: SamImageProcessor.__init__\nfunction: SamImageProcessor.preprocess"
                ],
                "src/transformers/models/sam/processing_sam.py": [
                    "function: SamProcessor.__call__"
                ],
                "src/transformers/image_transforms.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/sam/image_processing_sam.py b/src/transformers/models/sam/image_processing_sam.py\nindex 31c56d3..ee9b527 100644\n--- a/src/transformers/models/sam/image_processing_sam.py\n+++ b/src/transformers/models/sam/image_processing_sam.py\n@@ -239,6 +239,7 @@ class SamImageProcessor(BaseImageProcessor):\n     def preprocess(\n         self,\n         images: ImageInput,\n+        masks: ImageInput = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[Dict[str, int]] = None,\n         resample: Optional[\"PILImageResampling\"] = None,\n@@ -256,12 +257,15 @@ class SamImageProcessor(BaseImageProcessor):\n         **kwargs,\n     ):\n         \"\"\"\n-        Preprocess an image or batch of images.\n+        Preprocess an image or batch of images. Optionally preprocess associated masks for fine-tuning.\n \n         Args:\n             images (`ImageInput`):\n                 Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            masks (`ImageInput`, *optional*):\n+                Mask(s) to preprocess. If provided, masks are resized such that the longest edge is 256 and padded to\n+                256x256 to match SAM's low-resolution mask output shape.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n@@ -386,12 +390,51 @@ class SamImageProcessor(BaseImageProcessor):\n         images = [\n             to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n         ]\n+\n+        # Optional mask preprocessing for fine-tuning: resize/pad masks to 256x256 using nearest-neighbor.\n+        processed_masks = None\n+        if masks is not None:\n+            masks = make_list_of_images(masks)\n+            if not valid_images(masks):\n+                raise ValueError(\n+                    \"Invalid mask type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                    \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+                )\n+            # Convert masks to numpy arrays without RGB conversion\n+            masks = [to_numpy_array(mask) for mask in masks]\n+            mask_input_data_format = infer_channel_dimension_format(masks[0])\n+\n+            # Resize masks to longest_edge=256 using nearest neighbor to preserve labels\n+            mask_target_size = {\"longest_edge\": 256}\n+            mask_target_size = get_size_dict(max_size=mask_target_size, default_to_square=False)\n+            masks = [\n+                self.resize(\n+                    image=mask,\n+                    size=mask_target_size,\n+                    resample=PILImageResampling.NEAREST,\n+                    input_data_format=mask_input_data_format,\n+                )\n+                for mask in masks\n+            ]\n+\n+            # Pad masks to 256x256\n+            mask_pad_size = {\"height\": 256, \"width\": 256}\n+            mask_pad_size = get_size_dict(mask_pad_size, default_to_square=True)\n+            processed_masks = [\n+                self.pad_image(image=mask, pad_size=mask_pad_size, input_data_format=mask_input_data_format)\n+                for mask in masks\n+            ]\n+\n+        data_dict = {\n+            \"pixel_values\": images,\n+            \"original_sizes\": original_sizes,\n+            \"reshaped_input_sizes\": reshaped_input_sizes,\n+        }\n+        if processed_masks is not None:\n+            data_dict[\"ground_truth_mask\"] = processed_masks\n+\n         encoded_outputs = BatchFeature(\n-            data={\n-                \"pixel_values\": images,\n-                \"original_sizes\": original_sizes,\n-                \"reshaped_input_sizes\": reshaped_input_sizes,\n-            },\n+            data=data_dict,\n             tensor_type=return_tensors,\n         )\n         return encoded_outputs\ndiff --git a/src/transformers/models/sam/processing_sam.py b/src/transformers/models/sam/processing_sam.py\nindex 1c554a4..5aba4b4 100644\n--- a/src/transformers/models/sam/processing_sam.py\n+++ b/src/transformers/models/sam/processing_sam.py\n@@ -57,6 +57,7 @@ class SamProcessor(ProcessorMixin):\n     def __call__(\n         self,\n         images=None,\n+        masks=None,\n         input_points=None,\n         input_labels=None,\n         input_boxes=None,\n@@ -69,6 +70,7 @@ class SamProcessor(ProcessorMixin):\n         \"\"\"\n         encoding_image_processor = self.image_processor(\n             images,\n+            masks=masks,\n             return_tensors=return_tensors,\n             **kwargs,\n         )\n",
                "diff --git a/src/transformers/models/sam/image_processing_sam.py b/src/transformers/models/sam/image_processing_sam.py\nindex 31c56d3..816e288 100644\n--- a/src/transformers/models/sam/image_processing_sam.py\n+++ b/src/transformers/models/sam/image_processing_sam.py\n@@ -118,6 +118,8 @@ class SamImageProcessor(BaseImageProcessor):\n         do_pad: bool = True,\n         pad_size: int = None,\n         do_convert_rgb: bool = True,\n+        mask_longest_edge: int = 256,\n+        mask_pad_size: int = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -127,6 +129,31 @@ class SamImageProcessor(BaseImageProcessor):\n         pad_size = pad_size if pad_size is not None else {\"height\": 1024, \"width\": 1024}\n         pad_size = get_size_dict(pad_size, default_to_square=True)\n \n+        mask_pad_size = mask_pad_size if mask_pad_size is not None else {\"height\": 256, \"width\": 256}\n+        mask_pad_size = get_size_dict(mask_pad_size, default_to_square=True)\n+\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n+        self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n+        self.do_pad = do_pad\n+        self.pad_size = pad_size\n+        self.do_convert_rgb = do_convert_rgb\n+\n+        # Parameters for optional ground-truth mask preprocessing (downsample to 256 and pad to 256x256)\n+        self.mask_longest_edge = mask_longest_edge\n+        self.mask_pad_size = mask_pad_size\n+        super().__init__(**kwargs)\n+        size = size if size is not None else {\"longest_edge\": 1024}\n+        size = get_size_dict(max_size=size, default_to_square=False) if not isinstance(size, dict) else size\n+\n+        pad_size = pad_size if pad_size is not None else {\"height\": 1024, \"width\": 1024}\n+        pad_size = get_size_dict(pad_size, default_to_square=True)\n+\n         self.do_resize = do_resize\n         self.size = size\n         self.resample = resample\n@@ -239,6 +266,7 @@ class SamImageProcessor(BaseImageProcessor):\n     def preprocess(\n         self,\n         images: ImageInput,\n+        masks: Optional[ImageInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[Dict[str, int]] = None,\n         resample: Optional[\"PILImageResampling\"] = None,\n@@ -386,12 +414,53 @@ class SamImageProcessor(BaseImageProcessor):\n         images = [\n             to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n         ]\n+\n+        # Optional preprocessing of ground-truth masks to match SAM low-res output (256x256)\n+        gt_masks = None\n+        if masks is not None:\n+            masks = make_list_of_images(masks)\n+            if not valid_images(masks):\n+                raise ValueError(\n+                    \"Invalid mask type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                    \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+                )\n+            # Convert all masks to numpy arrays\n+            masks = [to_numpy_array(mask) for mask in masks]\n+\n+            # Infer mask input channel dimension format (supports 2D arrays)\n+            mask_input_data_format = infer_channel_dimension_format(masks[0])\n+\n+            # Resize masks so the longest edge is 256 (SAM low-res output size)\n+            if self.do_resize:\n+                masks = [\n+                    self.resize(\n+                        image=mask,\n+                        size={\"longest_edge\": self.mask_longest_edge},\n+                        resample=PILImageResampling.NEAREST,\n+                        input_data_format=mask_input_data_format,\n+                    )\n+                    for mask in masks\n+                ]\n+\n+            # Pad masks to 256x256\n+            if self.do_pad:\n+                masks = [\n+                    self.pad_image(image=mask, pad_size=self.mask_pad_size, input_data_format=mask_input_data_format)\n+                    for mask in masks\n+                ]\n+\n+            gt_masks = masks\n+\n+        data_dict = {\n+            \"pixel_values\": images,\n+            \"original_sizes\": original_sizes,\n+            \"reshaped_input_sizes\": reshaped_input_sizes,\n+        }\n+        if gt_masks is not None:\n+            data_dict[\"ground_truth_masks\"] = gt_masks\n+\n         encoded_outputs = BatchFeature(\n-            data={\n-                \"pixel_values\": images,\n-                \"original_sizes\": original_sizes,\n-                \"reshaped_input_sizes\": reshaped_input_sizes,\n-            },\n+            data=data_dict,\n             tensor_type=return_tensors,\n         )\n         return encoded_outputs\ndiff --git a/src/transformers/models/sam/processing_sam.py b/src/transformers/models/sam/processing_sam.py\nindex 1c554a4..da82502 100644\n--- a/src/transformers/models/sam/processing_sam.py\n+++ b/src/transformers/models/sam/processing_sam.py\n@@ -57,6 +57,7 @@ class SamProcessor(ProcessorMixin):\n     def __call__(\n         self,\n         images=None,\n+        masks=None,\n         input_points=None,\n         input_labels=None,\n         input_boxes=None,\n@@ -65,10 +66,12 @@ class SamProcessor(ProcessorMixin):\n     ) -> BatchEncoding:\n         \"\"\"\n         This method uses [`SamImageProcessor.__call__`] method to prepare image(s) for the model. It also prepares 2D\n-        points and bounding boxes for the model if they are provided.\n+        points and bounding boxes for the model if they are provided. If `masks` are provided, they will be downsampled\n+        and padded to match SAM's low-resolution output size (256x256) for convenient loss computation during fine-tuning.\n         \"\"\"\n         encoding_image_processor = self.image_processor(\n             images,\n+            masks=masks,\n             return_tensors=return_tensors,\n             **kwargs,\n         )\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "src/transformers/models/sam/image_processing_sam.py": [
                    "function: SamImageProcessor.__init__\nfunction: SamImageProcessor.preprocess\nfunction: SamImageProcessor.pad_image"
                ],
                "src/transformers/models/sam/processing_sam.py": [
                    "function: SamProcessor.__call__"
                ],
                "src/transformers/image_transforms.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/sam/image_processing_sam.py b/src/transformers/models/sam/image_processing_sam.py\nindex 31c56d3..2fbebda 100644\n--- a/src/transformers/models/sam/image_processing_sam.py\n+++ b/src/transformers/models/sam/image_processing_sam.py\n@@ -262,6 +262,8 @@ class SamImageProcessor(BaseImageProcessor):\n             images (`ImageInput`):\n                 Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            masks (`ImageInput`, *optional*):\n+                Optional mask or list of masks to preprocess along with the images. Masks will be resized and padded using nearest neighbor resampling to match the expected input size (e.g., 256x256).\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n@@ -386,12 +388,30 @@ class SamImageProcessor(BaseImageProcessor):\n         images = [\n             to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n         ]\n+        # preprocess masks if provided\n+        masks = kwargs.pop(\"masks\", None)\n+        if masks is not None:\n+            masks = make_list_of_images(masks)\n+            masks = [to_numpy_array(mask) for mask in masks]\n+            if do_resize:\n+                masks = [\n+                    self.resize(image=mask, size={\"longest_edge\": 256}, resample=PILImageResampling.NEAREST, input_data_format=input_data_format)\n+                    for mask in masks\n+                ]\n+            if do_pad:\n+                masks = [\n+                    self.pad_image(image=mask, pad_size={\"height\": 256, \"width\": 256}, input_data_format=input_data_format)\n+                    for mask in masks\n+                ]\n+        data_dict = {\n+            \"pixel_values\": images,\n+            \"original_sizes\": original_sizes,\n+            \"reshaped_input_sizes\": reshaped_input_sizes,\n+        }\n+        if masks is not None:\n+            data_dict[\"masks\"] = masks\n         encoded_outputs = BatchFeature(\n-            data={\n-                \"pixel_values\": images,\n-                \"original_sizes\": original_sizes,\n-                \"reshaped_input_sizes\": reshaped_input_sizes,\n-            },\n+            data=data_dict,\n             tensor_type=return_tensors,\n         )\n         return encoded_outputs\n",
                "diff --git a/src/transformers/models/sam/image_processing_sam.py b/src/transformers/models/sam/image_processing_sam.py\nindex 31c56d3..3690f12 100644\n--- a/src/transformers/models/sam/image_processing_sam.py\n+++ b/src/transformers/models/sam/image_processing_sam.py\n@@ -253,10 +253,16 @@ class SamImageProcessor(BaseImageProcessor):\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        masks: Optional[ImageInput] = None,\n+        mask_do_resize: Optional[bool] = True,\n+        mask_size: Optional[Dict[str, int]] = None,\n+        mask_resample: Optional[\"PILImageResampling\"] = None,\n+        mask_do_pad: Optional[bool] = True,\n+        mask_pad_size: Optional[Dict[str, int]] = None,\n         **kwargs,\n     ):\n         \"\"\"\n-        Preprocess an image or batch of images.\n+        Preprocess an image or batch of images, and optionally segmentation masks.\n \n         Args:\n             images (`ImageInput`):\n@@ -286,6 +292,22 @@ class SamImageProcessor(BaseImageProcessor):\n                 `pad_size[\"width\"]` if `do_pad` is set to `True`.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n                 Whether to convert the image to RGB.\n+            masks (`ImageInput`, *optional*):\n+                Ground truth masks to preprocess. If provided, they will be resized and padded to match the decoder\n+                output resolution (default longest edge 256 and padded to 256x256), suitable for fine-tuning loss\n+                computation.\n+            mask_do_resize (`bool`, *optional*, defaults to `True`):\n+                Whether to resize the masks.\n+            mask_size (`Dict[str, int]`, *optional*, defaults to `{\"longest_edge\": 256}`):\n+                Controls the size of the masks after `resize`. The longest edge is resized to `mask_size[\"longest_edge\"]`\n+                whilst preserving the aspect ratio.\n+            mask_resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.NEAREST`):\n+                Resampling filter to use when resizing the masks. Defaults to NEAREST to avoid interpolation artifacts\n+                on discrete labels.\n+            mask_do_pad (`bool`, *optional*, defaults to `True`):\n+                Whether to pad the masks.\n+            mask_pad_size (`Dict[str, int]`, *optional*, defaults to `{\"height\": 256, \"width\": 256}`):\n+                Size to pad masks to if `mask_do_pad` is set to `True`.\n             return_tensors (`str` or `TensorType`, *optional*):\n                 The type of tensors to return. Can be one of:\n                     - Unset: Return a list of `np.ndarray`.\n@@ -305,6 +327,7 @@ class SamImageProcessor(BaseImageProcessor):\n                 - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                 - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n         \"\"\"\n+        # Image processing args\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         size = size if size is not None else self.size\n         size = get_size_dict(max_size=size, default_to_square=False) if not isinstance(size, dict) else size\n@@ -319,6 +342,15 @@ class SamImageProcessor(BaseImageProcessor):\n         pad_size = get_size_dict(pad_size, default_to_square=True)\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n+        # Mask processing args\n+        mask_do_resize = True if mask_do_resize is None else mask_do_resize\n+        mask_size = mask_size if mask_size is not None else {\"longest_edge\": 256}\n+        mask_size = get_size_dict(max_size=mask_size, default_to_square=False) if not isinstance(mask_size, dict) else mask_size\n+        mask_resample = mask_resample if mask_resample is not None else PILImageResampling.NEAREST\n+        mask_do_pad = True if mask_do_pad is None else mask_do_pad\n+        mask_pad_size = mask_pad_size if mask_pad_size is not None else {\"height\": 256, \"width\": 256}\n+        mask_pad_size = get_size_dict(mask_pad_size, default_to_square=True)\n+\n         images = make_list_of_images(images)\n \n         if not valid_images(images):\n@@ -386,12 +418,44 @@ class SamImageProcessor(BaseImageProcessor):\n         images = [\n             to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n         ]\n+\n+        # Optional mask preprocessing (resize + pad to decoder resolution)\n+        ground_truth_masks = None\n+        if masks is not None:\n+            masks = make_list_of_images(masks)\n+            if not valid_images(masks):\n+                raise ValueError(\n+                    \"Invalid mask type. Must be of type PIL.Image.Image, numpy.ndarray, torch.Tensor, tf.Tensor or jax.ndarray.\"\n+                )\n+            # Do not convert color channels for masks; just to numpy\n+            masks = [to_numpy_array(mask) for mask in masks]\n+            mask_input_data_format = infer_channel_dimension_format(masks[0])\n+\n+            if mask_do_resize:\n+                masks = [\n+                    self.resize(image=mask, size=mask_size, resample=mask_resample, input_data_format=mask_input_data_format)\n+                    for mask in masks\n+                ]\n+\n+            if mask_do_pad:\n+                masks = [\n+                    self.pad_image(image=mask, pad_size=mask_pad_size, input_data_format=mask_input_data_format)\n+                    for mask in masks\n+                ]\n+\n+            # Keep masks in their native format (typically HxW). Users can add channel dim in collate_fn if needed.\n+            ground_truth_masks = masks\n+\n+        data_dict = {\n+            \"pixel_values\": images,\n+            \"original_sizes\": original_sizes,\n+            \"reshaped_input_sizes\": reshaped_input_sizes,\n+        }\n+        if ground_truth_masks is not None:\n+            data_dict[\"ground_truth_mask\"] = ground_truth_masks\n+\n         encoded_outputs = BatchFeature(\n-            data={\n-                \"pixel_values\": images,\n-                \"original_sizes\": original_sizes,\n-                \"reshaped_input_sizes\": reshaped_input_sizes,\n-            },\n+            data=data_dict,\n             tensor_type=return_tensors,\n         )\n         return encoded_outputs\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/src/transformers/models/sam/image_processing_sam.py b/src/transformers/models/sam/image_processing_sam.py\nindex 31c56d3..ee9b527 100644\n--- a/src/transformers/models/sam/image_processing_sam.py\n+++ b/src/transformers/models/sam/image_processing_sam.py\n@@ -239,6 +239,7 @@ class SamImageProcessor(BaseImageProcessor):\n     def preprocess(\n         self,\n         images: ImageInput,\n+        masks: ImageInput = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[Dict[str, int]] = None,\n         resample: Optional[\"PILImageResampling\"] = None,\n@@ -256,12 +257,15 @@ class SamImageProcessor(BaseImageProcessor):\n         **kwargs,\n     ):\n         \"\"\"\n-        Preprocess an image or batch of images.\n+        Preprocess an image or batch of images. Optionally preprocess associated masks for fine-tuning.\n \n         Args:\n             images (`ImageInput`):\n                 Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            masks (`ImageInput`, *optional*):\n+                Mask(s) to preprocess. If provided, masks are resized such that the longest edge is 256 and padded to\n+                256x256 to match SAM's low-resolution mask output shape.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n@@ -386,12 +390,51 @@ class SamImageProcessor(BaseImageProcessor):\n         images = [\n             to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n         ]\n+\n+        # Optional mask preprocessing for fine-tuning: resize/pad masks to 256x256 using nearest-neighbor.\n+        processed_masks = None\n+        if masks is not None:\n+            masks = make_list_of_images(masks)\n+            if not valid_images(masks):\n+                raise ValueError(\n+                    \"Invalid mask type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                    \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+                )\n+            # Convert masks to numpy arrays without RGB conversion\n+            masks = [to_numpy_array(mask) for mask in masks]\n+            mask_input_data_format = infer_channel_dimension_format(masks[0])\n+\n+            # Resize masks to longest_edge=256 using nearest neighbor to preserve labels\n+            mask_target_size = {\"longest_edge\": 256}\n+            mask_target_size = get_size_dict(max_size=mask_target_size, default_to_square=False)\n+            masks = [\n+                self.resize(\n+                    image=mask,\n+                    size=mask_target_size,\n+                    resample=PILImageResampling.NEAREST,\n+                    input_data_format=mask_input_data_format,\n+                )\n+                for mask in masks\n+            ]\n+\n+            # Pad masks to 256x256\n+            mask_pad_size = {\"height\": 256, \"width\": 256}\n+            mask_pad_size = get_size_dict(mask_pad_size, default_to_square=True)\n+            processed_masks = [\n+                self.pad_image(image=mask, pad_size=mask_pad_size, input_data_format=mask_input_data_format)\n+                for mask in masks\n+            ]\n+\n+        data_dict = {\n+            \"pixel_values\": images,\n+            \"original_sizes\": original_sizes,\n+            \"reshaped_input_sizes\": reshaped_input_sizes,\n+        }\n+        if processed_masks is not None:\n+            data_dict[\"ground_truth_mask\"] = processed_masks\n+\n         encoded_outputs = BatchFeature(\n-            data={\n-                \"pixel_values\": images,\n-                \"original_sizes\": original_sizes,\n-                \"reshaped_input_sizes\": reshaped_input_sizes,\n-            },\n+            data=data_dict,\n             tensor_type=return_tensors,\n         )\n         return encoded_outputs\ndiff --git a/src/transformers/models/sam/processing_sam.py b/src/transformers/models/sam/processing_sam.py\nindex 1c554a4..5aba4b4 100644\n--- a/src/transformers/models/sam/processing_sam.py\n+++ b/src/transformers/models/sam/processing_sam.py\n@@ -57,6 +57,7 @@ class SamProcessor(ProcessorMixin):\n     def __call__(\n         self,\n         images=None,\n+        masks=None,\n         input_points=None,\n         input_labels=None,\n         input_boxes=None,\n@@ -69,6 +70,7 @@ class SamProcessor(ProcessorMixin):\n         \"\"\"\n         encoding_image_processor = self.image_processor(\n             images,\n+            masks=masks,\n             return_tensors=return_tensors,\n             **kwargs,\n         )\n"
}