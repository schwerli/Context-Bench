{
    "instance_id": "SWE-PolyBench__python__evolution__feature__17a093c8",
    "original_id": "huggingface__transformers-19590",
    "1_model_selected_files": [
        "src/transformers/models/bert/tokenization_bert_tf.py",
        "src/transformers/utils/dummy_tensorflow_text_objects.py"
    ],
    "2_embedding_selected_files": [],
    "3_final_combined_files": [
        "src/transformers/models/bert/tokenization_bert_tf.py",
        "src/transformers/utils/dummy_tensorflow_text_objects.py"
    ],
    "4_related_elements": {
        "src/transformers/models/bert/tokenization_bert_tf.py": [
            "class: TFBertTokenizer"
        ],
        "src/transformers/utils/dummy_tensorflow_text_objects.py": [
            "class: TFBertTokenizer"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "src/transformers/models/bert/tokenization_bert_tf.py": [
                    "line: 6\nclass: TFBertTokenizer\nfunction: TFBertTokenizer.__init__\nline: 67\nline: 68\nline: 69\nfunction: TFBertTokenizer.unpaired_tokenize\nfunction: TFBertTokenizer.get_config\nfunction: TFBertTokenizer.from_pretrained"
                ],
                "src/transformers/utils/dummy_tensorflow_text_objects.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/bert/tokenization_bert_tf.py b/src/transformers/models/bert/tokenization_bert_tf.py\nindex aa04acb..cb1d20b 100644\n--- a/src/transformers/models/bert/tokenization_bert_tf.py\n+++ b/src/transformers/models/bert/tokenization_bert_tf.py\n@@ -1,9 +1,10 @@\n+\n import os\n from typing import List, Union\n \n import tensorflow as tf\n \n-from tensorflow_text import FastBertTokenizer, ShrinkLongestTrimmer, case_fold_utf8, combine_segments, pad_model_inputs\n+from tensorflow_text import FastBertTokenizer, BertTokenizer as TFBertTokenizerLayer, ShrinkLongestTrimmer, case_fold_utf8, combine_segments, pad_model_inputs\n \n from .tokenization_bert import BertTokenizer\n \n@@ -62,13 +63,29 @@ class TFBertTokenizer(tf.keras.layers.Layer):\n         pad_to_multiple_of: int = None,\n         return_token_type_ids: bool = True,\n         return_attention_mask: bool = True,\n+        use_fast: bool = True,\n     ):\n         super().__init__()\n-        self.tf_tokenizer = FastBertTokenizer(\n-            vocab_list, token_out_type=tf.int64, lower_case_nfd_strip_accents=do_lower_case\n-        )\n         self.vocab_list = vocab_list\n         self.do_lower_case = do_lower_case\n+        self.use_fast = use_fast\n+        if use_fast:\n+            self.tf_tokenizer = FastBertTokenizer(\n+                vocab_list, token_out_type=tf.int64, lower_case_nfd_strip_accents=do_lower_case\n+            )\n+        else:\n+            lookup_table = tf.lookup.StaticVocabularyTable(\n+                tf.lookup.KeyValueTensorInitializer(\n+                    keys=vocab_list,\n+                    key_dtype=tf.string,\n+                    values=tf.range(tf.size(vocab_list, out_type=tf.int64), dtype=tf.int64),\n+                    value_dtype=tf.int64,\n+                ),\n+                num_oov_buckets=1,\n+            )\n+            self.tf_tokenizer = TFBertTokenizerLayer(\n+                lookup_table, token_out_type=tf.int64, lower_case=do_lower_case\n+            )\n         self.cls_token_id = cls_token_id or vocab_list.index(\"[CLS]\")\n         self.sep_token_id = sep_token_id or vocab_list.index(\"[SEP]\")\n         self.pad_token_id = pad_token_id or vocab_list.index(\"[PAD]\")\n@@ -136,7 +153,7 @@ class TFBertTokenizer(tf.keras.layers.Layer):\n         return cls.from_tokenizer(tokenizer, **kwargs)\n \n     def unpaired_tokenize(self, texts):\n-        if self.do_lower_case:\n+        if self.use_fast and self.do_lower_case:\n             texts = case_fold_utf8(texts)\n         return self.tf_tokenizer.tokenize(texts)\n \n@@ -219,4 +236,5 @@ class TFBertTokenizer(tf.keras.layers.Layer):\n             \"cls_token_id\": self.cls_token_id,\n             \"sep_token_id\": self.sep_token_id,\n             \"pad_token_id\": self.pad_token_id,\n-        }\n+            \"use_fast\": self.use_fast,\n+        }\n",
                "diff --git a/src/transformers/models/bert/tokenization_bert_tf.py b/src/transformers/models/bert/tokenization_bert_tf.py\nindex aa04acb..b7b24c4 100644\n--- a/src/transformers/models/bert/tokenization_bert_tf.py\n+++ b/src/transformers/models/bert/tokenization_bert_tf.py\n@@ -1,9 +1,10 @@\n+\n import os\n from typing import List, Union\n \n import tensorflow as tf\n \n-from tensorflow_text import FastBertTokenizer, ShrinkLongestTrimmer, case_fold_utf8, combine_segments, pad_model_inputs\n+from tensorflow_text import FastBertTokenizer, BertTokenizer as TFBertTokenizerLayer, ShrinkLongestTrimmer, case_fold_utf8, combine_segments, pad_model_inputs\n \n from .tokenization_bert import BertTokenizer\n \n@@ -62,13 +63,31 @@ class TFBertTokenizer(tf.keras.layers.Layer):\n         pad_to_multiple_of: int = None,\n         return_token_type_ids: bool = True,\n         return_attention_mask: bool = True,\n+        use_fast: bool = True,\n     ):\n         super().__init__()\n-        self.tf_tokenizer = FastBertTokenizer(\n-            vocab_list, token_out_type=tf.int64, lower_case_nfd_strip_accents=do_lower_case\n-        )\n         self.vocab_list = vocab_list\n         self.do_lower_case = do_lower_case\n+        self.use_fast = use_fast\n+        if use_fast:\n+            self.tf_tokenizer = FastBertTokenizer(\n+                vocab_list, token_out_type=tf.int64, lower_case_nfd_strip_accents=do_lower_case\n+            )\n+        else:\n+            keys = tf.constant(vocab_list, dtype=tf.string)\n+            values = tf.range(tf.size(keys, out_type=tf.int64), dtype=tf.int64)\n+            lookup_table = tf.lookup.StaticVocabularyTable(\n+                tf.lookup.KeyValueTensorInitializer(\n+                    keys=keys,\n+                    key_dtype=tf.string,\n+                    values=values,\n+                    value_dtype=tf.int64,\n+                ),\n+                num_oov_buckets=1,\n+            )\n+            self.tf_tokenizer = TFBertTokenizerLayer(\n+                lookup_table, token_out_type=tf.int64, lower_case=do_lower_case\n+            )\n         self.cls_token_id = cls_token_id or vocab_list.index(\"[CLS]\")\n         self.sep_token_id = sep_token_id or vocab_list.index(\"[SEP]\")\n         self.pad_token_id = pad_token_id or vocab_list.index(\"[PAD]\")\n@@ -136,7 +155,7 @@ class TFBertTokenizer(tf.keras.layers.Layer):\n         return cls.from_tokenizer(tokenizer, **kwargs)\n \n     def unpaired_tokenize(self, texts):\n-        if self.do_lower_case:\n+        if self.use_fast and self.do_lower_case:\n             texts = case_fold_utf8(texts)\n         return self.tf_tokenizer.tokenize(texts)\n \n@@ -219,4 +238,5 @@ class TFBertTokenizer(tf.keras.layers.Layer):\n             \"cls_token_id\": self.cls_token_id,\n             \"sep_token_id\": self.sep_token_id,\n             \"pad_token_id\": self.pad_token_id,\n+            \"use_fast\": self.use_fast,\n         }\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "src/transformers/models/bert/tokenization_bert_tf.py": [
                    "line: 6\nfunction: TFBertTokenizer.__init__\nline: 67\nline: 68\nline: 69\nfunction: TFBertTokenizer.unpaired_tokenize\nfunction: TFBertTokenizer.get_config"
                ],
                "src/transformers/utils/dummy_tensorflow_text_objects.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/bert/tokenization_bert_tf.py b/src/transformers/models/bert/tokenization_bert_tf.py\nindex aa04acb..06510ef 100644\n--- a/src/transformers/models/bert/tokenization_bert_tf.py\n+++ b/src/transformers/models/bert/tokenization_bert_tf.py\n@@ -1,9 +1,10 @@\n+\n import os\n from typing import List, Union\n \n import tensorflow as tf\n \n-from tensorflow_text import FastBertTokenizer, ShrinkLongestTrimmer, case_fold_utf8, combine_segments, pad_model_inputs\n+from tensorflow_text import FastBertTokenizer, BertTokenizer as TFBertTokenizerLayer, ShrinkLongestTrimmer, case_fold_utf8, combine_segments, pad_model_inputs\n \n from .tokenization_bert import BertTokenizer\n \n@@ -62,16 +63,33 @@ class TFBertTokenizer(tf.keras.layers.Layer):\n         pad_to_multiple_of: int = None,\n         return_token_type_ids: bool = True,\n         return_attention_mask: bool = True,\n+        use_tf_text_bert_tokenizer: bool = False,\n     ):\n         super().__init__()\n-        self.tf_tokenizer = FastBertTokenizer(\n-            vocab_list, token_out_type=tf.int64, lower_case_nfd_strip_accents=do_lower_case\n-        )\n         self.vocab_list = vocab_list\n         self.do_lower_case = do_lower_case\n         self.cls_token_id = cls_token_id or vocab_list.index(\"[CLS]\")\n         self.sep_token_id = sep_token_id or vocab_list.index(\"[SEP]\")\n         self.pad_token_id = pad_token_id or vocab_list.index(\"[PAD]\")\n+        self.use_tf_text_bert_tokenizer = use_tf_text_bert_tokenizer\n+        if self.use_tf_text_bert_tokenizer:\n+            # Build a lookup table mapping tokens to ids for the standard tf-text BertTokenizer\n+            lookup_table = tf.lookup.StaticVocabularyTable(\n+                tf.lookup.KeyValueTensorInitializer(\n+                    keys=vocab_list,\n+                    key_dtype=tf.string,\n+                    values=tf.range(len(vocab_list), dtype=tf.int64),\n+                    value_dtype=tf.int64,\n+                ),\n+                num_oov_buckets=1,\n+            )\n+            self.tf_tokenizer = TFBertTokenizerLayer(\n+                lookup_table, token_out_type=tf.int64, lower_case=do_lower_case\n+            )\n+        else:\n+            self.tf_tokenizer = FastBertTokenizer(\n+                vocab_list, token_out_type=tf.int64, lower_case_nfd_strip_accents=do_lower_case\n+            )\n         self.paired_trimmer = ShrinkLongestTrimmer(max_length - 3, axis=1)  # Allow room for special tokens\n         self.max_length = max_length\n         self.padding = padding\n@@ -136,7 +154,9 @@ class TFBertTokenizer(tf.keras.layers.Layer):\n         return cls.from_tokenizer(tokenizer, **kwargs)\n \n     def unpaired_tokenize(self, texts):\n-        if self.do_lower_case:\n+        # For the fast tokenizer, we need to normalize/lower-case manually.\n+        # The standard tf-text BertTokenizer can handle lower-casing internally.\n+        if not self.use_tf_text_bert_tokenizer and self.do_lower_case:\n             texts = case_fold_utf8(texts)\n         return self.tf_tokenizer.tokenize(texts)\n \n@@ -219,4 +239,5 @@ class TFBertTokenizer(tf.keras.layers.Layer):\n             \"cls_token_id\": self.cls_token_id,\n             \"sep_token_id\": self.sep_token_id,\n             \"pad_token_id\": self.pad_token_id,\n-        }\n+            \"use_tf_text_bert_tokenizer\": self.use_tf_text_bert_tokenizer,\n+        }\n",
                "diff --git a/src/transformers/models/bert/tokenization_bert_tf.py b/src/transformers/models/bert/tokenization_bert_tf.py\nindex aa04acb..062f1ef 100644\n--- a/src/transformers/models/bert/tokenization_bert_tf.py\n+++ b/src/transformers/models/bert/tokenization_bert_tf.py\n@@ -1,9 +1,10 @@\n+\n import os\n from typing import List, Union\n \n import tensorflow as tf\n \n-from tensorflow_text import FastBertTokenizer, ShrinkLongestTrimmer, case_fold_utf8, combine_segments, pad_model_inputs\n+from tensorflow_text import FastBertTokenizer, ShrinkLongestTrimmer, case_fold_utf8, combine_segments, pad_model_inputs, BertTokenizer as TFBertTokenizerLayer\n \n from .tokenization_bert import BertTokenizer\n \n@@ -62,11 +63,28 @@ class TFBertTokenizer(tf.keras.layers.Layer):\n         pad_to_multiple_of: int = None,\n         return_token_type_ids: bool = True,\n         return_attention_mask: bool = True,\n+        use_fast_bert_tokenizer: bool = True,\n     ):\n         super().__init__()\n-        self.tf_tokenizer = FastBertTokenizer(\n-            vocab_list, token_out_type=tf.int64, lower_case_nfd_strip_accents=do_lower_case\n-        )\n+        self.use_fast_bert_tokenizer = use_fast_bert_tokenizer\n+        if self.use_fast_bert_tokenizer:\n+            self.tf_tokenizer = FastBertTokenizer(\n+                vocab_list, token_out_type=tf.int64, lower_case_nfd_strip_accents=do_lower_case\n+            )\n+        else:\n+            # Build a StaticVocabularyTable for the non-fast TensorFlow Text BertTokenizer\n+            lookup_table = tf.lookup.StaticVocabularyTable(\n+                tf.lookup.KeyValueTensorInitializer(\n+                    keys=tf.constant(vocab_list),\n+                    key_dtype=tf.string,\n+                    values=tf.range(tf.size(vocab_list, out_type=tf.int64), dtype=tf.int64),\n+                    value_dtype=tf.int64,\n+                ),\n+                num_oov_buckets=1,\n+            )\n+            self.tf_tokenizer = TFBertTokenizerLayer(\n+                lookup_table, token_out_type=tf.int64, lower_case=do_lower_case\n+            )\n         self.vocab_list = vocab_list\n         self.do_lower_case = do_lower_case\n         self.cls_token_id = cls_token_id or vocab_list.index(\"[CLS]\")\n@@ -136,7 +154,7 @@ class TFBertTokenizer(tf.keras.layers.Layer):\n         return cls.from_tokenizer(tokenizer, **kwargs)\n \n     def unpaired_tokenize(self, texts):\n-        if self.do_lower_case:\n+        if self.do_lower_case and self.use_fast_bert_tokenizer:\n             texts = case_fold_utf8(texts)\n         return self.tf_tokenizer.tokenize(texts)\n \n@@ -219,4 +237,5 @@ class TFBertTokenizer(tf.keras.layers.Layer):\n             \"cls_token_id\": self.cls_token_id,\n             \"sep_token_id\": self.sep_token_id,\n             \"pad_token_id\": self.pad_token_id,\n+            \"use_fast_bert_tokenizer\": self.use_fast_bert_tokenizer,\n         }\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/src/transformers/models/bert/tokenization_bert_tf.py b/src/transformers/models/bert/tokenization_bert_tf.py\nindex aa04acb..cb1d20b 100644\n--- a/src/transformers/models/bert/tokenization_bert_tf.py\n+++ b/src/transformers/models/bert/tokenization_bert_tf.py\n@@ -1,9 +1,10 @@\n+\n import os\n from typing import List, Union\n \n import tensorflow as tf\n \n-from tensorflow_text import FastBertTokenizer, ShrinkLongestTrimmer, case_fold_utf8, combine_segments, pad_model_inputs\n+from tensorflow_text import FastBertTokenizer, BertTokenizer as TFBertTokenizerLayer, ShrinkLongestTrimmer, case_fold_utf8, combine_segments, pad_model_inputs\n \n from .tokenization_bert import BertTokenizer\n \n@@ -62,13 +63,29 @@ class TFBertTokenizer(tf.keras.layers.Layer):\n         pad_to_multiple_of: int = None,\n         return_token_type_ids: bool = True,\n         return_attention_mask: bool = True,\n+        use_fast: bool = True,\n     ):\n         super().__init__()\n-        self.tf_tokenizer = FastBertTokenizer(\n-            vocab_list, token_out_type=tf.int64, lower_case_nfd_strip_accents=do_lower_case\n-        )\n         self.vocab_list = vocab_list\n         self.do_lower_case = do_lower_case\n+        self.use_fast = use_fast\n+        if use_fast:\n+            self.tf_tokenizer = FastBertTokenizer(\n+                vocab_list, token_out_type=tf.int64, lower_case_nfd_strip_accents=do_lower_case\n+            )\n+        else:\n+            lookup_table = tf.lookup.StaticVocabularyTable(\n+                tf.lookup.KeyValueTensorInitializer(\n+                    keys=vocab_list,\n+                    key_dtype=tf.string,\n+                    values=tf.range(tf.size(vocab_list, out_type=tf.int64), dtype=tf.int64),\n+                    value_dtype=tf.int64,\n+                ),\n+                num_oov_buckets=1,\n+            )\n+            self.tf_tokenizer = TFBertTokenizerLayer(\n+                lookup_table, token_out_type=tf.int64, lower_case=do_lower_case\n+            )\n         self.cls_token_id = cls_token_id or vocab_list.index(\"[CLS]\")\n         self.sep_token_id = sep_token_id or vocab_list.index(\"[SEP]\")\n         self.pad_token_id = pad_token_id or vocab_list.index(\"[PAD]\")\n@@ -136,7 +153,7 @@ class TFBertTokenizer(tf.keras.layers.Layer):\n         return cls.from_tokenizer(tokenizer, **kwargs)\n \n     def unpaired_tokenize(self, texts):\n-        if self.do_lower_case:\n+        if self.use_fast and self.do_lower_case:\n             texts = case_fold_utf8(texts)\n         return self.tf_tokenizer.tokenize(texts)\n \n@@ -219,4 +236,5 @@ class TFBertTokenizer(tf.keras.layers.Layer):\n             \"cls_token_id\": self.cls_token_id,\n             \"sep_token_id\": self.sep_token_id,\n             \"pad_token_id\": self.pad_token_id,\n-        }\n+            \"use_fast\": self.use_fast,\n+        }\n"
}