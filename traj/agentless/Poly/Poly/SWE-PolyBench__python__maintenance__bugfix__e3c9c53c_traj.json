{
    "instance_id": "SWE-PolyBench__python__maintenance__bugfix__e3c9c53c",
    "original_id": "huggingface__transformers-15158",
    "1_model_selected_files": [
        "src/transformers/models/bert/tokenization_bert_fast.py",
        "src/transformers/tokenization_utils_fast.py",
        "src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py",
        "src/transformers/models/bert/tokenization_bert.py"
    ],
    "2_embedding_selected_files": [],
    "3_final_combined_files": [
        "src/transformers/models/bert/tokenization_bert_fast.py",
        "src/transformers/tokenization_utils_fast.py",
        "src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py"
    ],
    "4_related_elements": {
        "src/transformers/models/bert/tokenization_bert_fast.py": [
            "function: BertTokenizerFast.__init__\nvariable: PRETRAINED_INIT_CONFIGURATION\nclass: BertTokenizerFast"
        ],
        "src/transformers/tokenization_utils_fast.py": [
            "function: PreTrainedTokenizerFast.__init__\nfunction: PreTrainedTokenizerFast._save_pretrained\nclass: PreTrainedTokenizerFast\nvariable: TOKENIZER_CONFIG_FILE"
        ],
        "src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py": [
            "function: convert_slow_checkpoint_to_fast\nvariable: TOKENIZER_CLASSES"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "src/transformers/models/bert/tokenization_bert_fast.py": [
                    "function: BertTokenizerFast.__init__\nline: 191\nline: 192\nline: 195\nline: 196\nline: 197\nline: 198\nline: 199"
                ],
                "src/transformers/tokenization_utils_fast.py": [
                    ""
                ],
                "src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/bert/tokenization_bert_fast.py b/src/transformers/models/bert/tokenization_bert_fast.py\nindex 6b5fbff..65b2b32 100644\n--- a/src/transformers/models/bert/tokenization_bert_fast.py\n+++ b/src/transformers/models/bert/tokenization_bert_fast.py\n@@ -189,13 +189,17 @@ class BertTokenizerFast(PreTrainedTokenizerFast):\n         )\n \n         pre_tok_state = json.loads(self.backend_tokenizer.normalizer.__getstate__())\n+        # Make sure the normalizer configuration reflects the user-provided options\n         if (\n             pre_tok_state.get(\"lowercase\", do_lower_case) != do_lower_case\n             or pre_tok_state.get(\"strip_accents\", strip_accents) != strip_accents\n+            or pre_tok_state.get(\"handle_chinese_chars\", tokenize_chinese_chars) != tokenize_chinese_chars\n         ):\n             pre_tok_class = getattr(normalizers, pre_tok_state.pop(\"type\"))\n             pre_tok_state[\"lowercase\"] = do_lower_case\n             pre_tok_state[\"strip_accents\"] = strip_accents\n+            # Align the Rust tokenizer normalizer flag with the Python argument\n+            pre_tok_state[\"handle_chinese_chars\"] = tokenize_chinese_chars\n             self.backend_tokenizer.normalizer = pre_tok_class(**pre_tok_state)\n \n         self.do_lower_case = do_lower_case\n",
                "diff --git a/src/transformers/models/bert/tokenization_bert_fast.py b/src/transformers/models/bert/tokenization_bert_fast.py\nindex 6b5fbff..eb4567d 100644\n--- a/src/transformers/models/bert/tokenization_bert_fast.py\n+++ b/src/transformers/models/bert/tokenization_bert_fast.py\n@@ -192,10 +192,12 @@ class BertTokenizerFast(PreTrainedTokenizerFast):\n         if (\n             pre_tok_state.get(\"lowercase\", do_lower_case) != do_lower_case\n             or pre_tok_state.get(\"strip_accents\", strip_accents) != strip_accents\n+            or pre_tok_state.get(\"handle_chinese_chars\", tokenize_chinese_chars) != tokenize_chinese_chars\n         ):\n             pre_tok_class = getattr(normalizers, pre_tok_state.pop(\"type\"))\n             pre_tok_state[\"lowercase\"] = do_lower_case\n             pre_tok_state[\"strip_accents\"] = strip_accents\n+            pre_tok_state[\"handle_chinese_chars\"] = tokenize_chinese_chars\n             self.backend_tokenizer.normalizer = pre_tok_class(**pre_tok_state)\n \n         self.do_lower_case = do_lower_case\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "src/transformers/models/bert/tokenization_bert_fast.py": [
                    "function: BertTokenizerFast.__init__\nline: 191\nline: 193\nline: 194\nline: 195\nline: 197\nline: 198\nline: 199"
                ],
                "src/transformers/tokenization_utils_fast.py": [
                    ""
                ],
                "src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/bert/tokenization_bert_fast.py b/src/transformers/models/bert/tokenization_bert_fast.py\nindex 6b5fbff..598a78a 100644\n--- a/src/transformers/models/bert/tokenization_bert_fast.py\n+++ b/src/transformers/models/bert/tokenization_bert_fast.py\n@@ -189,13 +189,18 @@ class BertTokenizerFast(PreTrainedTokenizerFast):\n         )\n \n         pre_tok_state = json.loads(self.backend_tokenizer.normalizer.__getstate__())\n+        current_lowercase = pre_tok_state.get(\"lowercase\", do_lower_case)\n+        current_strip_accents = pre_tok_state.get(\"strip_accents\", strip_accents)\n+        current_handle_chinese = pre_tok_state.get(\"handle_chinese_chars\", tokenize_chinese_chars)\n         if (\n-            pre_tok_state.get(\"lowercase\", do_lower_case) != do_lower_case\n-            or pre_tok_state.get(\"strip_accents\", strip_accents) != strip_accents\n+            current_lowercase != do_lower_case\n+            or current_strip_accents != strip_accents\n+            or current_handle_chinese != tokenize_chinese_chars\n         ):\n             pre_tok_class = getattr(normalizers, pre_tok_state.pop(\"type\"))\n             pre_tok_state[\"lowercase\"] = do_lower_case\n             pre_tok_state[\"strip_accents\"] = strip_accents\n+            pre_tok_state[\"handle_chinese_chars\"] = tokenize_chinese_chars\n             self.backend_tokenizer.normalizer = pre_tok_class(**pre_tok_state)\n \n         self.do_lower_case = do_lower_case\n",
                "diff --git a/src/transformers/models/bert/tokenization_bert_fast.py b/src/transformers/models/bert/tokenization_bert_fast.py\nindex 6b5fbff..eb4567d 100644\n--- a/src/transformers/models/bert/tokenization_bert_fast.py\n+++ b/src/transformers/models/bert/tokenization_bert_fast.py\n@@ -192,10 +192,12 @@ class BertTokenizerFast(PreTrainedTokenizerFast):\n         if (\n             pre_tok_state.get(\"lowercase\", do_lower_case) != do_lower_case\n             or pre_tok_state.get(\"strip_accents\", strip_accents) != strip_accents\n+            or pre_tok_state.get(\"handle_chinese_chars\", tokenize_chinese_chars) != tokenize_chinese_chars\n         ):\n             pre_tok_class = getattr(normalizers, pre_tok_state.pop(\"type\"))\n             pre_tok_state[\"lowercase\"] = do_lower_case\n             pre_tok_state[\"strip_accents\"] = strip_accents\n+            pre_tok_state[\"handle_chinese_chars\"] = tokenize_chinese_chars\n             self.backend_tokenizer.normalizer = pre_tok_class(**pre_tok_state)\n \n         self.do_lower_case = do_lower_case\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/src/transformers/models/bert/tokenization_bert_fast.py b/src/transformers/models/bert/tokenization_bert_fast.py\nindex 6b5fbff..65b2b32 100644\n--- a/src/transformers/models/bert/tokenization_bert_fast.py\n+++ b/src/transformers/models/bert/tokenization_bert_fast.py\n@@ -189,13 +189,17 @@ class BertTokenizerFast(PreTrainedTokenizerFast):\n         )\n \n         pre_tok_state = json.loads(self.backend_tokenizer.normalizer.__getstate__())\n+        # Make sure the normalizer configuration reflects the user-provided options\n         if (\n             pre_tok_state.get(\"lowercase\", do_lower_case) != do_lower_case\n             or pre_tok_state.get(\"strip_accents\", strip_accents) != strip_accents\n+            or pre_tok_state.get(\"handle_chinese_chars\", tokenize_chinese_chars) != tokenize_chinese_chars\n         ):\n             pre_tok_class = getattr(normalizers, pre_tok_state.pop(\"type\"))\n             pre_tok_state[\"lowercase\"] = do_lower_case\n             pre_tok_state[\"strip_accents\"] = strip_accents\n+            # Align the Rust tokenizer normalizer flag with the Python argument\n+            pre_tok_state[\"handle_chinese_chars\"] = tokenize_chinese_chars\n             self.backend_tokenizer.normalizer = pre_tok_class(**pre_tok_state)\n \n         self.do_lower_case = do_lower_case\n"
}