{
    "instance_id": "SWE-PolyBench__python__maintenance__bugfix__aaf309d2",
    "original_id": "langchain-ai__langchain-4009",
    "1_model_selected_files": [
        "langchain/callbacks/openai_info.py",
        "langchain/callbacks/utils.py",
        "langchain/llms/openai.py",
        "langchain/chat_models/openai.py"
    ],
    "2_embedding_selected_files": [
        "langchain/llms/openai.py",
        "langchain/chat_models/openai.py",
        "langchain/callbacks/openai_info.py",
        "langchain/llms/forefrontai.py",
        "langchain/llms/stochasticai.py",
        "langchain/llms/gooseai.py",
        "langchain/callbacks/clearml_callback.py",
        "langchain/callbacks/wandb_callback.py",
        "langchain/callbacks/manager.py",
        "langchain/llms/anthropic.py",
        "langchain/callbacks/comet_ml_callback.py",
        "langchain/llms/gpt4all.py",
        "langchain/llms/llamacpp.py",
        "langchain/callbacks/base.py",
        "langchain/llms/cerebriumai.py",
        "langchain/llms/__init__.py",
        "langchain/llms/huggingface_hub.py",
        "langchain/llms/ai21.py",
        "langchain/callbacks/arize_callback.py",
        "langchain/callbacks/__init__.py",
        "langchain/llms/nlpcloud.py",
        "langchain/llms/promptlayer_openai.py",
        "langchain/chat_models/__init__.py",
        "langchain/chat_models/anthropic.py",
        "langchain/llms/cohere.py",
        "langchain/base_language.py",
        "langchain/llms/bananadev.py",
        "langchain/llms/self_hosted_hugging_face.py",
        "langchain/llms/deepinfra.py",
        "langchain/llms/huggingface_endpoint.py",
        "langchain/llms/aleph_alpha.py",
        "langchain/model_laboratory.py",
        "langchain/chains/llm_requests.py",
        "langchain/experimental/autonomous_agents/autogpt/agent.py",
        "langchain/llms/google_palm.py",
        "langchain/llms/predictionguard.py",
        "langchain/llms/huggingface_pipeline.py",
        "langchain/llms/modal.py",
        "langchain/embeddings/openai.py",
        "langchain/llms/self_hosted.py",
        "langchain/chat_models/base.py",
        "langchain/llms/replicate.py",
        "langchain/callbacks/tracers/base.py",
        "langchain/llms/loading.py",
        "langchain/chains/pal/base.py",
        "langchain/chat_models/google_palm.py",
        "langchain/llms/rwkv.py",
        "langchain/llms/writer.py",
        "langchain/callbacks/aim_callback.py",
        "langchain/chat_models/promptlayer_openai.py",
        "langchain/chains/moderation.py",
        "langchain/experimental/__init__.py",
        "langchain/experimental/autonomous_agents/baby_agi/task_execution.py",
        "langchain/chains/pal/__init__.py",
        "langchain/agents/load_tools.py",
        "langchain/llms/base.py",
        "langchain/experimental/autonomous_agents/baby_agi/task_creation.py",
        "langchain/callbacks/utils.py",
        "langchain/chains/llm.py",
        "langchain/llms/sagemaker_endpoint.py",
        "langchain/callbacks/streaming_stdout.py",
        "langchain/llms/petals.py",
        "langchain/embeddings/self_hosted_hugging_face.py",
        "langchain/llms/fake.py",
        "langchain/embeddings/llamacpp.py",
        "langchain/chat_models/azure_openai.py"
    ],
    "3_final_combined_files": [
        "langchain/callbacks/openai_info.py",
        "langchain/llms/openai.py",
        "langchain/callbacks/utils.py",
        "langchain/chat_models/openai.py"
    ],
    "4_related_elements": {
        "langchain/callbacks/openai_info.py": [
            "function: get_openai_model_cost_per_1k_tokens\nclass: OpenAICallbackHandler"
        ],
        "langchain/llms/openai.py": [
            "function: BaseOpenAI.__new__\nfunction: BaseOpenAI._generate\nfunction: BaseOpenAI.create_llm_result\nfunction: BaseOpenAI._invocation_params\nfunction: BaseOpenAI._identifying_params\nfunction: BaseOpenAI.modelname_to_contextsize\nfunction: update_token_usage\nclass: OpenAI\nclass: OpenAIChat"
        ],
        "langchain/callbacks/utils.py": [
            ""
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "langchain/callbacks/openai_info.py": [
                    "function: get_openai_model_cost_per_1k_tokens\nfunction: OpenAICallbackHandler.on_llm_end"
                ],
                "langchain/llms/openai.py": [
                    "function: BaseOpenAI.modelname_to_contextsize"
                ],
                "langchain/callbacks/utils.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/langchain/callbacks/openai_info.py b/langchain/callbacks/openai_info.py\nindex 5051e53..73b7191 100644\n--- a/langchain/callbacks/openai_info.py\n+++ b/langchain/callbacks/openai_info.py\n@@ -1,3 +1,4 @@\n+\n \"\"\"Callback Handler that prints to std out.\"\"\"\n from typing import Any, Dict, List, Optional, Union\n \n@@ -30,11 +31,36 @@ def get_openai_model_cost_per_1k_tokens(\n         \"code-davinci-002\": 0.02,\n     }\n \n-    cost = model_cost_mapping.get(\n-        model_name.lower()\n-        + (\"-completion\" if is_completion and model_name.startswith(\"gpt-4\") else \"\"),\n-        None,\n-    )\n+    # Normalize model names to handle fine-tuned and versioned variants\n+    def _normalize_model_for_cost(name: str) -> str:\n+        n = name.lower()\n+        # New FT format: ft:gpt-3.5-turbo-1106:org:...\n+        if n.startswith(\"ft:\"):\n+            parts = n.split(\":\")\n+            if len(parts) > 1:\n+                n = parts[1]\n+        # Legacy FT format: ada:ft-..., curie:ft-..., davinci:ft-...\n+        elif \":\" in n:\n+            n = n.split(\":\")[0]\n+\n+        # Collapse versioned names to base keys we have pricing for\n+        if n.startswith(\"gpt-4-32k\"):\n+            n = \"gpt-4-32k\"\n+        elif n.startswith(\"gpt-4\"):\n+            n = \"gpt-4\"\n+        elif n.startswith(\"gpt-3.5-turbo\"):\n+            n = \"gpt-3.5-turbo\"\n+        elif n == \"davinci\":\n+            # Use davinci equivalent pricing (same as text-davinci-003/002)\n+            n = \"text-davinci-003\"\n+\n+        return n\n+\n+    normalized = _normalize_model_for_cost(model_name)\n+\n+    key = normalized + (\"-completion\" if is_completion and normalized.startswith(\"gpt-4\") else \"\")\n+\n+    cost = model_cost_mapping.get(key, None)\n     if cost is None:\n         raise ValueError(\n             f\"Unknown model: {model_name}. Please provide a valid OpenAI model name.\"\ndiff --git a/langchain/llms/openai.py b/langchain/llms/openai.py\nindex 1f9d305..b0f090c 100644\n--- a/langchain/llms/openai.py\n+++ b/langchain/llms/openai.py\n@@ -507,7 +507,38 @@ class BaseOpenAI(BaseLLM):\n             \"code-cushman-001\": 2048,\n         }\n \n-        context_size = model_token_mapping.get(modelname, None)\n+        # Normalize model names to handle fine-tuned and versioned variants\n+        def _normalize_model_for_context(name: str) -> str:\n+            n = name\n+            # New FT format: ft:gpt-3.5-turbo-1106:org:...\n+            if n.lower().startswith(\"ft:\"):\n+                parts = n.split(\":\")\n+                if len(parts) > 1:\n+                    n = parts[1]\n+            # Legacy FT format: ada:ft-..., curie:ft-..., davinci:ft-...\n+            elif \":\" in n:\n+                n = n.split(\":\")[0]\n+\n+            # Collapse versioned names to base keys we have context sizes for\n+            nl = n.lower()\n+            if nl.startswith(\"gpt-4-32k\"):\n+                n = \"gpt-4-32k\"\n+            elif nl.startswith(\"gpt-4\"):\n+                n = \"gpt-4\"\n+            elif nl.startswith(\"gpt-3.5-turbo\"):\n+                n = \"gpt-3.5-turbo\"\n+            elif nl == \"davinci\":\n+                n = \"davinci\"\n+            elif nl == \"babbage\":\n+                n = \"babbage\"\n+            elif nl == \"curie\":\n+                n = \"curie\"\n+            elif nl == \"ada\":\n+                n = \"ada\"\n+            return n\n+\n+        normalized = _normalize_model_for_context(modelname)\n+        context_size = model_token_mapping.get(normalized, None)\n \n         if context_size is None:\n             raise ValueError(\n",
                "diff --git a/langchain/callbacks/openai_info.py b/langchain/callbacks/openai_info.py\nindex 5051e53..31b0b3e 100644\n--- a/langchain/callbacks/openai_info.py\n+++ b/langchain/callbacks/openai_info.py\n@@ -1,3 +1,4 @@\n+\n \"\"\"Callback Handler that prints to std out.\"\"\"\n from typing import Any, Dict, List, Optional, Union\n \n@@ -30,16 +31,32 @@ def get_openai_model_cost_per_1k_tokens(\n         \"code-davinci-002\": 0.02,\n     }\n \n-    cost = model_cost_mapping.get(\n-        model_name.lower()\n-        + (\"-completion\" if is_completion and model_name.startswith(\"gpt-4\") else \"\"),\n-        None,\n-    )\n+    # Normalize fine-tuned and variant model names to their base model\n+    normalized = model_name.lower()\n+    # New-style fine-tune names start with \"ft:\"\n+    if normalized.startswith(\"ft:\"):\n+        parts = normalized.split(\":\")\n+        if len(parts) > 1:\n+            normalized = parts[1]\n+    # Old-style fine-tune names use \"<base>:ft-...\"\n+    elif \":\" in normalized:\n+        normalized = normalized.split(\":\")[0]\n+\n+    # Collapse family variants to canonical keys when possible\n+    if normalized.startswith(\"gpt-3.5-turbo\"):\n+        normalized = \"gpt-3.5-turbo\"\n+    elif normalized.startswith(\"gpt-4-32k\"):\n+        normalized = \"gpt-4-32k\"\n+    elif normalized.startswith(\"gpt-4\"):\n+        normalized = \"gpt-4\"\n+\n+    key = normalized + (\"-completion\" if is_completion and normalized.startswith(\"gpt-4\") else \"\")\n+    cost = model_cost_mapping.get(key, None)\n+\n+    # If we cannot determine the cost (e.g., unknown/unsupported model),\n+    # return 0.0 so token counting still works without error.\n     if cost is None:\n-        raise ValueError(\n-            f\"Unknown model: {model_name}. Please provide a valid OpenAI model name.\"\n-            \"Known models are: \" + \", \".join(model_cost_mapping.keys())\n-        )\n+        return 0.0\n \n     return cost\n \ndiff --git a/langchain/llms/openai.py b/langchain/llms/openai.py\nindex 1f9d305..d664ed4 100644\n--- a/langchain/llms/openai.py\n+++ b/langchain/llms/openai.py\n@@ -507,7 +507,26 @@ class BaseOpenAI(BaseLLM):\n             \"code-cushman-001\": 2048,\n         }\n \n-        context_size = model_token_mapping.get(modelname, None)\n+        # Normalize fine-tuned and variant model names to their base model\n+        normalized = modelname.lower()\n+        # New-style fine-tune names start with \"ft:\"\n+        if normalized.startswith(\"ft:\"):\n+            parts = normalized.split(\":\")\n+            if len(parts) > 1:\n+                normalized = parts[1]\n+        # Old-style fine-tune names use \"<base>:ft-...\"\n+        elif \":\" in normalized:\n+            normalized = normalized.split(\":\")[0]\n+\n+        # Collapse family variants to canonical keys when possible\n+        if normalized.startswith(\"gpt-3.5-turbo\"):\n+            normalized = \"gpt-3.5-turbo\"\n+        elif normalized.startswith(\"gpt-4-32k\"):\n+            normalized = \"gpt-4-32k\"\n+        elif normalized.startswith(\"gpt-4\"):\n+            normalized = \"gpt-4\"\n+\n+        context_size = model_token_mapping.get(normalized, None)\n \n         if context_size is None:\n             raise ValueError(\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "langchain/callbacks/openai_info.py": [
                    "function: get_openai_model_cost_per_1k_tokens\nfunction: OpenAICallbackHandler.on_llm_end\nline: 86\nline: 95"
                ],
                "langchain/llms/openai.py": [
                    "function: BaseOpenAI.modelname_to_contextsize"
                ],
                "langchain/callbacks/utils.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/langchain/callbacks/openai_info.py b/langchain/callbacks/openai_info.py\nindex 5051e53..c9da26c 100644\n--- a/langchain/callbacks/openai_info.py\n+++ b/langchain/callbacks/openai_info.py\n@@ -1,3 +1,4 @@\n+\n \"\"\"Callback Handler that prints to std out.\"\"\"\n from typing import Any, Dict, List, Optional, Union\n \n@@ -28,18 +29,46 @@ def get_openai_model_cost_per_1k_tokens(\n         \"text-davinci-003\": 0.02,\n         \"text-davinci-002\": 0.02,\n         \"code-davinci-002\": 0.02,\n+        # Add base davinci for fine-tuned legacy models (e.g., davinci:ft-*)\n+        \"davinci\": 0.02,\n     }\n \n-    cost = model_cost_mapping.get(\n-        model_name.lower()\n-        + (\"-completion\" if is_completion and model_name.startswith(\"gpt-4\") else \"\"),\n-        None,\n+    # First try direct lookup (with special handling for GPT-4 completion pricing)\n+    name_lower = model_name.lower()\n+    direct_key = name_lower + (\n+        \"-completion\" if is_completion and name_lower.startswith(\"gpt-4\") else \"\"\n     )\n+    cost = model_cost_mapping.get(direct_key, None)\n+\n+    # If not found, try to normalize fine-tuned and versioned model names to base models\n     if cost is None:\n-        raise ValueError(\n-            f\"Unknown model: {model_name}. Please provide a valid OpenAI model name.\"\n-            \"Known models are: \" + \", \".join(model_cost_mapping.keys())\n+        base_name = name_lower\n+\n+        # Handle old-style fine-tuned names like \"ada:ft-...\" or \"davinci:ft-...\"\n+        if \":\" in base_name and not base_name.startswith(\"ft:\"):\n+            base_name = base_name.split(\":\", 1)[0]\n+        # Handle new-style fine-tuned names like \"ft:gpt-3.5-turbo-1106:org:...\"\n+        elif base_name.startswith(\"ft:\"):\n+            parts = base_name.split(\":\")\n+            if len(parts) > 1:\n+                base_name = parts[1]\n+\n+        # Normalize versioned families to their canonical keys\n+        if base_name.startswith(\"gpt-3.5-turbo\"):\n+            base_name = \"gpt-3.5-turbo\"\n+        elif base_name.startswith(\"gpt-4-32k\"):\n+            base_name = \"gpt-4-32k\"\n+        elif base_name.startswith(\"gpt-4\"):\n+            base_name = \"gpt-4\"\n+\n+        normalized_key = base_name + (\n+            \"-completion\" if is_completion and base_name.startswith(\"gpt-4\") else \"\"\n         )\n+        cost = model_cost_mapping.get(normalized_key, None)\n+\n+    # As a last resort, if still unknown, return 0.0 so token accounting continues without crashing\n+    if cost is None:\n+        return 0.0\n \n     return cost\n \n",
                "diff --git a/langchain/callbacks/openai_info.py b/langchain/callbacks/openai_info.py\nindex 5051e53..0443a63 100644\n--- a/langchain/callbacks/openai_info.py\n+++ b/langchain/callbacks/openai_info.py\n@@ -1,3 +1,4 @@\n+\n \"\"\"Callback Handler that prints to std out.\"\"\"\n from typing import Any, Dict, List, Optional, Union\n \n@@ -8,6 +9,7 @@ from langchain.schema import AgentAction, AgentFinish, LLMResult\n def get_openai_model_cost_per_1k_tokens(\n     model_name: str, is_completion: bool = False\n ) -> float:\n+    # Base pricing mapping (per 1k tokens). For GPT-4, completion vs prompt differ.\n     model_cost_mapping = {\n         \"gpt-4\": 0.03,\n         \"gpt-4-0314\": 0.03,\n@@ -30,16 +32,41 @@ def get_openai_model_cost_per_1k_tokens(\n         \"code-davinci-002\": 0.02,\n     }\n \n-    cost = model_cost_mapping.get(\n-        model_name.lower()\n-        + (\"-completion\" if is_completion and model_name.startswith(\"gpt-4\") else \"\"),\n-        None,\n-    )\n+    def _normalize_model_name(name: str) -> str:\n+        n = name.lower()\n+\n+        # Handle fine-tuned naming patterns\n+        # Newer pattern: ft:<base>:<org>:<suffix>\n+        if n.startswith(\"ft:\"):\n+            parts = n.split(\":\")\n+            # parts[1] should be the base model name (e.g., gpt-3.5-turbo-0613)\n+            if len(parts) > 1 and parts[1]:\n+                n = parts[1]\n+        # Older pattern: <base>:ft-<suffix>\n+        elif \":ft-\" in n:\n+            n = n.split(\":\")[0]\n+\n+        # Normalize known families to their base keys in the mapping\n+        if n.startswith(\"gpt-4-32k\"):\n+            n = \"gpt-4-32k\"\n+        elif n.startswith(\"gpt-4\"):\n+            n = \"gpt-4\"\n+        elif n.startswith(\"gpt-3.5-turbo\"):\n+            n = \"gpt-3.5-turbo\"\n+        elif n == \"davinci\":\n+            # Map legacy davinci to text-davinci-003 for pricing purposes\n+            n = \"text-davinci-003\"\n+\n+        return n\n+\n+    normalized = _normalize_model_name(model_name)\n+    key = normalized + (\"-completion\" if is_completion and normalized.startswith(\"gpt-4\") else \"\")\n+    cost = model_cost_mapping.get(key, None)\n+\n+    # If still unknown, return 0.0 cost to avoid breaking the callback on fine-tuned models.\n+    # Token counting will still work; cost just won't be computed.\n     if cost is None:\n-        raise ValueError(\n-            f\"Unknown model: {model_name}. Please provide a valid OpenAI model name.\"\n-            \"Known models are: \" + \", \".join(model_cost_mapping.keys())\n-        )\n+        return 0.0\n \n     return cost\n \n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/langchain/callbacks/openai_info.py b/langchain/callbacks/openai_info.py\nindex 5051e53..73b7191 100644\n--- a/langchain/callbacks/openai_info.py\n+++ b/langchain/callbacks/openai_info.py\n@@ -1,3 +1,4 @@\n+\n \"\"\"Callback Handler that prints to std out.\"\"\"\n from typing import Any, Dict, List, Optional, Union\n \n@@ -30,11 +31,36 @@ def get_openai_model_cost_per_1k_tokens(\n         \"code-davinci-002\": 0.02,\n     }\n \n-    cost = model_cost_mapping.get(\n-        model_name.lower()\n-        + (\"-completion\" if is_completion and model_name.startswith(\"gpt-4\") else \"\"),\n-        None,\n-    )\n+    # Normalize model names to handle fine-tuned and versioned variants\n+    def _normalize_model_for_cost(name: str) -> str:\n+        n = name.lower()\n+        # New FT format: ft:gpt-3.5-turbo-1106:org:...\n+        if n.startswith(\"ft:\"):\n+            parts = n.split(\":\")\n+            if len(parts) > 1:\n+                n = parts[1]\n+        # Legacy FT format: ada:ft-..., curie:ft-..., davinci:ft-...\n+        elif \":\" in n:\n+            n = n.split(\":\")[0]\n+\n+        # Collapse versioned names to base keys we have pricing for\n+        if n.startswith(\"gpt-4-32k\"):\n+            n = \"gpt-4-32k\"\n+        elif n.startswith(\"gpt-4\"):\n+            n = \"gpt-4\"\n+        elif n.startswith(\"gpt-3.5-turbo\"):\n+            n = \"gpt-3.5-turbo\"\n+        elif n == \"davinci\":\n+            # Use davinci equivalent pricing (same as text-davinci-003/002)\n+            n = \"text-davinci-003\"\n+\n+        return n\n+\n+    normalized = _normalize_model_for_cost(model_name)\n+\n+    key = normalized + (\"-completion\" if is_completion and normalized.startswith(\"gpt-4\") else \"\")\n+\n+    cost = model_cost_mapping.get(key, None)\n     if cost is None:\n         raise ValueError(\n             f\"Unknown model: {model_name}. Please provide a valid OpenAI model name.\"\ndiff --git a/langchain/llms/openai.py b/langchain/llms/openai.py\nindex 1f9d305..b0f090c 100644\n--- a/langchain/llms/openai.py\n+++ b/langchain/llms/openai.py\n@@ -507,7 +507,38 @@ class BaseOpenAI(BaseLLM):\n             \"code-cushman-001\": 2048,\n         }\n \n-        context_size = model_token_mapping.get(modelname, None)\n+        # Normalize model names to handle fine-tuned and versioned variants\n+        def _normalize_model_for_context(name: str) -> str:\n+            n = name\n+            # New FT format: ft:gpt-3.5-turbo-1106:org:...\n+            if n.lower().startswith(\"ft:\"):\n+                parts = n.split(\":\")\n+                if len(parts) > 1:\n+                    n = parts[1]\n+            # Legacy FT format: ada:ft-..., curie:ft-..., davinci:ft-...\n+            elif \":\" in n:\n+                n = n.split(\":\")[0]\n+\n+            # Collapse versioned names to base keys we have context sizes for\n+            nl = n.lower()\n+            if nl.startswith(\"gpt-4-32k\"):\n+                n = \"gpt-4-32k\"\n+            elif nl.startswith(\"gpt-4\"):\n+                n = \"gpt-4\"\n+            elif nl.startswith(\"gpt-3.5-turbo\"):\n+                n = \"gpt-3.5-turbo\"\n+            elif nl == \"davinci\":\n+                n = \"davinci\"\n+            elif nl == \"babbage\":\n+                n = \"babbage\"\n+            elif nl == \"curie\":\n+                n = \"curie\"\n+            elif nl == \"ada\":\n+                n = \"ada\"\n+            return n\n+\n+        normalized = _normalize_model_for_context(modelname)\n+        context_size = model_token_mapping.get(normalized, None)\n \n         if context_size is None:\n             raise ValueError(\n"
}