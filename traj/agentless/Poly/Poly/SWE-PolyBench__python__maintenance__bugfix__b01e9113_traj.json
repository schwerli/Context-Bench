{
    "instance_id": "SWE-PolyBench__python__maintenance__bugfix__b01e9113",
    "original_id": "huggingface__transformers-29449",
    "1_model_selected_files": [
        "src/transformers/models/idefics/processing_idefics.py",
        "src/transformers/models/idefics/modeling_idefics.py",
        "src/transformers/models/idefics/configuration_idefics.py",
        "src/transformers/models/idefics/vision.py",
        "src/transformers/tokenization_utils_base.py"
    ],
    "2_embedding_selected_files": [],
    "3_final_combined_files": [
        "src/transformers/models/idefics/processing_idefics.py",
        "src/transformers/models/idefics/modeling_idefics.py",
        "src/transformers/models/idefics/configuration_idefics.py"
    ],
    "4_related_elements": {
        "src/transformers/models/idefics/processing_idefics.py": [
            "variable: IMAGE_TOKEN\nfunction: incremental_to_binary_attention_mask\nfunction: image_attention_mask_for_packed_input_ids\nfunction: IdeficsProcessor.__call__\nfunction: IdeficsProcessor.model_input_names"
        ],
        "src/transformers/models/idefics/modeling_idefics.py": [
            "function: expand_inputs_for_generation\nfunction: prepare_inputs_for_generation\nclass: IdeficsAttention\nclass: IdeficsDecoderLayer\nclass: IdeficsGatedCrossAttentionLayer\nclass: IdeficsModel\nclass: IdeficsForVisionText2Text"
        ],
        "src/transformers/models/idefics/configuration_idefics.py": [
            "class: IdeficsConfig"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "src/transformers/models/idefics/processing_idefics.py": [
                    "function: IdeficsProcessor.__call__\nline: 335\nline: 347\nline: 349\nline: 351\nline: 352\nline: 354"
                ],
                "src/transformers/models/idefics/modeling_idefics.py": [
                    ""
                ],
                "src/transformers/models/idefics/configuration_idefics.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/idefics/processing_idefics.py b/src/transformers/models/idefics/processing_idefics.py\nindex 34d9fb1..8b2bfef 100644\n--- a/src/transformers/models/idefics/processing_idefics.py\n+++ b/src/transformers/models/idefics/processing_idefics.py\n@@ -334,6 +334,9 @@ class IdeficsProcessor(ProcessorMixin):\n         )\n         all_texts = text_encoding[\"input_ids\"]\n \n+        # If tokenizer already padded the batch (e.g., padding='longest' or 'max_length'),\n+        # all sequences will have the same length and we should use its attention_mask.\n+        is_already_padded = len(set(len(x) for x in all_texts)) == 1\n         max_seq_len = max(len(x) for x in all_texts)\n \n         # max_num_images has to be at least 1 even when there are no images\n@@ -344,14 +347,26 @@ class IdeficsProcessor(ProcessorMixin):\n         output_input_ids = []\n         output_images = []\n         output_attention_masks = []\n-        for text, images in zip(all_texts, all_images):\n-            padded_input_ids = [self.tokenizer.pad_token_id] * max_seq_len\n-            unpadded_seq_len = len(text)\n-            start = max_seq_len - unpadded_seq_len\n-            padded_input_ids[start:] = text[:max_seq_len]\n+        for idx, (text, images) in enumerate(zip(all_texts, all_images)):\n+            if is_already_padded:\n+                # Use tokenizer's padding and attention mask directly to avoid incorrect masks\n+                padded_input_ids = text[:max_seq_len]\n+                # tokenizer provides attention_mask matching its padding side\n+                tokenizer_attention_mask = text_encoding.get(\"attention_mask\", None)\n+                if tokenizer_attention_mask is not None:\n+                    attention_mask = torch.tensor(tokenizer_attention_mask[idx], dtype=torch.long)\n+                else:\n+                    # Fallback: if no attention_mask is provided, assume all tokens are valid\n+                    attention_mask = torch.ones((len(padded_input_ids),), dtype=torch.long)\n+            else:\n+                # Left-pad to the longest sequence and build the corresponding attention mask\n+                padded_input_ids = [self.tokenizer.pad_token_id] * max_seq_len\n+                unpadded_seq_len = len(text)\n+                start = max_seq_len - unpadded_seq_len\n+                padded_input_ids[start:] = text[:max_seq_len]\n \n-            attention_mask = torch.zeros((max_seq_len,), dtype=torch.long)\n-            attention_mask[start:] = 1\n+                attention_mask = torch.zeros((max_seq_len,), dtype=torch.long)\n+                attention_mask[start:] = 1\n \n             image_count = padded_input_ids.count(self.image_token_id)\n             local_max_num_images = min(image_count, max_num_images)\n@@ -366,7 +381,6 @@ class IdeficsProcessor(ProcessorMixin):\n \n             output_images.append(padded_image_tensor)\n             output_input_ids.append(torch.tensor(padded_input_ids))\n-\n             output_attention_masks.append(attention_mask)\n \n         output_input_ids = torch.stack(output_input_ids)\n",
                "diff --git a/src/transformers/models/idefics/processing_idefics.py b/src/transformers/models/idefics/processing_idefics.py\nindex 34d9fb1..9a4f491 100644\n--- a/src/transformers/models/idefics/processing_idefics.py\n+++ b/src/transformers/models/idefics/processing_idefics.py\n@@ -333,6 +333,7 @@ class IdeficsProcessor(ProcessorMixin):\n             max_length=max_length,\n         )\n         all_texts = text_encoding[\"input_ids\"]\n+        all_attention_masks = text_encoding.get(\"attention_mask\", None)\n \n         max_seq_len = max(len(x) for x in all_texts)\n \n@@ -344,14 +345,25 @@ class IdeficsProcessor(ProcessorMixin):\n         output_input_ids = []\n         output_images = []\n         output_attention_masks = []\n-        for text, images in zip(all_texts, all_images):\n+        for idx, (text, images) in enumerate(zip(all_texts, all_images)):\n             padded_input_ids = [self.tokenizer.pad_token_id] * max_seq_len\n-            unpadded_seq_len = len(text)\n-            start = max_seq_len - unpadded_seq_len\n+            seq_len = len(text)\n+            start = max_seq_len - seq_len\n             padded_input_ids[start:] = text[:max_seq_len]\n \n-            attention_mask = torch.zeros((max_seq_len,), dtype=torch.long)\n-            attention_mask[start:] = 1\n+            # Build attention mask:\n+            # - If tokenizer provided one (e.g. when padding='longest' or 'max_length'), use it.\n+            # - Otherwise, compute it assuming left padding as done above.\n+            if all_attention_masks is not None:\n+                raw_mask = all_attention_masks[idx]\n+                if len(raw_mask) >= max_seq_len:\n+                    attention_mask = torch.tensor(raw_mask[:max_seq_len], dtype=torch.long)\n+                else:\n+                    attention_mask = torch.zeros((max_seq_len,), dtype=torch.long)\n+                    attention_mask[start:] = torch.tensor(raw_mask, dtype=torch.long)\n+            else:\n+                attention_mask = torch.zeros((max_seq_len,), dtype=torch.long)\n+                attention_mask[start:] = 1\n \n             image_count = padded_input_ids.count(self.image_token_id)\n             local_max_num_images = min(image_count, max_num_images)\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "src/transformers/models/idefics/processing_idefics.py": [
                    "function: IdeficsProcessor.__call__\nline: 331\nline: 347\nline: 353\nline: 354"
                ],
                "src/transformers/models/idefics/modeling_idefics.py": [
                    ""
                ],
                "src/transformers/models/idefics/configuration_idefics.py": [
                    ""
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/idefics/processing_idefics.py b/src/transformers/models/idefics/processing_idefics.py\nindex 34d9fb1..4559ff4 100644\n--- a/src/transformers/models/idefics/processing_idefics.py\n+++ b/src/transformers/models/idefics/processing_idefics.py\n@@ -333,7 +333,9 @@ class IdeficsProcessor(ProcessorMixin):\n             max_length=max_length,\n         )\n         all_texts = text_encoding[\"input_ids\"]\n+        all_attn_masks = text_encoding.get(\"attention_mask\", None)\n \n+        # Determine the target max sequence length from the tokenized outputs\n         max_seq_len = max(len(x) for x in all_texts)\n \n         # max_num_images has to be at least 1 even when there are no images\n@@ -344,14 +346,27 @@ class IdeficsProcessor(ProcessorMixin):\n         output_input_ids = []\n         output_images = []\n         output_attention_masks = []\n-        for text, images in zip(all_texts, all_images):\n-            padded_input_ids = [self.tokenizer.pad_token_id] * max_seq_len\n-            unpadded_seq_len = len(text)\n-            start = max_seq_len - unpadded_seq_len\n-            padded_input_ids[start:] = text[:max_seq_len]\n-\n-            attention_mask = torch.zeros((max_seq_len,), dtype=torch.long)\n-            attention_mask[start:] = 1\n+        for idx, (text, images) in enumerate(zip(all_texts, all_images)):\n+            # If tokenizer provided an attention_mask, use it to correctly identify non-pad tokens\n+            if all_attn_masks is not None:\n+                attn_mask_list = all_attn_masks[idx]\n+                # Extract only the real tokens (mask==1), irrespective of tokenizer padding side\n+                tokens_no_pad = [tok for tok, m in zip(text, attn_mask_list) if m == 1]\n+                unpadded_seq_len = len(tokens_no_pad)\n+                # Left-pad to max_seq_len to keep consistent packing as expected by the model\n+                start = max_seq_len - unpadded_seq_len\n+                padded_input_ids = [self.tokenizer.pad_token_id] * max_seq_len\n+                padded_input_ids[start : start + unpadded_seq_len] = tokens_no_pad\n+                attention_mask = torch.zeros((max_seq_len,), dtype=torch.long)\n+                attention_mask[start : start + unpadded_seq_len] = 1\n+            else:\n+                # Fallback: no attention_mask from tokenizer, assume sequence is unpadded\n+                unpadded_seq_len = len(text)\n+                start = max_seq_len - unpadded_seq_len\n+                padded_input_ids = [self.tokenizer.pad_token_id] * max_seq_len\n+                padded_input_ids[start:] = text[:max_seq_len]\n+                attention_mask = torch.zeros((max_seq_len,), dtype=torch.long)\n+                attention_mask[start:] = 1\n \n             image_count = padded_input_ids.count(self.image_token_id)\n             local_max_num_images = min(image_count, max_num_images)\n@@ -366,7 +381,6 @@ class IdeficsProcessor(ProcessorMixin):\n \n             output_images.append(padded_image_tensor)\n             output_input_ids.append(torch.tensor(padded_input_ids))\n-\n             output_attention_masks.append(attention_mask)\n \n         output_input_ids = torch.stack(output_input_ids)\n",
                "diff --git a/src/transformers/models/idefics/processing_idefics.py b/src/transformers/models/idefics/processing_idefics.py\nindex 34d9fb1..a2efbcb 100644\n--- a/src/transformers/models/idefics/processing_idefics.py\n+++ b/src/transformers/models/idefics/processing_idefics.py\n@@ -334,44 +334,79 @@ class IdeficsProcessor(ProcessorMixin):\n         )\n         all_texts = text_encoding[\"input_ids\"]\n \n-        max_seq_len = max(len(x) for x in all_texts)\n-\n         # max_num_images has to be at least 1 even when there are no images\n         max_num_images = max(len(x) for x in all_images)\n         max_num_images = max(1, max_num_images)\n \n         at_least_one_image = sum(len(x) for x in all_images) > 0\n-        output_input_ids = []\n+\n+        # Respect tokenizer padding when provided (e.g., padding='longest' or 'max_length'),\n+        # otherwise pad manually following tokenizer.padding_side.\n+        has_tokenizer_attn = \"attention_mask\" in text_encoding\n+        pad_to_longest = (padding is True) or (isinstance(padding, str) and padding == \"longest\")\n+        pad_to_max_length = isinstance(padding, str) and padding == \"max_length\"\n+        use_tokenizer_padding = has_tokenizer_attn and (pad_to_longest or pad_to_max_length)\n+\n         output_images = []\n-        output_attention_masks = []\n-        for text, images in zip(all_texts, all_images):\n-            padded_input_ids = [self.tokenizer.pad_token_id] * max_seq_len\n-            unpadded_seq_len = len(text)\n-            start = max_seq_len - unpadded_seq_len\n-            padded_input_ids[start:] = text[:max_seq_len]\n+        if use_tokenizer_padding:\n+            # Use tokenizer-produced input_ids and attention_mask directly\n+            for text, images in zip(all_texts, all_images):\n+                image_count = text.count(self.image_token_id)\n+                local_max_num_images = min(image_count, max_num_images)\n \n-            attention_mask = torch.zeros((max_seq_len,), dtype=torch.long)\n-            attention_mask[start:] = 1\n+                current_images = images[:local_max_num_images]\n \n-            image_count = padded_input_ids.count(self.image_token_id)\n-            local_max_num_images = min(image_count, max_num_images)\n+                if len(current_images) > 0:\n+                    padded_image_tensor = torch.zeros(max_num_images, *current_images.size()[1:])\n+                    padded_image_tensor[: current_images.size(0)] = current_images\n+                else:\n+                    padded_image_tensor = torch.zeros(max_num_images, *self.default_image_dims)\n \n-            current_images = images[:local_max_num_images]\n+                output_images.append(padded_image_tensor)\n \n-            if len(current_images) > 0:\n-                padded_image_tensor = torch.zeros(max_num_images, *current_images.size()[1:])\n-                padded_image_tensor[: current_images.size(0)] = current_images\n-            else:\n-                padded_image_tensor = torch.zeros(max_num_images, *self.default_image_dims)\n+            output_input_ids = torch.tensor(all_texts, dtype=torch.long)\n+            output_images = torch.stack(output_images)\n+            output_attention_masks = torch.tensor(text_encoding[\"attention_mask\"], dtype=torch.long)\n+        else:\n+            # Manually pad to the longest in batch, respecting tokenizer.padding_side\n+            max_seq_len = max(len(x) for x in all_texts)\n+            pad_on_left = getattr(self.tokenizer, \"padding_side\", \"right\") == \"left\"\n+\n+            output_input_ids = []\n+            output_attention_masks = []\n+\n+            for text, images in zip(all_texts, all_images):\n+                padded_input_ids = [self.tokenizer.pad_token_id] * max_seq_len\n+                unpadded_seq_len = len(text)\n+\n+                attention_mask = torch.zeros((max_seq_len,), dtype=torch.long)\n+                if pad_on_left:\n+                    start = max_seq_len - unpadded_seq_len\n+                    padded_input_ids[start:] = text[:max_seq_len]\n+                    attention_mask[start:] = 1\n+                else:\n+                    end = min(unpadded_seq_len, max_seq_len)\n+                    padded_input_ids[:end] = text[:end]\n+                    attention_mask[:end] = 1\n+\n+                image_count = padded_input_ids.count(self.image_token_id)\n+                local_max_num_images = min(image_count, max_num_images)\n \n-            output_images.append(padded_image_tensor)\n-            output_input_ids.append(torch.tensor(padded_input_ids))\n+                current_images = images[:local_max_num_images]\n+\n+                if len(current_images) > 0:\n+                    padded_image_tensor = torch.zeros(max_num_images, *current_images.size()[1:])\n+                    padded_image_tensor[: current_images.size(0)] = current_images\n+                else:\n+                    padded_image_tensor = torch.zeros(max_num_images, *self.default_image_dims)\n \n-            output_attention_masks.append(attention_mask)\n+                output_images.append(padded_image_tensor)\n+                output_input_ids.append(torch.tensor(padded_input_ids))\n+                output_attention_masks.append(attention_mask)\n \n-        output_input_ids = torch.stack(output_input_ids)\n-        output_images = torch.stack(output_images)\n-        output_attention_masks = torch.stack(output_attention_masks)\n+            output_input_ids = torch.stack(output_input_ids)\n+            output_images = torch.stack(output_images)\n+            output_attention_masks = torch.stack(output_attention_masks)\n \n         if at_least_one_image:\n             image_attention_mask, _ = image_attention_mask_for_packed_input_ids(output_input_ids, self.tokenizer)\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/src/transformers/models/idefics/processing_idefics.py b/src/transformers/models/idefics/processing_idefics.py\nindex 34d9fb1..8b2bfef 100644\n--- a/src/transformers/models/idefics/processing_idefics.py\n+++ b/src/transformers/models/idefics/processing_idefics.py\n@@ -334,6 +334,9 @@ class IdeficsProcessor(ProcessorMixin):\n         )\n         all_texts = text_encoding[\"input_ids\"]\n \n+        # If tokenizer already padded the batch (e.g., padding='longest' or 'max_length'),\n+        # all sequences will have the same length and we should use its attention_mask.\n+        is_already_padded = len(set(len(x) for x in all_texts)) == 1\n         max_seq_len = max(len(x) for x in all_texts)\n \n         # max_num_images has to be at least 1 even when there are no images\n@@ -344,14 +347,26 @@ class IdeficsProcessor(ProcessorMixin):\n         output_input_ids = []\n         output_images = []\n         output_attention_masks = []\n-        for text, images in zip(all_texts, all_images):\n-            padded_input_ids = [self.tokenizer.pad_token_id] * max_seq_len\n-            unpadded_seq_len = len(text)\n-            start = max_seq_len - unpadded_seq_len\n-            padded_input_ids[start:] = text[:max_seq_len]\n+        for idx, (text, images) in enumerate(zip(all_texts, all_images)):\n+            if is_already_padded:\n+                # Use tokenizer's padding and attention mask directly to avoid incorrect masks\n+                padded_input_ids = text[:max_seq_len]\n+                # tokenizer provides attention_mask matching its padding side\n+                tokenizer_attention_mask = text_encoding.get(\"attention_mask\", None)\n+                if tokenizer_attention_mask is not None:\n+                    attention_mask = torch.tensor(tokenizer_attention_mask[idx], dtype=torch.long)\n+                else:\n+                    # Fallback: if no attention_mask is provided, assume all tokens are valid\n+                    attention_mask = torch.ones((len(padded_input_ids),), dtype=torch.long)\n+            else:\n+                # Left-pad to the longest sequence and build the corresponding attention mask\n+                padded_input_ids = [self.tokenizer.pad_token_id] * max_seq_len\n+                unpadded_seq_len = len(text)\n+                start = max_seq_len - unpadded_seq_len\n+                padded_input_ids[start:] = text[:max_seq_len]\n \n-            attention_mask = torch.zeros((max_seq_len,), dtype=torch.long)\n-            attention_mask[start:] = 1\n+                attention_mask = torch.zeros((max_seq_len,), dtype=torch.long)\n+                attention_mask[start:] = 1\n \n             image_count = padded_input_ids.count(self.image_token_id)\n             local_max_num_images = min(image_count, max_num_images)\n@@ -366,7 +381,6 @@ class IdeficsProcessor(ProcessorMixin):\n \n             output_images.append(padded_image_tensor)\n             output_input_ids.append(torch.tensor(padded_input_ids))\n-\n             output_attention_masks.append(attention_mask)\n \n         output_input_ids = torch.stack(output_input_ids)\n"
}