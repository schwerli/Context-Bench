{
    "instance_id": "SWE-PolyBench__python__maintenance__bugfix__2a9201a9",
    "original_id": "huggingface__transformers-30556",
    "1_model_selected_files": [
        "src/transformers/data/data_collator.py"
    ],
    "2_embedding_selected_files": [],
    "3_final_combined_files": [
        "src/transformers/data/data_collator.py"
    ],
    "4_related_elements": {
        "src/transformers/data/data_collator.py": [
            "function: DataCollatorForSeq2Seq.__call__\nclass: DataCollatorWithPadding\nfunction: pad_without_fast_tokenizer_warning\nclass: DataCollatorForTokenClassification"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "src/transformers/data/data_collator.py": [
                    "class: DataCollatorForSeq2Seq\nfunction: DataCollatorForSeq2Seq.__call__\nline: 592\nline: 593\nline: 594\nline: 595\nline: 596\nline: 597\nline: 598\nline: 600\nline: 601\nline: 602\nline: 603\nline: 604\nline: 605\nline: 606\nline: 607\nline: 608\nline: 609\nline: 610\nline: 611"
                ]
            },
            "patches": [
                "diff --git a/src/transformers/data/data_collator.py b/src/transformers/data/data_collator.py\nindex e17e89d..3052fe4 100644\n--- a/src/transformers/data/data_collator.py\n+++ b/src/transformers/data/data_collator.py\n@@ -589,17 +589,42 @@ class DataCollatorForSeq2Seq:\n         # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n         # same length to return tensors.\n         if labels is not None:\n-            max_label_length = max(len(l) for l in labels)\n+            # Determine desired label length based on the padding strategy.\n+            # By default pad to the longest in the batch; if using MAX_LENGTH, pad to `self.max_length`\n+            # (or tokenizer.model_max_length if available and `self.max_length` is None).\n+            max_label_length_in_batch = max(len(l) for l in labels)\n+\n+            use_max_length = (\n+                self.padding == PaddingStrategy.MAX_LENGTH\n+                or (isinstance(self.padding, str) and self.padding == \"max_length\")\n+            )\n+\n+            if use_max_length:\n+                # Prefer the provided `max_length`; fall back to tokenizer.model_max_length if available.\n+                target_label_length = (\n+                    self.max_length\n+                    if self.max_length is not None\n+                    else getattr(self.tokenizer, \"model_max_length\", max_label_length_in_batch)\n+                )\n+                # In case model_max_length is not set or invalid, fall back to batch max.\n+                if not isinstance(target_label_length, int) or target_label_length <= 0:\n+                    target_label_length = max_label_length_in_batch\n+            else:\n+                target_label_length = max_label_length_in_batch\n+\n+            # Optionally round up to a multiple\n             if self.pad_to_multiple_of is not None:\n-                max_label_length = (\n-                    (max_label_length + self.pad_to_multiple_of - 1)\n+                target_label_length = (\n+                    (target_label_length + self.pad_to_multiple_of - 1)\n                     // self.pad_to_multiple_of\n                     * self.pad_to_multiple_of\n                 )\n \n             padding_side = self.tokenizer.padding_side\n             for feature in features:\n-                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n+                # Do not truncate labels here; only pad up to the target length if needed.\n+                remainder_length = max(0, target_label_length - len(feature[\"labels\"]))\n+                remainder = [self.label_pad_token_id] * remainder_length\n                 if isinstance(feature[\"labels\"], list):\n                     feature[\"labels\"] = (\n                         feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n",
                "diff --git a/src/transformers/data/data_collator.py b/src/transformers/data/data_collator.py\nindex e17e89d..180f1f2 100644\n--- a/src/transformers/data/data_collator.py\n+++ b/src/transformers/data/data_collator.py\n@@ -589,17 +589,36 @@ class DataCollatorForSeq2Seq:\n         # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n         # same length to return tensors.\n         if labels is not None:\n-            max_label_length = max(len(l) for l in labels)\n-            if self.pad_to_multiple_of is not None:\n-                max_label_length = (\n-                    (max_label_length + self.pad_to_multiple_of - 1)\n-                    // self.pad_to_multiple_of\n-                    * self.pad_to_multiple_of\n-                )\n+            # Compute target label padding length according to the padding strategy.\n+            padding_strategy = self.padding\n+            if isinstance(padding_strategy, str):\n+                padding_strategy_str = padding_strategy\n+            else:\n+                try:\n+                    padding_strategy_str = padding_strategy.value\n+                except AttributeError:\n+                    # Map boolean padding to the expected strategy string.\n+                    padding_strategy_str = \"longest\" if padding_strategy else \"do_not_pad\"\n+\n+            if padding_strategy_str == \"max_length\" and self.max_length is not None:\n+                # When padding to max_length, pad labels to the same max_length to keep shapes aligned.\n+                max_label_length = self.max_length\n+            else:\n+                # Default: pad labels to the longest label in the batch (optionally rounded to a multiple-of).\n+                max_label_length = max(len(l) for l in labels)\n+                if self.pad_to_multiple_of is not None:\n+                    max_label_length = (\n+                        (max_label_length + self.pad_to_multiple_of - 1)\n+                        // self.pad_to_multiple_of\n+                        * self.pad_to_multiple_of\n+                    )\n \n             padding_side = self.tokenizer.padding_side\n             for feature in features:\n-                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n+                remainder_len = max_label_length - len(feature[\"labels\"])\n+                if remainder_len <= 0:\n+                    continue\n+                remainder = [self.label_pad_token_id] * remainder_len\n                 if isinstance(feature[\"labels\"], list):\n                     feature[\"labels\"] = (\n                         feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "src/transformers/data/data_collator.py": [
                    "function: DataCollatorForSeq2Seq.__call__\nline: 592\nline: 593\nline: 594\nline: 595\nline: 596\nline: 597\nline: 598"
                ]
            },
            "patches": [
                "diff --git a/src/transformers/data/data_collator.py b/src/transformers/data/data_collator.py\nindex e17e89d..04c30f9 100644\n--- a/src/transformers/data/data_collator.py\n+++ b/src/transformers/data/data_collator.py\n@@ -589,17 +589,33 @@ class DataCollatorForSeq2Seq:\n         # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n         # same length to return tensors.\n         if labels is not None:\n-            max_label_length = max(len(l) for l in labels)\n+            # Determine the target length for labels so that it matches the chosen padding strategy.\n+            # - For 'max_length', pad labels up to `self.max_length` if provided, but never shorter than the longest label in the batch.\n+            # - Otherwise, pad labels to the longest label length in the batch.\n+            max_label_length_in_batch = max(len(l) for l in labels)\n+\n+            pad_to_max_length = (\n+                self.padding == PaddingStrategy.MAX_LENGTH\n+                or (isinstance(self.padding, str) and self.padding == \"max_length\")\n+            )\n+\n+            if pad_to_max_length and self.max_length is not None:\n+                target_label_length = max(self.max_length, max_label_length_in_batch)\n+            else:\n+                target_label_length = max_label_length_in_batch\n+\n+            # If requested, pad to a multiple of `pad_to_multiple_of`.\n             if self.pad_to_multiple_of is not None:\n-                max_label_length = (\n-                    (max_label_length + self.pad_to_multiple_of - 1)\n+                target_label_length = (\n+                    (target_label_length + self.pad_to_multiple_of - 1)\n                     // self.pad_to_multiple_of\n                     * self.pad_to_multiple_of\n                 )\n \n             padding_side = self.tokenizer.padding_side\n             for feature in features:\n-                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n+                remainder_len = max(0, target_label_length - len(feature[\"labels\"]))\n+                remainder = [self.label_pad_token_id] * remainder_len\n                 if isinstance(feature[\"labels\"], list):\n                     feature[\"labels\"] = (\n                         feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n",
                "diff --git a/src/transformers/data/data_collator.py b/src/transformers/data/data_collator.py\nindex e17e89d..349b1b5 100644\n--- a/src/transformers/data/data_collator.py\n+++ b/src/transformers/data/data_collator.py\n@@ -589,17 +589,30 @@ class DataCollatorForSeq2Seq:\n         # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n         # same length to return tensors.\n         if labels is not None:\n-            max_label_length = max(len(l) for l in labels)\n-            if self.pad_to_multiple_of is not None:\n-                max_label_length = (\n-                    (max_label_length + self.pad_to_multiple_of - 1)\n-                    // self.pad_to_multiple_of\n-                    * self.pad_to_multiple_of\n-                )\n+            # Compute the target label length:\n+            # - If padding strategy is MAX_LENGTH and a max_length is provided, pad labels to that max_length\n+            #   (but never below the longest label in the batch).\n+            # - Otherwise, pad to the longest label length in the batch, optionally rounded to pad_to_multiple_of.\n+            max_label_length_in_batch = max(len(l) for l in labels)\n+            use_max_length = (\n+                (isinstance(self.padding, str) and self.padding == \"max_length\")\n+                or (not isinstance(self.padding, bool) and self.padding == PaddingStrategy.MAX_LENGTH)\n+            )\n+            if use_max_length and self.max_length is not None:\n+                target_label_length = max(self.max_length, max_label_length_in_batch)\n+            else:\n+                target_label_length = max_label_length_in_batch\n+                if self.pad_to_multiple_of is not None:\n+                    target_label_length = (\n+                        (target_label_length + self.pad_to_multiple_of - 1)\n+                        // self.pad_to_multiple_of\n+                        * self.pad_to_multiple_of\n+                    )\n \n             padding_side = self.tokenizer.padding_side\n             for feature in features:\n-                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n+                pad_len = max(0, target_label_length - len(feature[\"labels\"]))\n+                remainder = [self.label_pad_token_id] * pad_len\n                 if isinstance(feature[\"labels\"], list):\n                     feature[\"labels\"] = (\n                         feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/src/transformers/data/data_collator.py b/src/transformers/data/data_collator.py\nindex e17e89d..3052fe4 100644\n--- a/src/transformers/data/data_collator.py\n+++ b/src/transformers/data/data_collator.py\n@@ -589,17 +589,42 @@ class DataCollatorForSeq2Seq:\n         # We have to pad the labels before calling `tokenizer.pad` as this method won't pad them and needs them of the\n         # same length to return tensors.\n         if labels is not None:\n-            max_label_length = max(len(l) for l in labels)\n+            # Determine desired label length based on the padding strategy.\n+            # By default pad to the longest in the batch; if using MAX_LENGTH, pad to `self.max_length`\n+            # (or tokenizer.model_max_length if available and `self.max_length` is None).\n+            max_label_length_in_batch = max(len(l) for l in labels)\n+\n+            use_max_length = (\n+                self.padding == PaddingStrategy.MAX_LENGTH\n+                or (isinstance(self.padding, str) and self.padding == \"max_length\")\n+            )\n+\n+            if use_max_length:\n+                # Prefer the provided `max_length`; fall back to tokenizer.model_max_length if available.\n+                target_label_length = (\n+                    self.max_length\n+                    if self.max_length is not None\n+                    else getattr(self.tokenizer, \"model_max_length\", max_label_length_in_batch)\n+                )\n+                # In case model_max_length is not set or invalid, fall back to batch max.\n+                if not isinstance(target_label_length, int) or target_label_length <= 0:\n+                    target_label_length = max_label_length_in_batch\n+            else:\n+                target_label_length = max_label_length_in_batch\n+\n+            # Optionally round up to a multiple\n             if self.pad_to_multiple_of is not None:\n-                max_label_length = (\n-                    (max_label_length + self.pad_to_multiple_of - 1)\n+                target_label_length = (\n+                    (target_label_length + self.pad_to_multiple_of - 1)\n                     // self.pad_to_multiple_of\n                     * self.pad_to_multiple_of\n                 )\n \n             padding_side = self.tokenizer.padding_side\n             for feature in features:\n-                remainder = [self.label_pad_token_id] * (max_label_length - len(feature[\"labels\"]))\n+                # Do not truncate labels here; only pad up to the target length if needed.\n+                remainder_length = max(0, target_label_length - len(feature[\"labels\"]))\n+                remainder = [self.label_pad_token_id] * remainder_length\n                 if isinstance(feature[\"labels\"], list):\n                     feature[\"labels\"] = (\n                         feature[\"labels\"] + remainder if padding_side == \"right\" else remainder + feature[\"labels\"]\n"
}