{
    "instance_id": "SWE-PolyBench__python__evolution__feature__674464fb",
    "original_id": "huggingface__transformers-28517",
    "1_model_selected_files": [
        "src/transformers/models/mixtral/modeling_mixtral.py"
    ],
    "2_embedding_selected_files": [],
    "3_final_combined_files": [
        "src/transformers/models/mixtral/modeling_mixtral.py"
    ],
    "4_related_elements": {
        "src/transformers/models/mixtral/modeling_mixtral.py": [
            "function: load_balancing_loss_func\nclass: MixtralSparseMoeBlock\nclass: MixtralBLockSparseTop2MLP\nclass: MixtralDecoderLayer\nfunction: MixtralModel.forward\nfunction: MixtralForCausalLM.forward\nfunction: MixtralForSequenceClassification.forward"
        ]
    },
    "5_sampled_edit_locs_and_patches": [
        {
            "sample_index": 0,
            "edit_locs": {
                "src/transformers/models/mixtral/modeling_mixtral.py": [
                    "function: load_balancing_loss_func\nline: 77\nfunction: MixtralForCausalLM.forward\nline: 1349\nline: 1350\nline: 1351"
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/mixtral/modeling_mixtral.py b/src/transformers/models/mixtral/modeling_mixtral.py\nindex 8cffcb1..457b605 100644\n--- a/src/transformers/models/mixtral/modeling_mixtral.py\n+++ b/src/transformers/models/mixtral/modeling_mixtral.py\n@@ -74,7 +74,12 @@ logger = logging.get_logger(__name__)\n _CONFIG_FOR_DOC = \"MixtralConfig\"\n \n \n-def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tensor = None, top_k=2) -> float:\n+def load_balancing_loss_func(\n+    gate_logits: torch.Tensor,\n+    num_experts: torch.Tensor = None,\n+    top_k: int = 2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> float:\n     r\"\"\"\n     Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n@@ -88,6 +93,11 @@ def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tenso\n             shape [batch_size X sequence_length, num_experts].\n         num_experts (`int`, *optional*):\n             Number of experts\n+        top_k (`int`, *optional*, defaults to 2):\n+            Number of experts selected per token.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            Attention mask of shape [batch_size, sequence_length] where 1 indicates valid tokens and 0 indicates padding.\n+            If provided, padding tokens are excluded from the auxiliary load balancing loss.\n \n     Returns:\n         The auxiliary loss.\n@@ -95,21 +105,35 @@ def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tenso\n     if gate_logits is None or not isinstance(gate_logits, tuple):\n         return 0\n \n-    if isinstance(gate_logits, tuple):\n-        compute_device = gate_logits[0].device\n-        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+    # Concatenate router logits from all layers along the token dimension\n+    compute_device = gate_logits[0].device\n+    concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n \n     routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n-\n     _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n-\n     expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n \n-    # Compute the percentage of tokens routed to each experts\n-    tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+    if attention_mask is not None:\n+        # Build a mask over the concatenated tokens to exclude padding tokens across all layers\n+        token_mask = attention_mask.to(device=compute_device)\n+        # Flatten to [batch_size * sequence_length]\n+        token_mask = token_mask.reshape(-1)\n+        # Repeat for each layer to match concatenation over layers\n+        token_mask = token_mask.repeat(len(gate_logits))\n+        token_mask = token_mask.to(dtype=routing_weights.dtype)  # float mask with 1 for valid, 0 for pad\n+\n+        denom = token_mask.sum().clamp_min(1e-9)\n \n-    # Compute the average probability of routing to these experts\n-    router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+        # Compute the percentage of tokens routed to each expert, excluding padding tokens\n+        tokens_per_expert = (expert_mask.float() * token_mask[:, None, None]).sum(dim=0) / denom\n+\n+        # Compute the average probability of routing to each expert, excluding padding tokens\n+        router_prob_per_expert = (routing_weights * token_mask[:, None]).sum(dim=0) / denom\n+    else:\n+        # Compute the percentage of tokens routed to each expert\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n \n     overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n     return overall_loss * num_experts\n@@ -1347,7 +1371,10 @@ class MixtralForCausalLM(MixtralPreTrainedModel):\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1], self.num_experts, self.num_experts_per_tok\n+                outputs.router_logits if return_dict else outputs[-1],\n+                self.num_experts,\n+                self.num_experts_per_tok,\n+                attention_mask=attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss\n",
                "diff --git a/src/transformers/models/mixtral/modeling_mixtral.py b/src/transformers/models/mixtral/modeling_mixtral.py\nindex 8cffcb1..18e4ac7 100644\n--- a/src/transformers/models/mixtral/modeling_mixtral.py\n+++ b/src/transformers/models/mixtral/modeling_mixtral.py\n@@ -74,7 +74,9 @@ logger = logging.get_logger(__name__)\n _CONFIG_FOR_DOC = \"MixtralConfig\"\n \n \n-def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tensor = None, top_k=2) -> float:\n+def load_balancing_loss_func(\n+    gate_logits: torch.Tensor, num_experts: int = None, top_k=2, attention_mask: torch.Tensor = None\n+) -> float:\n     r\"\"\"\n     Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n@@ -87,7 +89,12 @@ def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tenso\n             Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n             shape [batch_size X sequence_length, num_experts].\n         num_experts (`int`, *optional*):\n-            Number of experts\n+            Number of experts.\n+        top_k (`int`, *optional*):\n+            Number of experts selected per token.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            Attention mask of shape [batch_size, sequence_length] indicating which tokens are padding (0)\n+            and which are real tokens (1). Padding tokens will be excluded from the auxiliary loss.\n \n     Returns:\n         The auxiliary loss.\n@@ -95,9 +102,21 @@ def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tenso\n     if gate_logits is None or not isinstance(gate_logits, tuple):\n         return 0\n \n-    if isinstance(gate_logits, tuple):\n-        compute_device = gate_logits[0].device\n-        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+    compute_device = gate_logits[0].device\n+    concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    # Exclude padding tokens from contributing to the auxiliary load balancing loss\n+    if attention_mask is not None:\n+        # Flatten [batch_size, seq_len] -> [batch_size * seq_len] and cast to boolean\n+        flat_mask = attention_mask.reshape(-1).to(device=compute_device, dtype=torch.bool)\n+        # Repeat mask for each layer since gate_logits are concatenated across layers\n+        num_layers = len(gate_logits)\n+        flat_mask = flat_mask.repeat(num_layers)\n+        # Apply mask to filter out padding tokens\n+        concatenated_gate_logits = concatenated_gate_logits[flat_mask]\n+        # If all tokens are padding, return 0 loss to avoid invalid operations\n+        if concatenated_gate_logits.shape[0] == 0:\n+            return 0\n \n     routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n \n@@ -105,7 +124,7 @@ def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tenso\n \n     expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n \n-    # Compute the percentage of tokens routed to each experts\n+    # Compute the percentage of tokens routed to each expert (excluding padding tokens if masked)\n     tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n \n     # Compute the average probability of routing to these experts\n@@ -1347,7 +1366,10 @@ class MixtralForCausalLM(MixtralPreTrainedModel):\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1], self.num_experts, self.num_experts_per_tok\n+                outputs.router_logits if return_dict else outputs[-1],\n+                self.num_experts,\n+                self.num_experts_per_tok,\n+                attention_mask=attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss\n"
            ]
        },
        {
            "sample_index": 1,
            "edit_locs": {
                "src/transformers/models/mixtral/modeling_mixtral.py": [
                    "function: load_balancing_loss_func\nline: 1349\nline: 1350\nline: 1351"
                ]
            },
            "patches": [
                "diff --git a/src/transformers/models/mixtral/modeling_mixtral.py b/src/transformers/models/mixtral/modeling_mixtral.py\nindex 8cffcb1..3d23a00 100644\n--- a/src/transformers/models/mixtral/modeling_mixtral.py\n+++ b/src/transformers/models/mixtral/modeling_mixtral.py\n@@ -74,7 +74,12 @@ logger = logging.get_logger(__name__)\n _CONFIG_FOR_DOC = \"MixtralConfig\"\n \n \n-def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tensor = None, top_k=2) -> float:\n+def load_balancing_loss_func(\n+    gate_logits: torch.Tensor,\n+    num_experts: torch.Tensor = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> float:\n     r\"\"\"\n     Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n@@ -105,11 +110,19 @@ def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tenso\n \n     expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n \n-    # Compute the percentage of tokens routed to each experts\n-    tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+    if attention_mask is not None:\n+        # Flatten attention mask to match concatenated logits length\n+        # Repeat for top_k to match expert_mask shape\n+        flat_mask = attention_mask.view(-1)\n+        valid_token_mask = flat_mask.unsqueeze(-1).unsqueeze(-1)  # [total_tokens, 1, 1]\n+        expert_mask = expert_mask * valid_token_mask\n+        routing_weights = routing_weights * flat_mask.unsqueeze(-1)\n+\n+    # Compute the percentage of tokens routed to each experts (ignoring padding if masked)\n+    tokens_per_expert = torch.sum(expert_mask.float(), dim=0) / expert_mask.shape[0]\n \n     # Compute the average probability of routing to these experts\n-    router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    router_prob_per_expert = torch.sum(routing_weights, dim=0) / routing_weights.shape[0]\n \n     overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n     return overall_loss * num_experts\n@@ -1347,7 +1360,10 @@ class MixtralForCausalLM(MixtralPreTrainedModel):\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1], self.num_experts, self.num_experts_per_tok\n+                outputs.router_logits if return_dict else outputs[-1],\n+                self.num_experts,\n+                self.num_experts_per_tok,\n+                attention_mask=attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss\n",
                "diff --git a/src/transformers/models/mixtral/modeling_mixtral.py b/src/transformers/models/mixtral/modeling_mixtral.py\nindex 8cffcb1..23dbd3b 100644\n--- a/src/transformers/models/mixtral/modeling_mixtral.py\n+++ b/src/transformers/models/mixtral/modeling_mixtral.py\n@@ -74,7 +74,7 @@ logger = logging.get_logger(__name__)\n _CONFIG_FOR_DOC = \"MixtralConfig\"\n \n \n-def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tensor = None, top_k=2) -> float:\n+def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tensor = None, top_k=2, attention_mask: torch.Tensor = None) -> float:\n     r\"\"\"\n     Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n@@ -88,6 +88,10 @@ def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tenso\n             shape [batch_size X sequence_length, num_experts].\n         num_experts (`int`, *optional*):\n             Number of experts\n+        top_k (`int`, *optional*):\n+            Number of experts selected per token.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            Attention mask of shape [batch_size, sequence_length] where non-zero values indicate valid (non-padding) tokens.\n \n     Returns:\n         The auxiliary loss.\n@@ -95,9 +99,8 @@ def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tenso\n     if gate_logits is None or not isinstance(gate_logits, tuple):\n         return 0\n \n-    if isinstance(gate_logits, tuple):\n-        compute_device = gate_logits[0].device\n-        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+    compute_device = gate_logits[0].device\n+    concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n \n     routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n \n@@ -105,11 +108,26 @@ def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tenso\n \n     expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n \n-    # Compute the percentage of tokens routed to each experts\n-    tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+    if attention_mask is not None:\n+        # Build mask over concatenated tokens to exclude padding tokens from loss\n+        mask = attention_mask.to(dtype=torch.float32, device=compute_device).view(-1)\n+        # Repeat the mask for each layer since gate_logits are provided per layer\n+        concatenated_mask = torch.cat([mask for _ in range(len(gate_logits))], dim=0)\n+        valid_count = concatenated_mask.sum()\n+        if valid_count.item() == 0:\n+            return 0.0\n \n-    # Compute the average probability of routing to these experts\n-    router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+        # Compute the percentage of non-padding tokens routed to each expert\n+        tokens_per_expert = (expert_mask.float() * concatenated_mask.view(-1, 1, 1)).sum(dim=0) / valid_count\n+\n+        # Compute the average probability of routing to these experts over non-padding tokens\n+        router_prob_per_expert = (routing_weights * concatenated_mask.view(-1, 1)).sum(dim=0) / valid_count\n+    else:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n \n     overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n     return overall_loss * num_experts\n@@ -1347,7 +1365,10 @@ class MixtralForCausalLM(MixtralPreTrainedModel):\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1], self.num_experts, self.num_experts_per_tok\n+                outputs.router_logits if return_dict else outputs[-1],\n+                self.num_experts,\n+                self.num_experts_per_tok,\n+                attention_mask=attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss\n"
            ]
        }
    ],
    "6_final_selected_patch": "diff --git a/src/transformers/models/mixtral/modeling_mixtral.py b/src/transformers/models/mixtral/modeling_mixtral.py\nindex 8cffcb1..457b605 100644\n--- a/src/transformers/models/mixtral/modeling_mixtral.py\n+++ b/src/transformers/models/mixtral/modeling_mixtral.py\n@@ -74,7 +74,12 @@ logger = logging.get_logger(__name__)\n _CONFIG_FOR_DOC = \"MixtralConfig\"\n \n \n-def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tensor = None, top_k=2) -> float:\n+def load_balancing_loss_func(\n+    gate_logits: torch.Tensor,\n+    num_experts: torch.Tensor = None,\n+    top_k: int = 2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> float:\n     r\"\"\"\n     Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n@@ -88,6 +93,11 @@ def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tenso\n             shape [batch_size X sequence_length, num_experts].\n         num_experts (`int`, *optional*):\n             Number of experts\n+        top_k (`int`, *optional*, defaults to 2):\n+            Number of experts selected per token.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            Attention mask of shape [batch_size, sequence_length] where 1 indicates valid tokens and 0 indicates padding.\n+            If provided, padding tokens are excluded from the auxiliary load balancing loss.\n \n     Returns:\n         The auxiliary loss.\n@@ -95,21 +105,35 @@ def load_balancing_loss_func(gate_logits: torch.Tensor, num_experts: torch.Tenso\n     if gate_logits is None or not isinstance(gate_logits, tuple):\n         return 0\n \n-    if isinstance(gate_logits, tuple):\n-        compute_device = gate_logits[0].device\n-        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+    # Concatenate router logits from all layers along the token dimension\n+    compute_device = gate_logits[0].device\n+    concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n \n     routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n-\n     _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n-\n     expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n \n-    # Compute the percentage of tokens routed to each experts\n-    tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+    if attention_mask is not None:\n+        # Build a mask over the concatenated tokens to exclude padding tokens across all layers\n+        token_mask = attention_mask.to(device=compute_device)\n+        # Flatten to [batch_size * sequence_length]\n+        token_mask = token_mask.reshape(-1)\n+        # Repeat for each layer to match concatenation over layers\n+        token_mask = token_mask.repeat(len(gate_logits))\n+        token_mask = token_mask.to(dtype=routing_weights.dtype)  # float mask with 1 for valid, 0 for pad\n+\n+        denom = token_mask.sum().clamp_min(1e-9)\n \n-    # Compute the average probability of routing to these experts\n-    router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+        # Compute the percentage of tokens routed to each expert, excluding padding tokens\n+        tokens_per_expert = (expert_mask.float() * token_mask[:, None, None]).sum(dim=0) / denom\n+\n+        # Compute the average probability of routing to each expert, excluding padding tokens\n+        router_prob_per_expert = (routing_weights * token_mask[:, None]).sum(dim=0) / denom\n+    else:\n+        # Compute the percentage of tokens routed to each expert\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n \n     overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n     return overall_loss * num_experts\n@@ -1347,7 +1371,10 @@ class MixtralForCausalLM(MixtralPreTrainedModel):\n         aux_loss = None\n         if output_router_logits:\n             aux_loss = load_balancing_loss_func(\n-                outputs.router_logits if return_dict else outputs[-1], self.num_experts, self.num_experts_per_tok\n+                outputs.router_logits if return_dict else outputs[-1],\n+                self.num_experts,\n+                self.num_experts_per_tok,\n+                attention_mask=attention_mask,\n             )\n             if labels is not None:\n                 loss += self.router_aux_loss_coef * aux_loss\n"
}