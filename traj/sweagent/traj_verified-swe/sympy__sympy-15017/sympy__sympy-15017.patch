diff --git a/repro_len_scalar.py b/repro_len_scalar.py
new file mode 100644
index 000000000..5bf1ff776
--- /dev/null
+++ b/repro_len_scalar.py
@@ -0,0 +1,9 @@
+import sympy
+
+a = sympy.Array(3)
+print("Array:", a)
+print("shape:", a.shape)
+print("rank:", a.rank())
+print("len(a):", len(a))
+print("len(list(a)):", len(list(a)))
+print("list(a):", list(a))
diff --git a/sympy/tensor/array/dense_ndim_array.py b/sympy/tensor/array/dense_ndim_array.py
index 9be94a863..81457022e 100644
--- a/sympy/tensor/array/dense_ndim_array.py
+++ b/sympy/tensor/array/dense_ndim_array.py
@@ -149,7 +149,8 @@ def _new(cls, iterable, shape, **kwargs):
         self._shape = shape
         self._array = list(flat_list)
         self._rank = len(shape)
-        self._loop_size = functools.reduce(lambda x,y: x*y, shape) if shape else 0
+        # For rank-0 arrays (shape == ()), the number of elements is 1
+        self._loop_size = 1 if self._rank == 0 else functools.reduce(lambda x,y: x*y, shape)
         return self
 
     def __setitem__(self, index, value):
@@ -174,7 +175,8 @@ def _new(cls, iterable, shape, **kwargs):
         self._shape = shape
         self._array = list(flat_list)
         self._rank = len(shape)
-        self._loop_size = functools.reduce(lambda x,y: x*y, shape) if shape else 0
+        # For rank-0 arrays (shape == ()), the number of elements is 1
+        self._loop_size = 1 if self._rank == 0 else functools.reduce(lambda x,y: x*y, shape)
         return self
 
     def __setitem__(self, index, value):
diff --git a/sympy/tensor/array/ndim_array.py b/sympy/tensor/array/ndim_array.py
index 099b29116..5ba8b1b7f 100644
--- a/sympy/tensor/array/ndim_array.py
+++ b/sympy/tensor/array/ndim_array.py
@@ -164,6 +164,9 @@ def _handle_ndarray_creation_inputs(cls, iterable=None, shape=None, **kwargs):
     def __len__(self):
         """Overload common function len(). Returns number of elements in array.
 
+        For rank-0 arrays (scalars) this returns 1, matching the number of
+        elements produced by iteration and NumPy's behaviour.
+
         Examples
         ========
 
@@ -175,7 +178,7 @@ def __len__(self):
         9
 
         """
-        return self._loop_size
+        return self._loop_size if self._rank != 0 else 1
 
     @property
     def shape(self):
@@ -308,6 +311,8 @@ def tolist(self):
         """
 
         def f(sh, shape_left, i, j):
+            if len(shape_left) == 0:
+                return [self[()]]
             if len(shape_left) == 1:
                 return [self[e] for e in range(i, j)]
             result = []
diff --git a/sympy/tensor/array/sparse_ndim_array.py b/sympy/tensor/array/sparse_ndim_array.py
index 72b5d5129..a92126659 100644
--- a/sympy/tensor/array/sparse_ndim_array.py
+++ b/sympy/tensor/array/sparse_ndim_array.py
@@ -130,7 +130,7 @@ def __new__(cls, iterable=None, shape=None, **kwargs):
 
         shape, flat_list = cls._handle_ndarray_creation_inputs(iterable, shape, **kwargs)
         shape = Tuple(*map(_sympify, shape))
-        loop_size = functools.reduce(lambda x,y: x*y, shape) if shape else 0
+        loop_size = 1 if len(shape) == 0 else functools.reduce(lambda x,y: x*y, shape)
 
         # Sparse array:
         if isinstance(flat_list, (dict, Dict)):
@@ -167,7 +167,8 @@ def __new__(cls, iterable=None, shape=None, **kwargs):
         self = object.__new__(cls)
         self._shape = shape
         self._rank = len(shape)
-        self._loop_size = functools.reduce(lambda x,y: x*y, shape) if shape else 0
+        # For rank-0 arrays (shape == ()), the number of elements is 1
+        self._loop_size = 1 if self._rank == 0 else functools.reduce(lambda x,y: x*y, shape)
 
         # Sparse array:
         if isinstance(flat_list, (dict, Dict)):
