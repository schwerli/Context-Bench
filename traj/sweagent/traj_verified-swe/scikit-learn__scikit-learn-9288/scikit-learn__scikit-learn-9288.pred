{"model_name_or_path": "output", "instance_id": "scikit-learn__scikit-learn-9288", "model_patch": "diff --git a/repro_kmeans_n_jobs.py b/repro_kmeans_n_jobs.py\nnew file mode 100644\nindex 000000000..64e4fbdb9\n--- /dev/null\n+++ b/repro_kmeans_n_jobs.py\n@@ -0,0 +1,13 @@\n+import sys\n+sys.path.insert(0, '/testbed')\n+from sklearn.cluster import KMeans\n+from sklearn.datasets import make_blobs\n+\n+# Generate some data\n+X, y = make_blobs(n_samples=10000, centers=10, n_features=2, random_state=2)\n+\n+# Run KMeans with various n_jobs values\n+for n_jobs in range(1, 5):\n+    kmeans = KMeans(n_clusters=10, random_state=2, n_jobs=n_jobs)\n+    kmeans.fit(X)\n+    print('(n_jobs={}) kmeans.inertia_ = {}'.format(n_jobs, kmeans.inertia_))\ndiff --git a/sklearn/cluster/k_means_.py b/sklearn/cluster/k_means_.py\nindex b7fbdf7da..2ae0c1edb 100644\n--- a/sklearn/cluster/k_means_.py\n+++ b/sklearn/cluster/k_means_.py\n@@ -360,17 +360,19 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',\n     else:\n         raise ValueError(\"Algorithm must be 'auto', 'full' or 'elkan', got\"\n                          \" %s\" % str(algorithm))\n+    # To ensure reproducibility independently of the number of jobs,\n+    # generate the random seeds for each run upfront using the provided\n+    # random_state and use those seeds to initialize each run.\n+    seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\n     if effective_n_jobs(n_jobs) == 1:\n-        # For a single thread, less memory is needed if we just store one set\n-        # of the best results (as opposed to one set per run per thread).\n-        for it in range(n_init):\n-            # run a k-means once\n+        # Run sequentially but with per-run seeds, to be consistent with the\n+        # parallel case.\n+        for seed in seeds:\n             labels, inertia, centers, n_iter_ = kmeans_single(\n                 X, sample_weight, n_clusters, max_iter=max_iter, init=init,\n                 verbose=verbose, precompute_distances=precompute_distances,\n                 tol=tol, x_squared_norms=x_squared_norms,\n-                random_state=random_state)\n-            # determine if these results are the best so far\n+                random_state=seed)\n             if best_inertia is None or inertia < best_inertia:\n                 best_labels = labels.copy()\n                 best_centers = centers.copy()\n@@ -378,14 +380,12 @@ def k_means(X, n_clusters, sample_weight=None, init='k-means++',\n                 best_n_iter = n_iter_\n     else:\n         # parallelisation of k-means runs\n-        seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\n         results = Parallel(n_jobs=n_jobs, verbose=0)(\n             delayed(kmeans_single)(X, sample_weight, n_clusters,\n                                    max_iter=max_iter, init=init,\n                                    verbose=verbose, tol=tol,\n                                    precompute_distances=precompute_distances,\n                                    x_squared_norms=x_squared_norms,\n-                                   # Change seed to ensure variety\n                                    random_state=seed)\n             for seed in seeds)\n         # Get results with the lowest inertia\n"}