diff --git a/repro_strip_accents.py b/repro_strip_accents.py
new file mode 100644
index 000000000..123df3b86
--- /dev/null
+++ b/repro_strip_accents.py
@@ -0,0 +1,18 @@
+from sklearn.feature_extraction.text import strip_accents_unicode
+import unicodedata
+
+# s1: precomposed Ã± (U+00F1)
+s1 = chr(241)
+# s2: decomposed n + combining tilde (U+006E U+0303)
+s2 = chr(110) + chr(771)
+
+print('s1:', s1, [hex(ord(c)) for c in s1])
+print('s2:', s2, [hex(ord(c)) for c in s2])
+
+print('normalized s1 equals original?', unicodedata.normalize('NFKD', s1) == s1)
+print('normalized s2 equals original?', unicodedata.normalize('NFKD', s2) == s2)
+
+print('strip_accents_unicode(s1):', strip_accents_unicode(s1))
+print('strip_accents_unicode(s2):', strip_accents_unicode(s2))
+print('Expected both to be "n"')
+print('Does s2 remain unchanged?', strip_accents_unicode(s2) == s2)
diff --git a/sklearn/feature_extraction/text.py b/sklearn/feature_extraction/text.py
index bb5a9d646..d3f87b2dd 100644
--- a/sklearn/feature_extraction/text.py
+++ b/sklearn/feature_extraction/text.py
@@ -129,11 +129,14 @@ def strip_accents_unicode(s):
         Remove accentuated char for any unicode symbol that has a direct
         ASCII equivalent.
     """
+    # Always normalize to NFKD and then remove combining characters.
+    # Previously, this function returned the input unchanged if it was already
+    # in NFKD form. That behavior failed to strip accents for strings that were
+    # decomposed (e.g., "n" + combining tilde). By consistently filtering out
+    # combining marks, we ensure both precomposed and decomposed forms are
+    # normalized to their simple counterparts.
     normalized = unicodedata.normalize('NFKD', s)
-    if normalized == s:
-        return s
-    else:
-        return ''.join([c for c in normalized if not unicodedata.combining(c)])
+    return ''.join(c for c in normalized if not unicodedata.combining(c))
 
 
 def strip_accents_ascii(s):
